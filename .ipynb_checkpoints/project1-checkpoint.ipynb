{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from proj1_helpers import*\n",
    "from Implementations import*\n",
    "from Plot import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../../data/train.csv/train.csv' # TODO: download train data and supply path here \n",
    "y_train, tX_train, ids_train = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data to co a common pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../data/test.csv/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and parameters definition for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr, tX_tr, ids_tr, tX_te, ids_te = preprocess_data(y_train, tX_train, ids_train, tX_test, ids_test, param={'Build_poly': False})\n",
    "\n",
    "seed = 1\n",
    "k_fold = 4 \n",
    "model = 'least_squares'\n",
    "degrees = np.arange(1,16,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing degree 1/15, model: least_squares, arguments: None\n",
      "Optimizing degree 2/15, model: least_squares, arguments: None\n",
      "Optimizing degree 3/15, model: least_squares, arguments: None\n",
      "Optimizing degree 4/15, model: least_squares, arguments: None\n",
      "Optimizing degree 5/15, model: least_squares, arguments: None\n",
      "Optimizing degree 6/15, model: least_squares, arguments: None\n",
      "Optimizing degree 7/15, model: least_squares, arguments: None\n",
      "Optimizing degree 8/15, model: least_squares, arguments: None\n",
      "Optimizing degree 9/15, model: least_squares, arguments: None\n",
      "Optimizing degree 10/15, model: least_squares, arguments: None\n",
      "Optimizing degree 11/15, model: least_squares, arguments: None\n",
      "Optimizing degree 12/15, model: least_squares, arguments: None\n",
      "Optimizing degree 13/15, model: least_squares, arguments: None\n",
      "Optimizing degree 14/15, model: least_squares, arguments: None\n",
      "Optimizing degree 15/15, model: least_squares, arguments: None\n"
     ]
    }
   ],
   "source": [
    "losses_tr, losses_te, accs_tr, accs_te = params_optimization(y_tr, tX_tr, k_fold, model, degrees, params = None, seed = seed, feedback = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we optimized over degrees only, so we plot degree vs accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_vs_err(degrees, accs_tr, accs_te, model, 'Accuracy', save_img = True, img_name = 'least_squares_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot on MSE gave aberrant values, which illustrates that this loss is not appropriate for categorization, and gets out of hand. We use accuracy for model selection and performance assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually choose best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_degree = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute performance on k splits with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation 1/6 for least_squares, extended feature of degree 10 and arguments : None\n",
      "Starting cross-validation 2/6 for least_squares, extended feature of degree 10 and arguments : None\n",
      "Starting cross-validation 3/6 for least_squares, extended feature of degree 10 and arguments : None\n",
      "Starting cross-validation 4/6 for least_squares, extended feature of degree 10 and arguments : None\n",
      "Starting cross-validation 5/6 for least_squares, extended feature of degree 10 and arguments : None\n",
      "Starting cross-validation 6/6 for least_squares, extended feature of degree 10 and arguments : None\n"
     ]
    }
   ],
   "source": [
    "model = 'least_squares'\n",
    "seed = 1\n",
    "k_fold = 6\n",
    "k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "params = None\n",
    "\n",
    "accs_te = []\n",
    "losses_te = []\n",
    "for k in range(k_fold):\n",
    "    _, loss_te, _, acc_te = cross_validation(y_tr, tX_tr, k_indices, k, model, best_degree, params, feedback = True)\n",
    "    accs_te.append(acc_te)\n",
    "    losses_te.append(loss_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of best least squares model:  0.814\n"
     ]
    }
   ],
   "source": [
    "print('Mean accuracy of best least squares model: ', round(np.mean(accs_te),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store values for comparison with other models later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('best_models_perf/best_accs_' + model, accs_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares GD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and parameters definition for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr, tX_tr, ids_tr, tX_te, ids_te = preprocess_data(y_train, tX_train, ids_train, tX_test, ids_test, param={'Build_poly': False})\n",
    "\n",
    "seed = 1\n",
    "k_fold = 4 \n",
    "model = 'least_squares_GD'\n",
    "degrees = np.arange(1,16,1)\n",
    "max_iters=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Optimizing degree 1/15, model: least_squares_GD, arguments: {'max_iters': 100}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =0.483255048656341, w0=-0.15638754543867078, w1=0.0022675697389240024\n",
      "Gradient Descent(2/99): loss =0.4527426715262931, w0=-0.19473431432998656, w1=-0.018317043848454593\n",
      "Gradient Descent(3/99): loss =0.3905335196117481, w0=-0.22245075695700134, w1=0.043398214905318894\n",
      "Gradient Descent(4/99): loss =0.37138664279698524, w0=-0.2432659983158094, w1=-0.037970341804291055\n",
      "Gradient Descent(5/99): loss =0.36226042999252694, w0=-0.2590358828020507, w1=0.006775484886061903\n",
      "Gradient Descent(6/99): loss =0.3584686667199764, w0=-0.2711452021887987, w1=-0.020793785388985055\n",
      "Gradient Descent(7/99): loss =0.35489192842399936, w0=-0.28049025052362075, w1=-0.007925784848262254\n",
      "Gradient Descent(8/99): loss =0.3516845703374562, w0=-0.2877083679934033, w1=-0.014253620824273356\n",
      "Gradient Descent(9/99): loss =0.34968289014504356, w0=-0.2933076262913485, w1=-0.010207959174286806\n",
      "Gradient Descent(10/99): loss =0.3483734727330971, w0=-0.29764762076022383, w1=-0.010749662281560417\n",
      "Gradient Descent(11/99): loss =0.3474029445713962, w0=-0.3010185078405906, w1=-0.008755407426513699\n",
      "Gradient Descent(12/99): loss =0.34662596888519814, w0=-0.30363535155952875, w1=-0.007837704877916365\n",
      "Gradient Descent(13/99): loss =0.34597843340032974, w0=-0.30566877147357097, w1=-0.006351791266461181\n",
      "Gradient Descent(14/99): loss =0.3454263339820383, w0=-0.30724847209483724, w1=-0.005153193823238247\n",
      "Gradient Descent(15/99): loss =0.3449484833239673, w0=-0.3084762968682305, w1=-0.0038780952688148044\n",
      "Gradient Descent(16/99): loss =0.34453033279876316, w0=-0.30943058249370275, w1=-0.002724965193140249\n",
      "Gradient Descent(17/99): loss =0.34416130679029017, w0=-0.31017250863650553, w1=-0.0016159679656890798\n",
      "Gradient Descent(18/99): loss =0.34383343722870435, w0=-0.310749388559476, w1=-0.0005942146045445366\n",
      "Gradient Descent(19/99): loss =0.3435405573127139, w0=-0.31119807348026124, w1=0.00035847960218394494\n",
      "Gradient Descent(20/99): loss =0.34327778717887303, w0=-0.31154713293917863, w1=0.0012358851465281422\n",
      "Gradient Descent(21/99): loss =0.3430411895874427, w0=-0.3118187895542587, w1=0.0020450395252090783\n",
      "Gradient Descent(22/99): loss =0.34282753274570266, w0=-0.31203029136660915, w1=0.0027884524810167072\n",
      "Gradient Descent(23/99): loss =0.34263412330807247, w0=-0.312195046009934, w1=0.0034713029911658004\n",
      "Gradient Descent(24/99): loss =0.34245868659257145, w0=-0.3123234652157917, w1=0.00409791648183036\n",
      "Gradient Descent(25/99): loss =0.3422992791838842, w0=-0.3124236400001533, w1=0.004672943065774423\n",
      "Gradient Descent(26/99): loss =0.34215422409992285, w0=-0.31250185500710487, w1=0.0052007076393998895\n",
      "Gradient Descent(27/99): loss =0.34202206191996926, w0=-0.31256299343496824, w1=0.005685337968548712\n",
      "Gradient Descent(28/99): loss =0.34190151335748886, w0=-0.31261084879122114, w1=0.006130666170944734\n",
      "Gradient Descent(29/99): loss =0.34179145015501994, w0=-0.3126483685758362, w1=0.006540225554616808\n",
      "Gradient Descent(30/99): loss =0.34169087211007626, w0=-0.3126778428141928, w1=0.006917253454467315\n",
      "Gradient Descent(31/99): loss =0.3415988886792179, w0=-0.31270105093493356, w1=0.007264693629959211\n",
      "Gradient Descent(32/99): loss =0.3415147040454386, w0=-0.3127193756595512, w1=0.007585216921665702\n",
      "Gradient Descent(33/99): loss =0.3414376048403613, w0=-0.31273389159101883, w1=0.007881235468577273\n",
      "Gradient Descent(34/99): loss =0.3413669499280032, w0=-0.3127454339735095, w1=0.008154925369815587\n",
      "Gradient Descent(35/99): loss =0.34130216181031675, w0=-0.31275465213694054, w1=0.008408245415238954\n",
      "Gradient Descent(36/99): loss =0.3412427193248113, w0=-0.31276205099594306, w1=0.008642958104190376\n",
      "Gradient Descent(37/99): loss =0.34118815138444675, w0=-0.31276802329315184, w1=0.008860648197498253\n",
      "Gradient Descent(38/99): loss =0.34113803156839845, w0=-0.3127728746391474, w1=0.00906274099070176\n",
      "Gradient Descent(39/99): loss =0.3410919734154084, w0=-0.31277684296324476, w1=0.009250518712991164\n",
      "Gradient Descent(40/99): loss =0.3410496263035311, w0=-0.312780113618997, w1=0.009425135805097622\n",
      "Gradient Descent(41/99): loss =0.34101067182419126, w0=-0.31278283111615984, w1=0.009587632627147943\n",
      "Gradient Descent(42/99): loss =0.34097482057674394, w0=-0.31278510823128264, w1=0.009738947899286986\n",
      "Gradient Descent(43/99): loss =0.3409418093237045, w0=-0.31278703308273037, w1=0.0098799298130113\n",
      "Gradient Descent(44/99): loss =0.3409113984576232, w0=-0.3127886746244688, w1=0.010011345980954984\n",
      "Gradient Descent(45/99): loss =0.3408833697389992, w0=-0.3127900869119514, w1=0.010133892282434906\n",
      "Gradient Descent(46/99): loss =0.3408575242712799, w0=-0.312791312414377, w1=0.010248200726741945\n",
      "Gradient Descent(47/99): loss =0.34083368068427783, w0=-0.31279238458646924, w1=0.010354846419351619\n",
      "Gradient Descent(48/99): loss =0.3408116735016019, w0=-0.3127933298652836, w1=0.010454353729828586\n",
      "Gradient Descent(49/99): loss =0.34079135167116414, w0=-0.31279416922061526, w1=0.010547201744140198\n",
      "Gradient Descent(50/99): loss =0.34077257724067017, w0=-0.31279491935884784, w1=0.010633829082509432\n",
      "Gradient Descent(51/99): loss =0.3407552241623674, w0=-0.31279559365778314, w1=0.010714638154054728\n",
      "Gradient Descent(52/99): loss =0.3407391772133045, w0=-0.3127962028926597, w1=0.010789998913974449\n",
      "Gradient Descent(53/99): loss =0.34072433101902594, w0=-0.3127967558001089, w1=0.010860252181553072\n",
      "Gradient Descent(54/99): loss =0.34071058917005936, w0=-0.3127972595163429, w1=0.010925712571403588\n",
      "Gradient Descent(55/99): loss =0.34069786342176633, w0=-0.31279771991774785, w1=0.010986671084334486\n",
      "Gradient Descent(56/99): loss =0.34068607296919234, w0=-0.3127981418857524, w1=0.011043397399040396\n",
      "Gradient Descent(57/99): loss =0.340675143789469, w0=-0.3127985295129424, w1=0.01109614190095223\n",
      "Gradient Descent(58/99): loss =0.34066500804511884, w0=-0.31279888626359276, w1=0.011145137480294557\n",
      "Gradient Descent(59/99): loss =0.3406556035423259, w0=-0.3127992150988347, w1=0.011190601127523816\n",
      "Gradient Descent(60/99): loss =0.34064687323884807, w0=-0.31279951857438515, w1=0.011232735350894361\n",
      "Gradient Descent(61/99): loss =0.3406387647968004, w0=-0.3127997989169876, w1=0.011271729437854339\n",
      "Gradient Descent(62/99): loss =0.34063123017602137, w0=-0.3128000580843313, w1=0.01130776057928561\n",
      "Gradient Descent(63/99): loss =0.3406242252641729, w0=-0.31280029781214735, w1=0.011340994873235859\n",
      "Gradient Descent(64/99): loss =0.3406177095401012, w0=-0.3128005196513467, w1=0.011371588222707455\n",
      "Gradient Descent(65/99): loss =0.3406116457673367, w0=-0.3128007249974223, w1=0.01139968714024268\n",
      "Gradient Descent(66/99): loss =0.3406059997149183, w0=-0.3128009151138368, w1=0.011425429470443274\n",
      "Gradient Descent(67/99): loss =0.3406007399029953, w0=-0.31280109115073024, w1=0.011448945040162422\n",
      "Gradient Descent(68/99): loss =0.34059583737092, w0=-0.3128012541599808, w1=0.01147035624488259\n",
      "Gradient Descent(69/99): loss =0.34059126546575236, w0=-0.3128014051074201, w1=0.011489778578723044\n",
      "Gradient Descent(70/99): loss =0.34058699964930744, w0=-0.3128015448828231, w1=0.01150732111458706\n",
      "Gradient Descent(71/99): loss =0.3405830173220493, w0=-0.31280167430815287, w1=0.01152308694014443\n",
      "Gradient Descent(72/99): loss =0.34057929766229983, w0=-0.31280179414443376, w1=0.011537173554634078\n",
      "Gradient Descent(73/99): loss =0.3405758214793737, w0=-0.3128019050975406, w1=0.01154967323085225\n",
      "Gradient Descent(74/99): loss =0.34057257107938405, w0=-0.312802007823128, w1=0.01156067334615127\n",
      "Gradient Descent(75/99): loss =0.34056953014257807, w0=-0.3128021029308743, w1=0.011570256685803292\n",
      "Gradient Descent(76/99): loss =0.34056668361117254, w0=-0.31280219098817397, w1=0.011578501721672593\n",
      "Gradient Descent(77/99): loss =0.3405640175867511, w0=-0.3128022725233841, w1=0.011585482868782873\n",
      "Gradient Descent(78/99): loss =0.34056151923637473, w0=-0.3128023480287066, w1=0.011591270722052096\n",
      "Gradient Descent(79/99): loss =0.34055917670663544, w0=-0.31280241796277014, w1=0.01159593227519764\n",
      "Gradient Descent(80/99): loss =0.34055697904495175, w0=-0.3128024827529615, w1=0.011599531123574855\n",
      "Gradient Descent(81/99): loss =0.34055491612747, w0=-0.31280254279754627, w1=0.011602127652505513\n",
      "Gradient Descent(82/99): loss =0.34055297859299616, w0=-0.3128025984676091, w1=0.011603779212472227\n",
      "Gradient Descent(83/99): loss =0.34055115778242967, w0=-0.3128026501088394, w1=0.011604540282395591\n",
      "Gradient Descent(84/99): loss =0.34054944568322426, w0=-0.312802698043181, w1=0.011604462622073111\n",
      "Gradient Descent(85/99): loss =0.3405478348784397, w0=-0.31280274257036267, w1=0.011603595414737737\n",
      "Gradient Descent(86/99): loss =0.3405463184999901, w0=-0.31280278396932204, w1=0.011601985400588412\n",
      "Gradient Descent(87/99): loss =0.34054489018572764, w0=-0.31280282249953356, w1=0.01159967700205099\n",
      "Gradient Descent(88/99): loss =0.34054354404003545, w0=-0.3128028584022496, w1=0.01159671244144829\n",
      "Gradient Descent(89/99): loss =0.34054227459762826, w0=-0.312802891901662, w1=0.011593131851684585\n",
      "Gradient Descent(90/99): loss =0.34054107679029233, w0=-0.3128029232059906, w1=0.011588973380488875\n",
      "Gradient Descent(91/99): loss =0.3405399459163125, w0=-0.3128029525085042, w1=0.011584273288704448\n",
      "Gradient Descent(92/99): loss =0.3405388776123625, w0=-0.31280297998847845, w1=0.011579066043065061\n",
      "Gradient Descent(93/99): loss =0.3405378678276512, w0=-0.3128030058120958, w1=0.011573384403853974\n",
      "Gradient Descent(94/99): loss =0.34053691280013626, w0=-0.3128030301332904, w1=0.011567259507804753\n",
      "Gradient Descent(95/99): loss =0.340536009034632, w0=-0.3128030530945418, w1=0.011560720946569516\n",
      "Gradient Descent(96/99): loss =0.34053515328265693, w0=-0.31280307482762093, w1=0.011553796841049378\n",
      "Gradient Descent(97/99): loss =0.3405343425238739, w0=-0.3128030954542906, w1=0.011546513911857738\n",
      "Gradient Descent(98/99): loss =0.34053357394899364, w0=-0.3128031150869636, w1=0.01153889754616079\n",
      "Gradient Descent(99/99): loss =0.34053284494402164, w0=-0.3128031338293213, w1=0.011530971861121701\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =0.4843317370646539, w0=-0.1564069245004354, w1=0.002203595528667751\n",
      "Gradient Descent(2/99): loss =0.4512310306853524, w0=-0.19503791661528777, w1=-0.023444748116516954\n",
      "Gradient Descent(3/99): loss =0.3903205282428487, w0=-0.2230684141001829, w1=0.040212441166941865\n",
      "Gradient Descent(4/99): loss =0.3707440484702347, w0=-0.24414377340585192, w1=-0.038472374599034936\n",
      "Gradient Descent(5/99): loss =0.36151992841578645, w0=-0.260230366768826, w1=0.004289321330594786\n",
      "Gradient Descent(6/99): loss =0.3581339192677553, w0=-0.2726222618051328, w1=-0.02270602464200626\n",
      "Gradient Descent(7/99): loss =0.35457475227282725, w0=-0.28222396406026473, w1=-0.00925742551745957\n",
      "Gradient Descent(8/99): loss =0.3512748431055695, w0=-0.28967569444864827, w1=-0.016453305233121333\n",
      "Gradient Descent(9/99): loss =0.34921136178223977, w0=-0.2954795391999161, w1=-0.011530521725926893\n",
      "Gradient Descent(10/99): loss =0.3478773817543152, w0=-0.29999851276854245, w1=-0.012666353938516946\n",
      "Gradient Descent(11/99): loss =0.34689957630530077, w0=-0.3035229318922784, w1=-0.010185387735470154\n",
      "Gradient Descent(12/99): loss =0.346122340896496, w0=-0.30627050937438005, w1=-0.009491065956311573\n",
      "Gradient Descent(13/99): loss =0.34547727286268853, w0=-0.30841386997897835, w1=-0.007769389195548504\n",
      "Gradient Descent(14/99): loss =0.34492871762807165, w0=-0.31008525767348166, w1=-0.006605641736691925\n",
      "Gradient Descent(15/99): loss =0.3444547539624261, w0=-0.3113887753588753, w1=-0.005197340817378985\n",
      "Gradient Descent(16/99): loss =0.3440405060592483, w0=-0.31240500343726313, w1=-0.00400230904022127\n",
      "Gradient Descent(17/99): loss =0.3436752309335213, w0=-0.3131971145685581, w1=-0.0027943047474822593\n",
      "Gradient Descent(18/99): loss =0.34335088106974593, w0=-0.3138142416156152, w1=-0.0017019994284191017\n",
      "Gradient Descent(19/99): loss =0.3430612537928285, w0=-0.31429482189580665, w1=-0.0006602230902809205\n",
      "Gradient Descent(20/99): loss =0.3428014600707135, w0=-0.3146688147131609, w1=0.00029772462081304753\n",
      "Gradient Descent(21/99): loss =0.3425675679936832, w0=-0.31495963310430575, w1=0.0011929857567309372\n",
      "Gradient Descent(22/99): loss =0.3423563595323232, w0=-0.31518554318206515, w1=0.002019641958967375\n",
      "Gradient Descent(23/99): loss =0.3421651592511882, w0=-0.31536081232376306, w1=0.002786858811003962\n",
      "Gradient Descent(24/99): loss =0.34199171215557034, w0=-0.3154965757603078, w1=0.0034962982414033323\n",
      "Gradient Descent(25/99): loss =0.34183409474701115, w0=-0.3156015285383923, w1=0.004153624382814252\n",
      "Gradient Descent(26/99): loss =0.341690649339857, w0=-0.31568245764861225, w1=0.004762269270044548\n",
      "Gradient Descent(27/99): loss =0.3415599346975135, w0=-0.31574466172821586, w1=0.005326553175768265\n",
      "Gradient Descent(28/99): loss =0.3414406883746816, w0=-0.31579227655813835, w1=0.0058499747429087015\n",
      "Gradient Descent(29/99): loss =0.34133179752449905, w0=-0.31582853037249464, w1=0.006336060523018989\n",
      "Gradient Descent(30/99): loss =0.34123227593024286, w0=-0.3158559426124421, w1=0.006787907012222477\n",
      "Gradient Descent(31/99): loss =0.3411412456652762, w0=-0.31587647943423874, w1=0.007208421952307425\n",
      "Gradient Descent(32/99): loss =0.3410579222448741, w0=-0.3158916749724869, w1=0.007600213280069861\n",
      "Gradient Descent(33/99): loss =0.34098160244543485, w0=-0.3159027261110328, w1=0.007965668661114308\n",
      "Gradient Descent(34/99): loss =0.3409116541893713, w0=-0.31591056643765875, w1=0.008306939917985045\n",
      "Gradient Descent(35/99): loss =0.3408475080502947, w0=-0.31591592401125745, w1=0.008625978731244172\n",
      "Gradient Descent(36/99): loss =0.34078865004589465, w0=-0.3159193664537162, w1=0.00892454599899736\n",
      "Gradient Descent(37/99): loss =0.3407346154670561, w0=-0.31592133616235246, w1=0.009204234202966809\n",
      "Gradient Descent(38/99): loss =0.3406849835511287, w0=-0.3159221777984901, w1=0.009466481703399416\n",
      "Gradient Descent(39/99): loss =0.3406393728508746, w0=-0.31592215974985693, w1=0.00971258954519798\n",
      "Gradient Descent(40/99): loss =0.34059743718304564, w0=-0.3159214908850277, w1=0.009943735090198427\n",
      "Gradient Descent(41/99): loss =0.34055886206481295, w0=-0.31592033363319116, w1=0.010160985366624308\n",
      "Gradient Descent(42/99): loss =0.34052336156464696, w0=-0.3159188141940883, w1=0.010365308655736172\n",
      "Gradient Descent(43/99): loss =0.3404906755082609, w0=-0.31591703050765396, w1=0.010557585160293386\n",
      "Gradient Descent(44/99): loss =0.3404605669910421, w0=-0.315915058474394, w1=0.010738616425012242\n",
      "Gradient Descent(45/99): loss =0.3404328201568138, w0=-0.31591295681019993, w1=0.010909133807250584\n",
      "Gradient Descent(46/99): loss =0.34040723820939583, w0=-0.315910770835059, w1=0.011069805977610904\n",
      "Gradient Descent(47/99): loss =0.34038364162869666, w0=-0.31590853542955416, w1=0.011221245596252568\n",
      "Gradient Descent(48/99): loss =0.34036186656730383, w0=-0.31590627734173543, w1=0.011364015220442811\n",
      "Gradient Descent(49/99): loss =0.34034176340697825, w0=-0.3159040169869324, w1=0.011498632538887273\n",
      "Gradient Descent(50/99): loss =0.34032319545727463, w0=-0.3159017698518077, w1=0.01162557499904749\n",
      "Gradient Descent(51/99): loss =0.34030603778085106, w0=-0.3158995475895446, w1=0.011745283899529797\n",
      "Gradient Descent(52/99): loss =0.3402901761319812, w0=-0.3158973588740003, w1=0.011858168006986908\n",
      "Gradient Descent(53/99): loss =0.3402755059964383, w0=-0.3158952100657766, w1=0.01196460675411817\n",
      "Gradient Descent(54/99): loss =0.3402619317223151, w0=-0.31589310573153817, w1=0.012064953067827493\n",
      "Gradient Descent(55/99): loss =0.3402493657325566, w0=-0.3158910490488381, w1=0.012159535872065325\n",
      "Gradient Descent(56/99): loss =0.34023772781101236, w0=-0.31588904212162716, w1=0.012248662304439712\n",
      "Gradient Descent(57/99): loss =0.3402269444547224, w0=-0.3158870862260956, w1=0.012332619681387118\n",
      "Gradient Descent(58/99): loss =0.34021694828593324, w0=-0.31588518200217797, w1=0.01241167724248318\n",
      "Gradient Descent(59/99): loss =0.3402076775180345, w0=-0.3158833296026841, w1=0.012486087700868018\n",
      "Gradient Descent(60/99): loss =0.34019907547020817, w0=-0.31588152880938974, w1=0.012556088623460268\n",
      "Gradient Descent(61/99): loss =0.3401910901261267, w0=-0.3158797791233659, w1=0.012621903661744854\n",
      "Gradient Descent(62/99): loss =0.3401836737325036, w0=-0.31587807983522775, w1=0.01268374365134277\n",
      "Gradient Descent(63/99): loss =0.34017678243373034, w0=-0.3158764300797302, w1=0.0127418075963067\n",
      "Gradient Descent(64/99): loss =0.3401703759392029, w0=-0.3158748288781655, w1=0.012796283552089836\n",
      "Gradient Descent(65/99): loss =0.3401644172202845, w0=-0.31587327517125435, w1=0.012847349419382537\n",
      "Gradient Descent(66/99): loss =0.3401588722341427, w0=-0.3158717678446318, w1=0.012895173659476372\n",
      "Gradient Descent(67/99): loss =0.34015370967197817, w0=-0.31587030574856206, w1=0.012939915940466618\n",
      "Gradient Descent(68/99): loss =0.3401489007293934, w0=-0.3158688877131603, w1=0.01298172772243132\n",
      "Gradient Descent(69/99): loss =0.34014441889687425, w0=-0.3158675125601127, w1=0.013020752788694396\n",
      "Gradient Descent(70/99): loss =0.3401402397685467, w0=-0.3158661791116715, w1=0.013057127729385085\n",
      "Gradient Descent(71/99): loss =0.3401363408675472, w0=-0.3158648861975267, w1=0.013090982382722036\n",
      "Gradient Descent(72/99): loss =0.34013270148650465, w0=-0.31586363266002526, w1=0.0131224402387695\n",
      "Gradient Descent(73/99): loss =0.34012930254177165, w0=-0.31586241735810344, w1=0.013151618809816704\n",
      "Gradient Descent(74/99): loss =0.34012612644016915, w0=-0.3158612391702174, w1=0.013178629971015701\n",
      "Gradient Descent(75/99): loss =0.3401231569571284, w0=-0.31586009699649414, w1=0.013203580274459519\n",
      "Gradient Descent(76/99): loss =0.34012037912521415, w0=-0.31585898976027577, w1=0.013226571239491475\n",
      "Gradient Descent(77/99): loss =0.3401177791321096, w0=-0.3158579164091914, w1=0.013247699621692307\n",
      "Gradient Descent(78/99): loss =0.34011534422722534, w0=-0.31585687591586203, w1=0.0132670576626963\n",
      "Gradient Descent(79/99): loss =0.3401130626361766, w0=-0.31585586727831955, w1=0.013284733322724554\n",
      "Gradient Descent(80/99): loss =0.340110923482436, w0=-0.31585488952020413, w1=0.013300810497501028\n",
      "Gradient Descent(81/99): loss =0.3401089167155381, w0=-0.3158539416907894, w1=0.013315369221016478\n",
      "Gradient Descent(82/99): loss =0.340107033045264, w0=-0.31585302286487377, w1=0.01332848585543649\n",
      "Gradient Descent(83/99): loss =0.34010526388128914, w0=-0.3158521321425689, w1=0.013340233269297326\n",
      "Gradient Descent(84/99): loss =0.3401036012778217, w0=-0.3158512686490083, w1=0.013350681005004934\n",
      "Gradient Descent(85/99): loss =0.34010203788280363, w0=-0.31585043153399495, w1=0.013359895436536559\n",
      "Gradient Descent(86/99): loss =0.34010056689128165, w0=-0.31584961997160216, w1=0.013367939918145022\n",
      "Gradient Descent(87/99): loss =0.34009918200259504, w0=-0.31584883315973944, w1=0.013374874924779689\n",
      "Gradient Descent(88/99): loss =0.34009787738105224, w0=-0.3158480703196918, w1=0.013380758184859965\n",
      "Gradient Descent(89/99): loss =0.3400966476198032, w0=-0.3158473306956403, w1=0.013385644805972022\n",
      "Gradient Descent(90/99): loss =0.340095487707637, w0=-0.31584661355416843, w1=0.013389587393999942\n",
      "Gradient Descent(91/99): loss =0.3400943929984573, w0=-0.31584591818375995, w1=0.013392636166151356\n",
      "Gradient Descent(92/99): loss =0.34009335918321376, w0=-0.3158452438942909, w1=0.01339483905829255\n",
      "Gradient Descent(93/99): loss =0.34009238226408145, w0=-0.3158445900165188, w1=0.013396241826967917\n",
      "Gradient Descent(94/99): loss =0.34009145853070366, w0=-0.31584395590157166, w1=0.013396888146444427\n",
      "Gradient Descent(95/99): loss =0.34009058453832414, w0=-0.31584334092043803, w1=0.013396819701089805\n",
      "Gradient Descent(96/99): loss =0.34008975708765615, w0=-0.31584274446346045, w1=0.013396076273366768\n",
      "Gradient Descent(97/99): loss =0.34008897320634135, w0=-0.3158421659398325, w1=0.01339469582770069\n",
      "Gradient Descent(98/99): loss =0.34008823013187145, w0=-0.31584160477710155, w1=0.013392714590457575\n",
      "Gradient Descent(99/99): loss =0.34008752529585046, w0=-0.31584106042067694, w1=0.013390167126248728\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =0.48314853251121964, w0=-0.15618438910720278, w1=0.0019742699526571067\n",
      "Gradient Descent(2/99): loss =0.4497918721847011, w0=-0.19478247821547154, w1=-0.0248080136046422\n",
      "Gradient Descent(3/99): loss =0.3899501749709371, w0=-0.22258365275850078, w1=0.037928023840007596\n",
      "Gradient Descent(4/99): loss =0.37087832867993725, w0=-0.24352058242469946, w1=-0.040491820679269194\n",
      "Gradient Descent(5/99): loss =0.3627013635648381, w0=-0.2594249707465352, w1=0.003146631847837436\n",
      "Gradient Descent(6/99): loss =0.3588385221776773, w0=-0.2716667725813069, w1=-0.024099990991739678\n",
      "Gradient Descent(7/99): loss =0.3546679833010586, w0=-0.2811260735030992, w1=-0.010243095781458137\n",
      "Gradient Descent(8/99): loss =0.35169729520264204, w0=-0.2884520816092219, w1=-0.017153961691558698\n",
      "Gradient Descent(9/99): loss =0.34988676088869014, w0=-0.2941461350531033, w1=-0.01220510790161964\n",
      "Gradient Descent(10/99): loss =0.3486555541122542, w0=-0.29857021238408565, w1=-0.013091831812871308\n",
      "Gradient Descent(11/99): loss =0.3477181492270156, w0=-0.3020138164769144, w1=-0.010635584340800083\n",
      "Gradient Descent(12/99): loss =0.34695780458317427, w0=-0.30469316291210624, w1=-0.009844011748733983\n",
      "Gradient Descent(13/99): loss =0.34631967587673307, w0=-0.30677956039685655, w1=-0.008169604243437763\n",
      "Gradient Descent(14/99): loss =0.34577311289615775, w0=-0.3084037992355208, w1=-0.007016847414024741\n",
      "Gradient Descent(15/99): loss =0.3452984086889062, w0=-0.30966866080626976, w1=-0.005686576435018114\n",
      "Gradient Descent(16/99): loss =0.3448818476870149, w0=-0.3106534750407353, w1=-0.004566927949540321\n",
      "Gradient Descent(17/99): loss =0.3445133785395467, w0=-0.3114202994055096, w1=-0.003465386850544587\n",
      "Gradient Descent(18/99): loss =0.34418538955876854, w0=-0.3120172752834231, w1=-0.0024850205539311706\n",
      "Gradient Descent(19/99): loss =0.3438919591230675, w0=-0.31248197763732305, w1=-0.001569803253792982\n",
      "Gradient Descent(20/99): loss =0.3436283783612739, w0=-0.3128436232697809, w1=-0.0007433182369675068\n",
      "Gradient Descent(21/99): loss =0.34339082848773084, w0=-0.31312499280034317, w1=1.4044064564267401e-05\n",
      "Gradient Descent(22/99): loss =0.34317615944408036, w0=-0.313343818644908, w1=0.0007004378540069858\n",
      "Gradient Descent(23/99): loss =0.34298173344302035, w0=-0.3135139209688619, w1=0.0013254614843063181\n",
      "Gradient Descent(24/99): loss =0.342805312857834, w0=-0.31364606301555553, w1=0.0018927211179525511\n",
      "Gradient Descent(25/99): loss =0.34264497816438844, w0=-0.3137486321226252, w1=0.002408590486574917\n",
      "Gradient Descent(26/99): loss =0.34249906696412985, w0=-0.31382816224755844, w1=0.0028775311083017964\n",
      "Gradient Descent(27/99): loss =0.3423661278457386, w0=-0.31388974494895283, w1=0.0033044254296165395\n",
      "Gradient Descent(28/99): loss =0.34224488492835, w0=-0.3139373473900917, w1=0.0036933394463840676\n",
      "Gradient Descent(29/99): loss =0.34213421016623075, w0=-0.31397406111419557, w1=0.004048159278302317\n",
      "Gradient Descent(30/99): loss =0.34203310139090476, w0=-0.31400229526611045, w1=0.004372282778146032\n",
      "Gradient Descent(31/99): loss =0.34194066464525813, w0=-0.3140239273968017, w1=0.0046688107595675\n",
      "Gradient Descent(32/99): loss =0.341856099776516, w0=-0.31404042079223615, w1=0.004940488036273824\n",
      "Gradient Descent(33/99): loss =0.34177868853552684, w0=-0.3140529159550279, w1=0.005189776459253204\n",
      "Gradient Descent(34/99): loss =0.3417077846303802, w0=-0.3140623018358586, w1=0.005418860601446442\n",
      "Gradient Descent(35/99): loss =0.34164280532369734, w0=-0.31406927135133755, w1=0.005629686974822734\n",
      "Gradient Descent(36/99): loss =0.3415832242651791, w0=-0.31407436462995325, w1=0.005823983815095778\n",
      "Gradient Descent(37/99): loss =0.3415285653248915, w0=-0.31407800271414743, w1=0.006003288076747133\n",
      "Gradient Descent(38/99): loss =0.3414783972470535, w0=-0.31408051381894075, w1=0.006168965641859336\n",
      "Gradient Descent(39/99): loss =0.34143232898416687, w0=-0.31408215379585397, w1=0.006322231861250611\n",
      "Gradient Descent(40/99): loss =0.34139000560125893, w0=-0.3140831220799166, w1=0.006464168824862293\n",
      "Gradient Descent(41/99): loss =0.34135110466253415, w0=-0.3140835741185235, w1=0.006595741360081816\n",
      "Gradient Descent(42/99): loss =0.341315333029869, w0=-0.31408363105845155, w1=0.0067178109266481005\n",
      "Gradient Descent(43/99): loss =0.34128242401571784, w0=-0.31408338729672, w1=0.006831148058451073\n",
      "Gradient Descent(44/99): loss =0.34125213484319805, w0=-0.31408291636669877, w1=0.006936443249384081\n",
      "Gradient Descent(45/99): loss =0.3412242443740885, w0=-0.3140822755269795, w1=0.007034316565522087\n",
      "Gradient Descent(46/99): loss =0.3411985510717915, w0=-0.3140815093392248, w1=0.0071253260582600465\n",
      "Gradient Descent(47/99): loss =0.34117487117134654, w0=-0.31408065245806954, w1=0.007209975146064405\n",
      "Gradient Descent(48/99): loss =0.3411530370326617, w0=-0.3140797318068583, w1=0.0072887190685252765\n",
      "Gradient Descent(49/99): loss =0.34113289565644916, w0=-0.3140787682746532, w1=0.007361970532304126\n",
      "Gradient Descent(50/99): loss =0.341114307345093, w0=-0.31407777804004444, w1=0.007430104643382021\n",
      "Gradient Descent(51/99): loss =0.34109714449296225, w0=-0.3140767736040077, w1=0.007493463216578907\n",
      "Gradient Descent(52/99): loss =0.3410812904925941, w0=-0.3140757645959051, w1=0.007552358539642741\n",
      "Gradient Descent(53/99): loss =0.34106663874480037, w0=-0.3140747584025865, w1=0.007607076661964558\n",
      "Gradient Descent(54/99): loss =0.3410530917621391, w0=-0.31407376065953, w1=0.007657880268672166\n",
      "Gradient Descent(55/99): loss =0.34104056035638436, w0=-0.3140727756343746, w1=0.007705011193871313\n",
      "Gradient Descent(56/99): loss =0.3410289629016631, w0=-0.31407180652650624, w1=0.007748692619849394\n",
      "Gradient Descent(57/99): loss =0.3410182246658316, w0=-0.3140708557011469, w1=0.007789131003244985\n",
      "Gradient Descent(58/99): loss =0.3410082772034474, w0=-0.31406992487232843, w1=0.007826517763861694\n",
      "Gradient Descent(59/99): loss =0.34099905780439166, w0=-0.3140690152459694, w1=0.007861030767211908\n",
      "Gradient Descent(60/99): loss =0.3409905089928061, w0=-0.31406812763180136, w1=0.007892835627797102\n",
      "Gradient Descent(61/99): loss =0.3409825780715536, w0=-0.3140672625309679, w1=0.007922086856586645\n",
      "Gradient Descent(62/99): loss =0.34097521670788994, w0=-0.3140664202046194, w1=0.007948928873047947\n",
      "Gradient Descent(63/99): loss =0.3409683805564695, w0=-0.3140656007276557, w1=0.007973496899381493\n",
      "Gradient Descent(64/99): loss =0.3409620289161837, w0=-0.314064804030858, w1=0.007995917752257383\n",
      "Gradient Descent(65/99): loss =0.3409561244176804, w0=-0.31406402993393917, w1=0.008016310545309659\n",
      "Gradient Descent(66/99): loss =0.34095063273871545, w0=-0.3140632781714877, w1=0.008034787313868223\n",
      "Gradient Descent(67/99): loss =0.3409455223447633, w0=-0.3140625484133473, w1=0.008051453571872423\n",
      "Gradient Descent(68/99): loss =0.3409407642525625, w0=-0.3140618402806366, w1=0.008066408809576604\n",
      "Gradient Descent(69/99): loss =0.3409363318144943, w0=-0.3140611533583511, w1=0.008079746939506315\n",
      "Gradient Descent(70/99): loss =0.3409322005218893, w0=-0.3140604872052825, w1=0.008091556697124355\n",
      "Gradient Descent(71/99): loss =0.3409283478255419, w0=-0.31405984136183124, w1=0.008101922001804634\n",
      "Gradient Descent(72/99): loss =0.34092475297187197, w0=-0.31405921535616255, w1=0.0081109222829645\n",
      "Gradient Descent(73/99): loss =0.34092139685331974, w0=-0.31405860870905866, w1=0.00811863277556209\n",
      "Gradient Descent(74/99): loss =0.34091826187169344, w0=-0.31405802093774304, w1=0.008125124788607751\n",
      "Gradient Descent(75/99): loss =0.34091533181330697, w0=-0.31405745155889336, w1=0.008130465949856924\n",
      "Gradient Descent(76/99): loss =0.3409125917348547, w0=-0.3140569000910132, w1=0.008134720429436465\n",
      "Gradient Descent(77/99): loss =0.3409100278590658, w0=-0.3140563660562956, w1=0.008137949144796229\n",
      "Gradient Descent(78/99): loss =0.34090762747927, w0=-0.3140558489820831, w1=0.008140209949067263\n",
      "Gradient Descent(79/99): loss =0.34090537887208705, w0=-0.3140553484020079, w1=0.008141557804640135\n",
      "Gradient Descent(80/99): loss =0.3409032712175214, w0=-0.31405486385687564, w1=0.008142044943543428\n",
      "Gradient Descent(81/99): loss =0.34090129452581375, w0=-0.3140543948953457, w1=0.008141721016003585\n",
      "Gradient Descent(82/99): loss =0.3408994395704555, w0=-0.3140539410744472, w1=0.008140633228391968\n",
      "Gradient Descent(83/99): loss =0.34089769782682794, w0=-0.3140535019599641, w1=0.008138826471616272\n",
      "Gradient Descent(84/99): loss =0.3408960614159786, w0=-0.3140530771267145, w1=0.008136343440882\n",
      "Gradient Descent(85/99): loss =0.3408945230530862, w0=-0.3140526661587436, w1=0.008133224747638982\n",
      "Gradient Descent(86/99): loss =0.34089307600021096, w0=-0.31405226864944863, w1=0.0081295090244276\n",
      "Gradient Descent(87/99): loss =0.3408917140229604, w0=-0.3140518842016463, w1=0.0081252330232583\n",
      "Gradient Descent(88/99): loss =0.3408904313507334, w0=-0.3140515124275953, w1=0.008120431708080826\n",
      "Gradient Descent(89/99): loss =0.34088922264023663, w0=-0.31405115294898095, w1=0.0081151383418387\n",
      "Gradient Descent(90/99): loss =0.34088808294199513, w0=-0.3140508053968693, w1=0.008109384568547446\n",
      "Gradient Descent(91/99): loss =0.3408870076696001, w0=-0.31405046941163645, w1=0.008103200490787459\n",
      "Gradient Descent(92/99): loss =0.34088599257146357, w0=-0.3140501446428769, w1=0.008096614742959803\n",
      "Gradient Descent(93/99): loss =0.3408850337048673, w0=-0.31404983074929543, w1=0.008089654560617365\n",
      "Gradient Descent(94/99): loss =0.34088412741211194, w0=-0.3140495273985854, w1=0.008082345846150862\n",
      "Gradient Descent(95/99): loss =0.3408832702985929, w0=-0.31404923426729525, w1=0.008074713231081831\n",
      "Gradient Descent(96/99): loss =0.34088245921263577, w0=-0.31404895104068664, w1=0.008066780135189817\n",
      "Gradient Descent(97/99): loss =0.34088169122695255, w0=-0.3140486774125846, w1=0.008058568822679565\n",
      "Gradient Descent(98/99): loss =0.3408809636215756, w0=-0.3140484130852223, w1=0.008050100455575008\n",
      "Gradient Descent(99/99): loss =0.3408802738681529, w0=-0.3140481577690814, w1=0.008041395144510164\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =0.4841593140141096, w0=-0.156355777203846, w1=0.0028840638226198395\n",
      "Gradient Descent(2/99): loss =0.4503133376400413, w0=-0.19491049182564996, w1=-0.023390890307641733\n",
      "Gradient Descent(3/99): loss =0.38965941222650885, w0=-0.22285931832545555, w1=0.043001763446247454\n",
      "Gradient Descent(4/99): loss =0.370523526215525, w0=-0.24388344229796383, w1=-0.03711551094747663\n",
      "Gradient Descent(5/99): loss =0.36115020444817236, w0=-0.25988191663308174, w1=0.005930105329632278\n",
      "Gradient Descent(6/99): loss =0.3574623181555147, w0=-0.27223351203206664, w1=-0.020333283495161347\n",
      "Gradient Descent(7/99): loss =0.3543653551945443, w0=-0.2817406877692834, w1=-0.007945354243881837\n",
      "Gradient Descent(8/99): loss =0.35111375069539763, w0=-0.28915422867761187, w1=-0.013936438709555996\n",
      "Gradient Descent(9/99): loss =0.3490288715317507, w0=-0.29489211431902973, w1=-0.009944423955528376\n",
      "Gradient Descent(10/99): loss =0.34770224299497304, w0=-0.2993675213699545, w1=-0.010385519520308863\n",
      "Gradient Descent(11/99): loss =0.3467415018609931, w0=-0.30284333367980093, w1=-0.008368301313437198\n",
      "Gradient Descent(12/99): loss =0.34598176589382573, w0=-0.30555148679919447, w1=-0.007403382902099466\n",
      "Gradient Descent(13/99): loss =0.345352820457279, w0=-0.30765681737124856, w1=-0.0058853899805468785\n",
      "Gradient Descent(14/99): loss =0.34481889047598224, w0=-0.30929521666494253, w1=-0.004652816812395898\n",
      "Gradient Descent(15/99): loss =0.34435828169049, w0=-0.3105684383580244, w1=-0.003346859117075096\n",
      "Gradient Descent(16/99): loss =0.343956314853287, w0=-0.3115578185596362, w1=-0.002163703346233158\n",
      "Gradient Descent(17/99): loss =0.343602407652157, w0=-0.31232567558517793, w1=-0.0010255269438300377\n",
      "Gradient Descent(18/99): loss =0.3432886306098468, w0=-0.31292114186751707, w1=2.4694657044687556e-05\n",
      "Gradient Descent(19/99): loss =0.34300887340431807, w0=-0.3133822376157089, w1=0.0010054497225599054\n",
      "Gradient Descent(20/99): loss =0.3427583189071496, w0=-0.3137387532173185, w1=0.0019104433297573195\n",
      "Gradient Descent(21/99): loss =0.34253309335704124, w0=-0.31401383845518044, w1=0.0027468508762238044\n",
      "Gradient Descent(22/99): loss =0.3423300263574952, w0=-0.31422557989905653, w1=0.0035171298973128897\n",
      "Gradient Descent(23/99): loss =0.34214648249701335, w0=-0.3143880557403159, w1=0.004226490901660789\n",
      "Gradient Descent(24/99): loss =0.34198024098462604, w0=-0.314512248844092, w1=0.004879220497972778\n",
      "Gradient Descent(25/99): loss =0.3418294081299727, w0=-0.314606712181183, w1=0.005479958081364112\n",
      "Gradient Descent(26/99): loss =0.3416923526361894, w0=-0.3146781120121519, w1=0.006032996271029775\n",
      "Gradient Descent(27/99): loss =0.3415676569801129, w0=-0.31473163911889235, w1=0.0065424405625733525\n",
      "Gradient Descent(28/99): loss =0.3414540802893381, w0=-0.3147713360221172, w1=0.007012096419273065\n",
      "Gradient Descent(29/99): loss =0.34135052955147166, w0=-0.3148003488857145, w1=0.007445475616837251\n",
      "Gradient Descent(30/99): loss =0.3412560369420245, w0=-0.3148211257185611, w1=0.007845795153697583\n",
      "Gradient Descent(31/99): loss =0.3411697417071882, w0=-0.3148355700714968, w1=0.008215982328856784\n",
      "Gradient Descent(32/99): loss =0.34109087548264494, w0=-0.3148451613641686, w1=0.0085586942827037\n",
      "Gradient Descent(33/99): loss =0.3410187502396504, w0=-0.3148510484826961, w1=0.008876332830646457\n",
      "Gradient Descent(34/99): loss =0.3409527482668639, w0=-0.31485412288685155, w1=0.009171066687177485\n",
      "Gradient Descent(35/99): loss =0.3408923137506924, w0=-0.3148550755475479, w1=0.009444850191507924\n",
      "Gradient Descent(36/99): loss =0.34083694562725575, w0=-0.31485444136812246, w1=0.009699444033827945\n",
      "Gradient Descent(37/99): loss =0.3407861914588742, w0=-0.31485263379371914, w1=0.00993643362177273\n",
      "Gradient Descent(38/99): loss =0.3407396421461388, w0=-0.31484997179398444, w1=0.010157247095919412\n",
      "Gradient Descent(39/99): loss =0.34069692732944384, w0=-0.3148467008847884, w1=0.01036317150339952\n",
      "Gradient Descent(40/99): loss =0.34065771136563155, w0=-0.3148430095085339, w1=0.010555367840434902\n",
      "Gradient Descent(41/99): loss =0.3406216897892382, w0=-0.31483904179195715, w1=0.010734884542803198\n",
      "Gradient Descent(42/99): loss =0.34058858618583326, w0=-0.3148349074815613, w1=0.010902669717541414\n",
      "Gradient Descent(43/99): loss =0.3405581494187242, w0=-0.3148306896779795, w1=0.011059582061252423\n",
      "Gradient Descent(44/99): loss =0.3405301511609085, w0=-0.31482645085526395, w1=0.011206400630542629\n",
      "Gradient Descent(45/99): loss =0.34050438369244085, w0=-0.3148222375433607, w1=0.011343833523542108\n",
      "Gradient Descent(46/99): loss =0.34048065792990073, w0=-0.3148180839691181, w1=0.011472525593593718\n",
      "Gradient Descent(47/99): loss =0.34045880165983916, w0=-0.31481401488591615, w1=0.01159306528018477\n",
      "Gradient Descent(48/99): loss =0.340438657952266, w0=-0.3148100477714012, w1=0.011705990655095972\n",
      "Gradient Descent(49/99): loss =0.34042008373363125, w0=-0.31480619453318887, w1=0.011811794765825185\n",
      "Gradient Descent(50/99): loss =0.3404029485015571, w0=-0.31480246283157526, w1=0.011910930356567598\n",
      "Gradient Descent(51/99): loss =0.3403871331658873, w0=-0.3147988571042192, w1=0.012003814037179995\n",
      "Gradient Descent(52/99): loss =0.34037252900257053, w0=-0.3147953793590043, w1=0.012090829965032393\n",
      "Gradient Descent(53/99): loss =0.3403590367085336, w0=-0.31479202978665655, w1=0.012172333097194996\n",
      "Gradient Descent(54/99): loss =0.3403465655471005, w0=-0.314788807233289, w1=0.012248652064564892\n",
      "Gradient Descent(55/99): loss =0.3403350325747167, w0=-0.3147857095641562, w1=0.01232009171354776\n",
      "Gradient Descent(56/99): loss =0.3403243619407752, w0=-0.3147827339429683, w1=0.012386935355758386\n",
      "Gradient Descent(57/99): loss =0.3403144842532424, w0=-0.31477987704571775, w1=0.012449446761384472\n",
      "Gradient Descent(58/99): loss =0.3403053360035743, w0=-0.31477713522376183, w1=0.012507871927615098\n",
      "Gradient Descent(59/99): loss =0.34029685904510154, w0=-0.3147745046276271, w1=0.012562440649705194\n",
      "Gradient Descent(60/99): loss =0.34028900011967206, w0=-0.31477198130044864, w1=0.0126133679188677\n",
      "Gradient Descent(61/99): loss =0.3402817104278838, w0=-0.31476956124796845, w1=0.012660855168180685\n",
      "Gradient Descent(62/99): loss =0.34027494523871193, w0=-0.31476724049047083, w1=0.01270509138505446\n",
      "Gradient Descent(63/99): loss =0.3402686635347622, w0=-0.3147650151008266, w1=0.012746254106474345\n",
      "Gradient Descent(64/99): loss =0.3402628276897628, w0=-0.3147628812318818, w1=0.012784510311192024\n",
      "Gradient Descent(65/99): loss =0.34025740317524195, w0=-0.31476083513569764, w1=0.012820017221246502\n",
      "Gradient Descent(66/99): loss =0.3402523582936414, w0=-0.3147588731765816, w1=0.012852923023629507\n",
      "Gradient Descent(67/99): loss =0.3402476639353875, w0=-0.31475699183940936, w1=0.012883367521540342\n",
      "Gradient Descent(68/99): loss =0.3402432933576806, w0=-0.3147551877343955, w1=0.012911482723478926\n",
      "Gradient Descent(69/99): loss =0.3402392219829867, w0=-0.3147534575992052, w1=0.012937393377383722\n",
      "Gradient Descent(70/99): loss =0.3402354272154045, w0=-0.314751798299094, w1=0.012961217456111116\n",
      "Gradient Descent(71/99): loss =0.34023188827325984, w0=-0.3147502068256018, w1=0.012983066599761107\n",
      "Gradient Descent(72/99): loss =0.3402285860364374, w0=-0.31474868029420544, w1=0.013003046519663887\n",
      "Gradient Descent(73/99): loss =0.3402255029071016, w0=-0.3147472159412374, w1=0.01302125736824132\n",
      "Gradient Descent(74/99): loss =0.34022262268258385, w0=-0.3147458111203043, w1=0.013037794078433888\n",
      "Gradient Descent(75/99): loss =0.3402199304393351, w0=-0.31474446329838324, w1=0.01305274667592856\n",
      "Gradient Descent(76/99): loss =0.3402174124269396, w0=-0.31474317005172775, w1=0.013066200567026245\n",
      "Gradient Descent(77/99): loss =0.3402150559712841, w0=-0.3147419290616843, w1=0.013078236804643317\n",
      "Gradient Descent(78/99): loss =0.3402128493860607, w0=-0.3147407381104915, w1=0.013088932334638792\n",
      "Gradient Descent(79/99): loss =0.34021078189185866, w0=-0.31473959507711674, w1=0.013098360224399834\n",
      "Gradient Descent(80/99): loss =0.3402088435421655, w0=-0.31473849793316755, w1=0.013106589875387539\n",
      "Gradient Descent(81/99): loss =0.3402070251556681, w0=-0.31473744473890625, w1=0.013113687221146744\n",
      "Gradient Descent(82/99): loss =0.34020531825429323, w0=-0.3147364336393845, w1=0.013119714912111056\n",
      "Gradient Descent(83/99): loss =0.340203715006483, w0=-0.3147354628607112, w1=0.013124732488380452\n",
      "Gradient Descent(84/99): loss =0.3402022081752451, w0=-0.3147345307064585, w1=0.013128796541518897\n",
      "Gradient Descent(85/99): loss =0.3402007910705587, w0=-0.31473363555421147, w1=0.013131960866302196\n",
      "Gradient Descent(86/99): loss =0.3401994575057595, w0=-0.3147327758522599, w1=0.013134276603245027\n",
      "Gradient Descent(87/99): loss =0.3401982017575537, w0=-0.31473195011643174, w1=0.01313579237264884\n",
      "Gradient Descent(88/99): loss =0.3401970185293507, w0=-0.31473115692706555, w1=0.013136554400832197\n",
      "Gradient Descent(89/99): loss =0.34019590291762564, w0=-0.31473039492611754, w1=0.013136606639139925\n",
      "Gradient Descent(90/99): loss =0.34019485038105207, w0=-0.31472966281439935, w1=0.013135990876264476\n",
      "Gradient Descent(91/99): loss =0.34019385671216845, w0=-0.3147289593489414, w1=0.013134746844362178\n",
      "Gradient Descent(92/99): loss =0.34019291801136076, w0=-0.3147282833404775, w1=0.013132912319400203\n",
      "Gradient Descent(93/99): loss =0.3401920306629653, w0=-0.31472763365104434, w1=0.013130523216128042\n",
      "Gradient Descent(94/99): loss =0.3401911913133109, w0=-0.3147270091916925, w1=0.013127613678032307\n",
      "Gradient Descent(95/99): loss =0.34019039685053803, w0=-0.31472640892030246, w1=0.013124216162601099\n",
      "Gradient Descent(96/99): loss =0.340189644386045, w0=-0.3147258318395017, w1=0.013120361522194946\n",
      "Gradient Descent(97/99): loss =0.34018893123742366, w0=-0.31472527699467767, w1=0.013116079080797758\n",
      "Gradient Descent(98/99): loss =0.34018825491276117, w0=-0.31472474347208246, w1=0.013111396706896384\n",
      "Gradient Descent(99/99): loss =0.3401876130961949, w0=-0.3147242303970244, w1=0.013106340882719369\n",
      "Optimizing degree 2/15, model: least_squares_GD, arguments: {'max_iters': 100}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =7.839059935623126, w0=-0.0026156943882950368, w1=0.19999316275092116\n",
      "Gradient Descent(2/99): loss =8.75329449924172, w0=-0.049107858033846194, w1=0.16678973987966308\n",
      "Gradient Descent(3/99): loss =1.8621114635202523, w0=-0.022850700072873317, w1=0.1726996630256814\n",
      "Gradient Descent(4/99): loss =0.7383762858533274, w0=-0.02701458582923431, w1=0.15578907023023617\n",
      "Gradient Descent(5/99): loss =0.5662999384979516, w0=-0.021923980358801984, w1=0.1486280847753041\n",
      "Gradient Descent(6/99): loss =0.5156944373733406, w0=-0.021017199928453953, w1=0.14030139119986393\n",
      "Gradient Descent(7/99): loss =0.48810568484290595, w0=-0.019649961929925353, w1=0.1341529768138526\n",
      "Gradient Descent(8/99): loss =0.46901730367531697, w0=-0.01913208615736729, w1=0.12872586535326513\n",
      "Gradient Descent(9/99): loss =0.45474042160114586, w0=-0.018869814395055542, w1=0.12420143984865449\n",
      "Gradient Descent(10/99): loss =0.4436061254789799, w0=-0.01892850243342921, w1=0.12029547107675036\n",
      "Gradient Descent(11/99): loss =0.4346440702968133, w0=-0.019188281057969252, w1=0.11693350232873535\n",
      "Gradient Descent(12/99): loss =0.4272391938952367, w0=-0.019614762596568268, w1=0.11401174841549663\n",
      "Gradient Descent(13/99): loss =0.4209853862663078, w0=-0.020163763643151724, w1=0.11146559970743769\n",
      "Gradient Descent(14/99): loss =0.4156060227675358, w0=-0.02080843635664514, w1=0.10923848514048146\n",
      "Gradient Descent(15/99): loss =0.41090740269902987, w0=-0.02152654929090058, w1=0.10728697178545459\n",
      "Gradient Descent(16/99): loss =0.40675037679517834, w0=-0.02230213125513223, w1=0.10557501244815369\n",
      "Gradient Descent(17/99): loss =0.40303259616363574, w0=-0.023122874138783066, w1=0.10407312949895886\n",
      "Gradient Descent(18/99): loss =0.3996771318101973, w0=-0.023979542503003444, w1=0.10275660798362515\n",
      "Gradient Descent(19/99): loss =0.3966250094052997, w0=-0.024865060837867405, w1=0.10160460676562937\n",
      "Gradient Descent(20/99): loss =0.39383020286708625, w0=-0.02577401241522336, w1=0.10059932131799326\n",
      "Gradient Descent(21/99): loss =0.3912562055125026, w0=-0.026702202581713557, w1=0.0997254163385572\n",
      "Gradient Descent(22/99): loss =0.3888736344128424, w0=-0.027646359937290702, w1=0.09896956455039031\n",
      "Gradient Descent(23/99): loss =0.386658525058665, w0=-0.028603904239552336, w1=0.0983201006651155\n",
      "Gradient Descent(24/99): loss =0.38459109610457165, w0=-0.029572777592395044, w1=0.09776674607327517\n",
      "Gradient Descent(25/99): loss =0.38265484006955297, w0=-0.030551316854735652, w1=0.09730039297893242\n",
      "Gradient Descent(26/99): loss =0.3808358439427492, w0=-0.031538159002259085, w1=0.09691293090297175\n",
      "Gradient Descent(27/99): loss =0.37912227455878444, w0=-0.03253217001478434, w1=0.09659710672065247\n",
      "Gradient Descent(28/99): loss =0.37750398383131095, w0=-0.03353239171910362, w1=0.09634641006475483\n",
      "Gradient Descent(29/99): loss =0.3759722023872525, w0=-0.034538001854355356, w1=0.09615497869395927\n",
      "Gradient Descent(30/99): loss =0.3745192992385789, w0=-0.03554828410100614, w1=0.0960175194593783\n",
      "Gradient Descent(31/99): loss =0.3731385913710672, w0=-0.036562605532312206, w1=0.09592924169663718\n",
      "Gradient Descent(32/99): loss =0.3718241914777759, w0=-0.03758039964063864, w1=0.09588580056375463\n",
      "Gradient Descent(33/99): loss =0.3705708851360639, w0=-0.03860115353521199, w1=0.09588324844453347\n",
      "Gradient Descent(34/99): loss =0.3693740309246791, w0=-0.03962439826978405, w1=0.09591799294834454\n",
      "Gradient Descent(35/99): loss =0.36822947856958255, w0=-0.04064970151502547, w1=0.09598676036300258\n",
      "Gradient Descent(36/99): loss =0.3671335013739608, w0=-0.04167666198824831, w1=0.09608656365673683\n",
      "Gradient Descent(37/99): loss =0.3660827400521508, w0=-0.0427049051985624, w1=0.0962146743108104\n",
      "Gradient Descent(38/99): loss =0.3650741557337672, w0=-0.04373408017596334, w1=0.09636859740520445\n",
      "Gradient Descent(39/99): loss =0.36410499039248717, w0=-0.04476385693528622, w1=0.09654604948922944\n",
      "Gradient Descent(40/99): loss =0.36317273332564776, w0=-0.04579392448821304, w1=0.09674493885380675\n",
      "Gradient Descent(41/99): loss =0.3622750925960749, w0=-0.046823989263290185, w1=0.09696334788885669\n",
      "Gradient Descent(42/99): loss =0.3614099705680758, w0=-0.047853773829184806, w1=0.09719951726189954\n",
      "Gradient Descent(43/99): loss =0.36057544284115683, w0=-0.04888301584295818, w1=0.09745183169598837\n",
      "Gradient Descent(44/99): loss =0.3597697400194523, w0=-0.049911467165133, w1=0.097718807158861\n",
      "Gradient Descent(45/99): loss =0.3589912318607727, w0=-0.05093889309837366, w1=0.0979990793025898\n",
      "Gradient Descent(46/99): loss =0.3582384134331153, w0=-0.05196507171790121, w1=0.09829139301541528\n",
      "Gradient Descent(47/99): loss =0.35750989297334634, w0=-0.052989793270238846, w1=0.09859459296594207\n",
      "Gradient Descent(48/99): loss =0.3568043811963269, w0=-0.054012859623221436, w1=0.0989076150352675\n",
      "Gradient Descent(49/99): loss =0.3561206818458572, w0=-0.05503408375492592, w1=0.09922947854553062\n",
      "Gradient Descent(50/99): loss =0.3554576833136855, w0=-0.05605328927268386, w1=0.09955927920429852\n",
      "Gradient Descent(51/99): loss =0.3548143511811677, w0=-0.05707030995592364, w1=0.09989618269352311\n",
      "Gradient Descent(52/99): loss =0.3541897215613066, w0=-0.05808498931848476, w1=0.1002394188398002\n",
      "Gradient Descent(53/99): loss =0.35358289513788344, w0=-0.059097180187423146, w1=0.1005882763095768\n",
      "Gradient Descent(54/99): loss =0.3529930318140404, w0=-0.06010674429631461, w1=0.1009420977789641\n",
      "Gradient Descent(55/99): loss =0.3524193458956281, w0=-0.06111355189176397, w1=0.10130027553307111\n",
      "Gradient Descent(56/99): loss =0.3518611017453944, w0=-0.062117481352312626, w1=0.10166224745439435\n",
      "Gradient Descent(57/99): loss =0.35131760985309196, w0=-0.06311841881926657, w1=0.10202749336387762\n",
      "Gradient Descent(58/99): loss =0.3507882232741185, w0=-0.06411625783918048, w1=0.10239553168187161\n",
      "Gradient Descent(59/99): loss =0.3502723343956601, w0=-0.06511089901786557, w1=0.10276591637943824\n",
      "Gradient Descent(60/99): loss =0.34976937199467656, w0=-0.06610224968586104, w1=0.10313823419331387\n",
      "Gradient Descent(61/99): loss =0.3492787985566309, w0=-0.06709022357534178, w1=0.1035121020804117\n",
      "Gradient Descent(62/99): loss =0.34880010782775417, w0=-0.068074740508439, w1=0.10388716489004482\n",
      "Gradient Descent(63/99): loss =0.34833282257695686, w0=-0.06905572609693704, w1=0.10426309323411875\n",
      "Gradient Descent(64/99): loss =0.34787649254636416, w0=-0.07003311145328578, w1=0.10463958153740255\n",
      "Gradient Descent(65/99): loss =0.3474306925719056, w0=-0.07100683291283702, w1=0.10501634625166424\n",
      "Gradient Descent(66/99): loss =0.3469950208575221, w0=-0.07197683176718149, w1=0.10539312421896922\n",
      "Gradient Descent(67/99): loss =0.3465690973884021, w0=-0.07294305400843018, w1=0.10576967117080667\n",
      "Gradient Descent(68/99): loss =0.34615256247026266, w0=-0.07390545008425314, w1=0.10614576035094415\n",
      "Gradient Descent(69/99): loss =0.34574507538309557, w0=-0.07486397466346102, w1=0.1065211812510285\n",
      "Gradient Descent(70/99): loss =0.3453463131390307, w0=-0.07581858641188956, w1=0.10689573844896216\n",
      "Gradient Descent(71/99): loss =0.3449559693350421, w0=-0.07676924777832658, w1=0.10726925054100082\n",
      "Gradient Descent(72/99): loss =0.34457375309217686, w0=-0.07771592479020309, w1=0.10764154915934786\n",
      "Gradient Descent(73/99): loss =0.3441993880738217, w0=-0.07865858685875618, w1=0.10801247806777375\n",
      "Gradient Descent(74/99): loss =0.34383261157626716, w0=-0.07959720659336099, w1=0.10838189232847055\n",
      "Gradient Descent(75/99): loss =0.3434731736854816, w0=-0.08053175962472085, w1=0.10874965753396985\n",
      "Gradient Descent(76/99): loss =0.3431208364945981, w0=-0.08146222443660034, w1=0.10911564909851422\n",
      "Gradient Descent(77/99): loss =0.34277537337713176, w0=-0.08238858220578321, w1=0.10947975160378023\n",
      "Gradient Descent(78/99): loss =0.3424365683114135, w0=-0.08331081664993732, w1=0.10984185819431358\n",
      "Gradient Descent(79/99): loss =0.34210421525213874, w0=-0.08422891388306995, w1=0.11020187001845527\n",
      "Gradient Descent(80/99): loss =0.34177811754530385, w0=-0.08514286227826073, w1=0.11055969571091841\n",
      "Gradient Descent(81/99): loss =0.3414580873831339, w0=-0.08605265233736352, w1=0.11091525091351982\n",
      "Gradient Descent(82/99): loss =0.34114394529590886, w0=-0.08695827656737508, w1=0.11126845783088433\n",
      "Gradient Descent(83/99): loss =0.3408355196778616, w0=-0.08785972936317459, w1=0.1116192448182235\n",
      "Gradient Descent(84/99): loss =0.3405326463445665, w0=-0.08875700689634614, w1=0.11196754599854883\n",
      "Gradient Descent(85/99): loss =0.3402351681194582, w0=-0.08965010700980407, w1=0.11231330090691415\n",
      "Gradient Descent(86/99): loss =0.3399429344473154, w0=-0.09053902911794998, w1=0.11265645415949402\n",
      "Gradient Descent(87/99): loss =0.3396558010327295, w0=-0.09142377411209898, w1=0.11299695514549904\n",
      "Gradient Descent(88/99): loss =0.3393736295017357, w0=-0.09230434427092211, w1=0.11333475774010399\n",
      "Gradient Descent(89/99): loss =0.33909628708493583, w0=-0.09318074317566094, w1=0.11366982003672434\n",
      "Gradient Descent(90/99): loss =0.3388236463205772, w0=-0.09405297562987995, w1=0.1140021040971218\n",
      "Gradient Descent(91/99): loss =0.3385555847761706, w0=-0.09492104758353154, w1=0.11433157571795097\n",
      "Gradient Descent(92/99): loss =0.33829198478734623, w0=-0.09578496606111754, w1=0.11465820421247887\n",
      "Gradient Descent(93/99): loss =0.33803273321274685, w0=-0.09664473909374073, w1=0.11498196220631837\n",
      "Gradient Descent(94/99): loss =0.33777772120384764, w0=-0.09750037565484837, w1=0.11530282544611489\n",
      "Gradient Descent(95/99): loss =0.33752684398868377, w0=-0.09835188559947874, w1=0.1156207726202161\n",
      "Gradient Descent(96/99): loss =0.33728000066853553, w0=-0.09919927960683025, w1=0.11593578519043617\n",
      "Gradient Descent(97/99): loss =0.3370370940267008, w0=-0.1000425691259809, w1=0.11624784723410078\n",
      "Gradient Descent(98/99): loss =0.33679803034854416, w0=-0.10088176632459392, w1=0.1165569452956265\n",
      "Gradient Descent(99/99): loss =0.33656271925207265, w0=-0.1017168840404535, w1=0.1168630682469504\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =23.218819551185142, w0=-0.0020510006922389717, w1=0.1999930949740934\n",
      "Gradient Descent(2/99): loss =1.7069669301591468, w0=-0.03396607658744928, w1=0.1690559486179855\n",
      "Gradient Descent(3/99): loss =0.5726444386439423, w0=-0.024211784986175496, w1=0.17004295386238566\n",
      "Gradient Descent(4/99): loss =0.46677094460534024, w0=-0.02149860327065887, w1=0.16105178656120314\n",
      "Gradient Descent(5/99): loss =0.4225764690527554, w0=-0.018797038362096615, w1=0.15357027736961631\n",
      "Gradient Descent(6/99): loss =0.3948155370765971, w0=-0.01711676744769795, w1=0.14669099945660716\n",
      "Gradient Descent(7/99): loss =0.3770714648911897, w0=-0.01604159185094181, w1=0.14078531139132835\n",
      "Gradient Descent(8/99): loss =0.3654080625309092, w0=-0.015486911312984684, w1=0.1355541388225885\n",
      "Gradient Descent(9/99): loss =0.3573863205854477, w0=-0.01530260610356254, w1=0.13095865431315462\n",
      "Gradient Descent(10/99): loss =0.35163854679842077, w0=-0.015411619329248444, w1=0.12685950666861862\n",
      "Gradient Descent(11/99): loss =0.3473749456496453, w0=-0.015736799716261716, w1=0.12320553184956098\n",
      "Gradient Descent(12/99): loss =0.3441163310846299, w0=-0.016230261261968675, w1=0.11992655950010864\n",
      "Gradient Descent(13/99): loss =0.3415598967617122, w0=-0.016850916001337816, w1=0.11698229700198702\n",
      "Gradient Descent(14/99): loss =0.3395079072341074, w0=-0.01757069395290656, w1=0.11433108480252545\n",
      "Gradient Descent(15/99): loss =0.3378275281254208, w0=-0.018367004189139533, w1=0.11194259574569443\n",
      "Gradient Descent(16/99): loss =0.3364271959131066, w0=-0.019223520921392588, w1=0.10978857130658752\n",
      "Gradient Descent(17/99): loss =0.33524226230563003, w0=-0.0201274165319058, w1=0.10784590653063736\n",
      "Gradient Descent(18/99): loss =0.33422604626718566, w0=-0.021069085967963773, w1=0.10609374718268237\n",
      "Gradient Descent(19/99): loss =0.3333441297443167, w0=-0.02204101603550966, w1=0.10451408751191944\n",
      "Gradient Descent(20/99): loss =0.3325706446703583, w0=-0.02303743940804837, w1=0.10309076137928837\n",
      "Gradient Descent(21/99): loss =0.3318858072558858, w0=-0.024053819523054847, w1=0.10180944515275785\n",
      "Gradient Descent(22/99): loss =0.33127424831203583, w0=-0.025086603127016188, w1=0.10065724512213074\n",
      "Gradient Descent(23/99): loss =0.33072386080406224, w0=-0.026132962825704698, w1=0.09962258870199718\n",
      "Gradient Descent(24/99): loss =0.3302249894245567, w0=-0.027190641398268895, w1=0.09869501804674356\n",
      "Gradient Descent(25/99): loss =0.3297698502795603, w0=-0.028257813949403906, w1=0.09786508180996797\n",
      "Gradient Descent(26/99): loss =0.32935210807703186, w0=-0.02933299383820917, w1=0.09712421079538955\n",
      "Gradient Descent(27/99): loss =0.32896656296919424, w0=-0.030414955330757935, w1=0.0964646296939365\n",
      "Gradient Descent(28/99): loss =0.3286089150274391, w0=-0.03150267736304122, w1=0.09587927159202025\n",
      "Gradient Descent(29/99): loss =0.3282755845875087, w0=-0.032595298794314775, w1=0.09536170851278146\n",
      "Gradient Descent(30/99): loss =0.3279635734442593, w0=-0.0336920847721315, w1=0.09490608779431149\n",
      "Gradient Descent(31/99): loss =0.3276703563668851, w0=-0.03479240032749299, w1=0.09450707723943848\n",
      "Gradient Descent(32/99): loss =0.3273937954395894, w0=-0.035895690170821636, w1=0.09415981561080168\n",
      "Gradient Descent(33/99): loss =0.32713207181121823, w0=-0.03700146290390246, w1=0.09385986880221653\n",
      "Gradient Descent(34/99): loss =0.32688363088162453, w0=-0.03810927881844595, w1=0.0936031903371955\n",
      "Gradient Descent(35/99): loss =0.32664713797055095, w0=-0.03921874036690496, w1=0.09338608595415374\n",
      "Gradient Descent(36/99): loss =0.3264214422427579, w0=-0.040329484751591926, w1=0.09320518162273647\n",
      "Gradient Descent(37/99): loss =0.32620554719105815, w0=-0.041441178127579345, w1=0.09305739468404009\n",
      "Gradient Descent(38/99): loss =0.3259985863671306, w0=-0.04255351107109815, w1=0.09293990772489347\n",
      "Gradient Descent(39/99): loss =0.32579980333923625, w0=-0.04366619502225998, w1=0.09285014491965338\n",
      "Gradient Descent(40/99): loss =0.3256085350741836, w0=-0.044778959487878786, w1=0.0927857505720355\n",
      "Gradient Descent(41/99): loss =0.32542419810743295, w0=-0.04589154983208143, w1=0.09274456964216297\n",
      "Gradient Descent(42/99): loss =0.3252462769936835, w0=-0.04700372552401755, w1=0.09272463005940172\n",
      "Gradient Descent(43/99): loss =0.32507431463029723, w0=-0.04811525873939268, w1=0.092724126649593\n",
      "Gradient Descent(44/99): loss =0.32490790412445636, w0=-0.049225933236404784, w1=0.09274140652119486\n",
      "Gradient Descent(45/99): loss =0.3247466819370858, w0=-0.050335543443876765, w1=0.09277495577278899\n",
      "Gradient Descent(46/99): loss =0.3245903220860575, w0=-0.0514438937134978, w1=0.0928233873977649\n",
      "Gradient Descent(47/99): loss =0.3244385312308379, w0=-0.0525507976987217, w1=0.09288543027489155\n",
      "Gradient Descent(48/99): loss =0.3242910444926528, w0=-0.053656077831364196, w1=0.09295991914423779\n",
      "Gradient Descent(49/99): loss =0.3241476218900585, w0=-0.0547595648734642, w1=0.09304578547777784\n",
      "Gradient Descent(50/99): loss =0.3240080452907689, w0=-0.05586109752711659, w1=0.09314204916264524\n",
      "Gradient Descent(51/99): loss =0.32387211579766567, w0=-0.05696052208896383, w1=0.09324781092281367\n",
      "Gradient Descent(52/99): loss =0.3237396515008849, w0=-0.05805769213914781, w1=0.09336224541194833\n",
      "Gradient Descent(53/99): loss =0.32361048553932076, w0=-0.05915246825693651, w1=0.09348459491646702\n",
      "Gradient Descent(54/99): loss =0.323484464424298, w0=-0.060244717757116296, w1=0.09361416361351277\n",
      "Gradient Descent(55/99): loss =0.3233614465859261, w0=-0.06133431444269093, w1=0.09375031233366167\n",
      "Gradient Descent(56/99): loss =0.32324130110905364, w0=-0.06242113837054824, w1=0.09389245378281756\n",
      "Gradient Descent(57/99): loss =0.32312390663105095, w0=-0.06350507562761475, w1=0.09404004818193644\n",
      "Gradient Descent(58/99): loss =0.3230091503780532, w0=-0.06458601811567696, w1=0.09419259928701959\n",
      "Gradient Descent(59/99): loss =0.32289692731995845, w0=-0.06566386334354762, w1=0.09434965075525553\n",
      "Gradient Descent(60/99): loss =0.32278713942752807, w0=-0.06673851422563232, w1=0.09451078282631184\n",
      "Gradient Descent(61/99): loss =0.32267969501748617, w0=-0.06780987888623467, w1=0.0946756092906088\n",
      "Gradient Descent(62/99): loss =0.32257450817364824, w0=-0.06887787046914659, w1=0.09484377471897612\n",
      "Gradient Descent(63/99): loss =0.3224714982338954, w0=-0.06994240695222234, w1=0.09501495193042585\n",
      "Gradient Descent(64/99): loss =0.32237058933431706, w0=-0.0710034109667443, w1=0.09518883967689101\n",
      "Gradient Descent(65/99): loss =0.32227171000310423, w0=-0.07206080962146391, w1=0.09536516052570194\n",
      "Gradient Descent(66/99): loss =0.32217479279784905, w0=-0.07311453433125278, w1=0.09554365892231648\n",
      "Gradient Descent(67/99): loss =0.3220797739808005, w0=-0.07416452065033101, w1=0.09572409941740533\n",
      "Gradient Descent(68/99): loss =0.3219865932273948, w0=-0.07521070811005905, w1=0.09590626504383229\n",
      "Gradient Descent(69/99): loss =0.3218951933640251, w0=-0.07625304006128791, w1=0.09608995583037601\n",
      "Gradient Descent(70/99): loss =0.3218055201315635, w0=-0.07729146352126452, w1=0.09627498744022656\n",
      "Gradient Descent(71/99): loss =0.32171752197162196, w0=-0.07832592902508569, w1=0.09646118992336802\n",
      "Gradient Descent(72/99): loss =0.321631149832933, w0=-0.07935639048168806, w1=0.09664840657293744\n",
      "Gradient Descent(73/99): loss =0.32154635699557904, w0=-0.08038280503435295, w1=0.09683649287653945\n",
      "Gradient Descent(74/99): loss =0.3214630989110874, w0=-0.08140513292569557, w1=0.0970253155543044\n",
      "Gradient Descent(75/99): loss =0.321381333056664, w0=-0.08242333736709859, w1=0.0972147516762111\n",
      "Gradient Descent(76/99): loss =0.3213010188020516, w0=-0.08343738441254032, w1=0.0974046878518629\n",
      "Gradient Descent(77/99): loss =0.32122211728768774, w0=-0.08444724283675852, w1=0.09759501948651116\n",
      "Gradient Descent(78/99): loss =0.3211445913129963, w0=-0.08545288401768242, w1=0.09778565009767151\n",
      "Gradient Descent(79/99): loss =0.32106840523378755, w0=-0.08645428182305807, w1=0.0979764906871787\n",
      "Gradient Descent(80/99): loss =0.3209935248678635, w0=-0.08745141250118486, w1=0.09816745916398063\n",
      "Gradient Descent(81/99): loss =0.3209199174080248, w0=-0.08844425457567565, w1=0.0983584798133864\n",
      "Gradient Descent(82/99): loss =0.3208475513417741, w0=-0.0894327887441477, w1=0.09854948280885904\n",
      "Gradient Descent(83/99): loss =0.32077639637708577, w0=-0.09041699778074748, w1=0.09874040376278617\n",
      "Gradient Descent(84/99): loss =0.32070642337368066, w0=-0.0913968664424093, w1=0.09893118331297271\n",
      "Gradient Descent(85/99): loss =0.3206376042793096, w0=-0.09237238137874497, w1=0.09912176674188343\n",
      "Gradient Descent(86/99): loss =0.3205699120705984, w0=-0.09334353104546025, w1=0.09931210362592055\n",
      "Gradient Descent(87/99): loss =0.3205033206980568, w0=-0.09431030562119233, w1=0.09950214751225649\n",
      "Gradient Descent(88/99): loss =0.3204378050348925, w0=-0.09527269692766219, w1=0.09969185562095541\n",
      "Gradient Descent(89/99): loss =0.3203733408293114, w0=-0.09623069835303578, w1=0.09988118857031175\n",
      "Gradient Descent(90/99): loss =0.32030990466001275, w0=-0.0971843047783882, w1=0.10007011012351116\n",
      "Gradient Descent(91/99): loss =0.3202474738946209, w0=-0.09813351250716587, w1=0.10025858695488091\n",
      "Gradient Descent(92/99): loss =0.32018602665081564, w0=-0.09907831919754308, w1=0.10044658843414378\n",
      "Gradient Descent(93/99): loss =0.3201255417599522, w0=-0.10001872379757051, w1=0.10063408642722398\n",
      "Gradient Descent(94/99): loss =0.3200659987329721, w0=-0.1009547264830154, w1=0.10082105511227563\n",
      "Gradient Descent(95/99): loss =0.32000737772843635, w0=-0.10188632859779453, w1=0.10100747080971637\n",
      "Gradient Descent(96/99): loss =0.3199496595225161, w0=-0.10281353259690386, w1=0.10119331182515026\n",
      "Gradient Descent(97/99): loss =0.319892825480798, w0=-0.10373634199175039, w1=0.10137855830415739\n",
      "Gradient Descent(98/99): loss =0.3198368575317726, w0=-0.10465476129779475, w1=0.10156319209801211\n",
      "Gradient Descent(99/99): loss =0.3197817381418818, w0=-0.10556879598441499, w1=0.10174719663946999\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =5.578057212932534, w0=-0.002047111290296444, w1=0.19999399172225774\n",
      "Gradient Descent(2/99): loss =1.7141143981388647, w0=-0.03389075983595424, w1=0.1696272482543819\n",
      "Gradient Descent(3/99): loss =0.6339522592005832, w0=-0.01886759405924081, w1=0.1694274386845904\n",
      "Gradient Descent(4/99): loss =0.47481818301930356, w0=-0.019030693047147003, w1=0.15700158285496552\n",
      "Gradient Descent(5/99): loss =0.423130454478631, w0=-0.016324825882761535, w1=0.14953121687621543\n",
      "Gradient Descent(6/99): loss =0.39353447333673064, w0=-0.01548409148260912, w1=0.14252124693016052\n",
      "Gradient Descent(7/99): loss =0.37521297483373234, w0=-0.014925985162894216, w1=0.13695441447300577\n",
      "Gradient Descent(8/99): loss =0.363355417013044, w0=-0.014906284270334658, w1=0.13217694908123329\n",
      "Gradient Descent(9/99): loss =0.3552831313742584, w0=-0.01518100497516602, w1=0.12813488390115982\n",
      "Gradient Descent(10/99): loss =0.3495566098309737, w0=-0.01570272265927653, w1=0.12464300303786183\n",
      "Gradient Descent(11/99): loss =0.3453567299343533, w0=-0.016390733272057458, w1=0.12161911423739591\n",
      "Gradient Descent(12/99): loss =0.3421880019492587, w0=-0.01720330932638111, w1=0.11897775327046112\n",
      "Gradient Descent(13/99): loss =0.33973687900227195, w0=-0.018103350456262445, w1=0.1166630022405215\n",
      "Gradient Descent(14/99): loss =0.33779804584800505, w0=-0.019067202789988566, w1=0.11462576371985761\n",
      "Gradient Descent(15/99): loss =0.3362332452003369, w0=-0.02007643842702523, w1=0.11282889326736716\n",
      "Gradient Descent(16/99): loss =0.33494709539375606, w0=-0.021118506822360404, w1=0.11124081859899307\n",
      "Gradient Descent(17/99): loss =0.33387235786344166, w0=-0.02218405773906319, w1=0.1098359249242065\n",
      "Gradient Descent(18/99): loss =0.3329606981340034, w0=-0.0232665457975421, w1=0.10859237580017182\n",
      "Gradient Descent(19/99): loss =0.33217674702989075, w0=-0.024361189273504835, w1=0.10749175603284514\n",
      "Gradient Descent(20/99): loss =0.33149419856264556, w0=-0.02546460044112696, w1=0.10651817917719228\n",
      "Gradient Descent(21/99): loss =0.33089319430617165, w0=-0.026574329935172433, w1=0.1056579322066511\n",
      "Gradient Descent(22/99): loss =0.3303585375814156, w0=-0.027688634092616255, w1=0.10489904142066886\n",
      "Gradient Descent(23/99): loss =0.32987845321914694, w0=-0.02880625885795573, w1=0.10423101700198427\n",
      "Gradient Descent(24/99): loss =0.32944371241265763, w0=-0.02992630788986587, w1=0.10364460938616508\n",
      "Gradient Descent(25/99): loss =0.3290470059237914, w0=-0.03104813486979688, w1=0.1031316353539945\n",
      "Gradient Descent(26/99): loss =0.3286824888465438, w0=-0.032171271762130256, w1=0.10268482590350106\n",
      "Gradient Descent(27/99): loss =0.3283454456069583, w0=-0.03329537383955552, w1=0.10229770657784577\n",
      "Gradient Descent(28/99): loss =0.328032040393795, w0=-0.034420181612990215, w1=0.10196449502012314\n",
      "Gradient Descent(29/99): loss =0.3277391290858649, w0=-0.03554549270624904, w1=0.10168001618950029\n",
      "Gradient Descent(30/99): loss =0.32746411599931036, w0=-0.03667114218982625, w1=0.10143962961953017\n",
      "Gradient Descent(31/99): loss =0.32720484368741615, w0=-0.03779698848528378, w1=0.10123916748239167\n",
      "Gradient Descent(32/99): loss =0.32695950738827984, w0=-0.03892290363933507, w1=0.10107488097447392\n",
      "Gradient Descent(33/99): loss =0.326726588046912, w0=-0.04004876662536919, w1=0.1009433938731033\n",
      "Gradient Descent(34/99): loss =0.32650479947329086, w0=-0.04117445891836284, w1=0.10084166196153006\n",
      "Gradient Descent(35/99): loss =0.3262930463575765, w0=-0.04229986167030141, w1=0.10076693748509778\n",
      "Gradient Descent(36/99): loss =0.32609039069529117, w0=-0.04342485404969297, w1=0.10071673786010486\n",
      "Gradient Descent(37/99): loss =0.32589602477805985, w0=-0.04454931239381334, w1=0.1006888180516619\n",
      "Gradient Descent(38/99): loss =0.32570924934697904, w0=-0.04567310993039224, w1=0.1006811461120712\n",
      "Gradient Descent(39/99): loss =0.325529455832275, w0=-0.04679611688274704, w1=0.10069188146912371\n",
      "Gradient Descent(40/99): loss =0.3253561118468549, w0=-0.0479182008264266, w1=0.10071935561194034\n",
      "Gradient Descent(41/99): loss =0.3251887492852428, w0=-0.04903922719977387, w1=0.10076205487887319\n",
      "Gradient Descent(42/99): loss =0.3250269545192067, w0=-0.050159059899265314, w1=0.10081860509322331\n",
      "Gradient Descent(43/99): loss =0.3248703602885711, w0=-0.051277561910120596, w1=0.10088775782903874\n",
      "Gradient Descent(44/99): loss =0.32471863896848846, w0=-0.052394595937963355, w1=0.1009683781182709\n",
      "Gradient Descent(45/99): loss =0.3245714969588411, w0=-0.05351002501822351, w1=0.10105943343554369\n",
      "Gradient Descent(46/99): loss =0.32442866999185715, w0=-0.05462371308817225, w1=0.10115998381754636\n",
      "Gradient Descent(47/99): loss =0.324289919193725, w0=-0.055735525512368225, w1=0.10126917299188871\n",
      "Gradient Descent(48/99): loss =0.3241550277674342, w0=-0.05684532955659337, w1=0.10138622040542905\n",
      "Gradient Descent(49/99): loss =0.3240237981890838, w0=-0.057952994808398765, w1=0.10151041405518993\n",
      "Gradient Descent(50/99): loss =0.3238960498299103, w0=-0.059058393544521214, w1=0.10164110403628945\n",
      "Gradient Descent(51/99): loss =0.323771616932338, w0=-0.060161401046884194, w1=0.10177769673115539\n",
      "Gradient Descent(52/99): loss =0.3236503468812988, w0=-0.06126189586985051, w1=0.101919649572865\n",
      "Gradient Descent(53/99): loss =0.32353209872253963, w0=-0.06235976006197758, w1=0.10206646632295971\n",
      "Gradient Descent(54/99): loss =0.32341674188812036, w0=-0.06345487934584228, w1=0.10221769281067006\n",
      "Gradient Descent(55/99): loss =0.32330415509622457, w0=-0.064547143259626, w1=0.10237291308628241\n",
      "Gradient Descent(56/99): loss =0.3231942253980439, w0=-0.06563644526413923, w1=0.10253174594649012\n",
      "Gradient Descent(57/99): loss =0.32308684734911586, w0=-0.06672268281885951, w1=0.10269384179408927\n",
      "Gradient Descent(58/99): loss =0.32298192228628303, w0=-0.0678057574303912, w1=0.10285887979837846\n",
      "Gradient Descent(59/99): loss =0.32287935769455633, w0=-0.06888557467654982, w1=0.10302656532616873\n",
      "Gradient Descent(60/99): loss =0.32277906665073614, w0=-0.06996204420904832, w1=0.10319662761646028\n",
      "Gradient Descent(61/99): loss =0.32268096733276525, w0=-0.07103507973752712, w1=0.1033688176746435\n",
      "Gradient Descent(62/99): loss =0.32258498258554547, w0=-0.07210459899743411, w1=0.10354290636457658\n",
      "Gradient Descent(63/99): loss =0.322491039535407, w0=-0.07317052370403082, w1=0.10371868267911558\n",
      "Gradient Descent(64/99): loss =0.32239906924663453, w0=-0.0742327794945805, w1=0.10389595217165647\n",
      "Gradient Descent(65/99): loss =0.32230900641446003, w0=-0.07529129586056593, w1=0.10407453553302114\n",
      "Gradient Descent(66/99): loss =0.3222207890897798, w0=-0.07634600607159, w1=0.10425426729960294\n",
      "Gradient Descent(67/99): loss =0.322134358431554, w0=-0.0773968470924323, w1=0.1044349946801047\n",
      "Gradient Descent(68/99): loss =0.3220496584834461, w0=-0.07844375949456943, w1=0.10461657648947066\n",
      "Gradient Descent(69/99): loss =0.3219666359717459, w0=-0.07948668736331543, w1=0.10479888217975024\n",
      "Gradient Descent(70/99): loss =0.32188524012204944, w0=-0.08052557820160136, w1=0.10498179095865072\n",
      "Gradient Descent(71/99): loss =0.3218054224925128, w0=-0.08156038283128833, w1=0.10516519098744982\n",
      "Gradient Descent(72/99): loss =0.321727136821802, w0=-0.08259105529279649, w1=0.10534897865075923\n",
      "Gradient Descent(73/99): loss =0.3216503388901097, w0=-0.0836175527437311, w1=0.10553305789136731\n",
      "Gradient Descent(74/99): loss =0.32157498639182686, w0=-0.08463983535709677, w1=0.1057173396040507\n",
      "Gradient Descent(75/99): loss =0.3215010388186393, w0=-0.08565786621960997, w1=0.10590174108283984\n",
      "Gradient Descent(76/99): loss =0.3214284573519749, w0=-0.086671611230548, w1=0.10608618551675822\n",
      "Gradient Descent(77/99): loss =0.3213572047638625, w0=-0.08768103900150837, w1=0.10627060152953724\n",
      "Gradient Descent(78/99): loss =0.32128724532537517, w0=-0.08868612075739585, w1=0.10645492275924126\n",
      "Gradient Descent(79/99): loss =0.3212185447219322, w0=-0.08968683023890428, w1=0.10663908747412829\n",
      "Gradient Descent(80/99): loss =0.3211510699748162, w0=-0.09068314360671523, w1=0.10682303822142314\n",
      "Gradient Descent(81/99): loss =0.3210847893683371, w0=-0.091675039347597, w1=0.10700672150599688\n",
      "Gradient Descent(82/99): loss =0.32101967238213736, w0=-0.09266249818255268, w1=0.10719008749623193\n",
      "Gradient Descent(83/99): loss =0.32095568962818766, w0=-0.09364550297713559, w1=0.10737308975461018\n",
      "Gradient Descent(84/99): loss =0.32089281279207205, w0=-0.09462403865402433, w1=0.10755568499079367\n",
      "Gradient Descent(85/99): loss =0.32083101457820024, w0=-0.09559809210792616, w1=0.10773783283517742\n",
      "Gradient Descent(86/99): loss =0.32077026865862696, w0=-0.09656765212285746, w1=0.10791949563108351\n",
      "Gradient Descent(87/99): loss =0.32071054962518425, w0=-0.09753270929183255, w1=0.1081006382439364\n",
      "Gradient Descent(88/99): loss =0.32065183294466565, w0=-0.09849325593897702, w1=0.10828122788591431\n",
      "Gradient Descent(89/99): loss =0.3205940949168254, w0=-0.09944928604406857, w1=0.10846123395471122\n",
      "Gradient Descent(90/99): loss =0.32053731263497587, w0=-0.10040079516949735, w1=0.1086406278851702\n",
      "Gradient Descent(91/99): loss =0.3204814639489892, w0=-0.10134778038962822, w1=0.10881938301266322\n",
      "Gradient Descent(92/99): loss =0.3204265274305257, w0=-0.10229024022253912, w1=0.10899747444719585\n",
      "Gradient Descent(93/99): loss =0.32037248234032445, w0=-0.10322817456410309, w1=0.10917487895730876\n",
      "Gradient Descent(94/99): loss =0.32031930859741164, w0=-0.10416158462437569, w1=0.10935157486293293\n",
      "Gradient Descent(95/99): loss =0.3202669867500888, w0=-0.10509047286624483, w1=0.10952754193643144\n",
      "Gradient Descent(96/99): loss =0.320215497948576, w0=-0.1060148429462964, w1=0.10970276131113113\n",
      "Gradient Descent(97/99): loss =0.32016482391920026, w0=-0.10693469965784563, w1=0.10987721539670943\n",
      "Gradient Descent(98/99): loss =0.32011494694001885, w0=-0.1078500488760821, w1=0.11005088780085928\n",
      "Gradient Descent(99/99): loss =0.3200658498177857, w0=-0.10876089750527401, w1=0.11022376325670662\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =23.466899011446415, w0=-0.002038632791394962, w1=0.19999210909830573\n",
      "Gradient Descent(2/99): loss =1.70393562722261, w0=-0.03389926990921584, w1=0.16951806985661821\n",
      "Gradient Descent(3/99): loss =0.5741206124037165, w0=-0.024835240817469007, w1=0.16827237368189496\n",
      "Gradient Descent(4/99): loss =0.46823941579859873, w0=-0.022280131940082586, w1=0.15773300821939001\n",
      "Gradient Descent(5/99): loss =0.4223777936231357, w0=-0.01970966013960773, w1=0.1489535348254683\n",
      "Gradient Descent(6/99): loss =0.39428992621597886, w0=-0.01807126605972524, w1=0.14125744384724875\n",
      "Gradient Descent(7/99): loss =0.3766760955867179, w0=-0.017022023891492398, w1=0.13480012863410618\n",
      "Gradient Descent(8/99): loss =0.3650528008181287, w0=-0.01646093243502827, w1=0.12929877333571324\n",
      "Gradient Descent(9/99): loss =0.35702574600486736, w0=-0.016263651612295444, w1=0.12459252684239017\n",
      "Gradient Descent(10/99): loss =0.35127574529484507, w0=-0.01634781823649628, w1=0.12052170944488041\n",
      "Gradient Descent(11/99): loss =0.34702636031042533, w0=-0.016645135642591843, w1=0.11697864067417378\n",
      "Gradient Descent(12/99): loss =0.3437978101064592, w0=-0.017106571825808835, w1=0.11387348989948054\n",
      "Gradient Descent(13/99): loss =0.3412829886104663, w0=-0.01769400204704518, w1=0.11113941298708586\n",
      "Gradient Descent(14/99): loss =0.3392796174054282, w0=-0.018379257071405086, w1=0.1087218054631607\n",
      "Gradient Descent(15/99): loss =0.33765107215631135, w0=-0.019140780713697424, w1=0.10657744659036107\n",
      "Gradient Descent(16/99): loss =0.3363029335827154, w0=-0.019962349493843734, w1=0.1046706467098308\n",
      "Gradient Descent(17/99): loss =0.33516855377693267, w0=-0.020831534789388213, w1=0.10297202780076503\n",
      "Gradient Descent(18/99): loss =0.33419995660946655, w0=-0.021738839519668828, w1=0.10145688384409651\n",
      "Gradient Descent(19/99): loss =0.3333619743138316, w0=-0.022676925075188245, w1=0.10010432816869799\n",
      "Gradient Descent(20/99): loss =0.33262839175690107, w0=-0.02364010055430633, w1=0.09889648036797029\n",
      "Gradient Descent(21/99): loss =0.3319793621536357, w0=-0.024623912145313148, w1=0.09781793203750243\n",
      "Gradient Descent(22/99): loss =0.33139964369512825, w0=-0.025624850914042676, w1=0.09685529522324333\n",
      "Gradient Descent(23/99): loss =0.3308773759185121, w0=-0.026640126797970124, w1=0.0959968696467109\n",
      "Gradient Descent(24/99): loss =0.33040321705901954, w0=-0.027667501852195357, w1=0.09523237054819716\n",
      "Gradient Descent(25/99): loss =0.3299697267323993, w0=-0.02870516268889496, w1=0.0945527155456292\n",
      "Gradient Descent(26/99): loss =0.329570917889501, w0=-0.029751624511399368, w1=0.0939498499966351\n",
      "Gradient Descent(27/99): loss =0.32920192725059955, w0=-0.03080565771456437, w1=0.09341660511053712\n",
      "Gradient Descent(28/99): loss =0.3288587698075067, w0=-0.03186623200326283, w1=0.09294658009040427\n",
      "Gradient Descent(29/99): loss =0.3285381537596803, w0=-0.03293247349115906, w1=0.09253404393340804\n",
      "Gradient Descent(30/99): loss =0.3282373394408891, w0=-0.03400363176668125, w1=0.0921738525610797\n",
      "Gradient Descent(31/99): loss =0.32795403065236073, w0=-0.03507905448696519, w1=0.09186137847498525\n",
      "Gradient Descent(32/99): loss =0.32768629014285655, w0=-0.036158167749013606, w1=0.09159245053947473\n",
      "Gradient Descent(33/99): loss =0.3274324732774588, w0=-0.03724046087313838, w1=0.09136330213839838\n",
      "Gradient Descent(34/99): loss =0.3271911755480973, w0=-0.03832547458237689, w1=0.09117052627086865\n",
      "Gradient Descent(35/99): loss =0.3269611907194979, w0=-0.03941279179418973, w1=0.09101103647088488\n",
      "Gradient Descent(36/99): loss =0.32674147722055646, w0=-0.04050203042986772, w1=0.09088203264152907\n",
      "Gradient Descent(37/99): loss =0.3265311309816143, w0=-0.041592837783529686, w1=0.09078097107140727\n",
      "Gradient Descent(38/99): loss =0.32632936334969626, w0=-0.04268488609942029, w1=0.09070553802997816\n",
      "Gradient Descent(39/99): loss =0.32613548303245654, w0=-0.043777869086152534, w1=0.09065362644406229\n",
      "Gradient Descent(40/99): loss =0.3259488812592159, w0=-0.04487149915843451, w1=0.09062331523972549\n",
      "Gradient Descent(41/99): loss =0.3257690195263805, w0=-0.04596550524399125, w1=0.09061285100041533\n",
      "Gradient Descent(42/99): loss =0.3255954194304143, w0=-0.047059631029884356, w1=0.09062063164577314\n",
      "Gradient Descent(43/99): loss =0.32542765419565417, w0=-0.048153633550531164, w1=0.09064519187953171\n",
      "Gradient Descent(44/99): loss =0.3252653415846444, w0=-0.04924728204151354, w1=0.0906851901910589\n",
      "Gradient Descent(45/99): loss =0.3251081379412136, w0=-0.050340357000150035, w1=0.09073939722519188\n",
      "Gradient Descent(46/99): loss =0.3249557331655059, w0=-0.051432649406934094, w1=0.09080668536015832\n",
      "Gradient Descent(47/99): loss =0.3248078464587943, w0=-0.05252396007215399, w1=0.09088601935457905\n",
      "Gradient Descent(48/99): loss =0.3246642227065204, w0=-0.053614099079969765, w1=0.09097644794250545\n",
      "Gradient Descent(49/99): loss =0.3245246293924026, w0=-0.05470288530842683, w1=0.0910770962707496\n",
      "Gradient Descent(50/99): loss =0.32438885395599776, w0=-0.055790146008726574, w1=0.09118715908586694\n",
      "Gradient Descent(51/99): loss =0.32425670152182173, w0=-0.05687571643084927, w1=0.09130589458942032\n",
      "Gradient Descent(52/99): loss =0.3241279929408312, w0=-0.05795943948556889, w1=0.09143261888988222\n",
      "Gradient Descent(53/99): loss =0.32400256309536735, w0=-0.05904116543519269, w1=0.09156670098796182\n",
      "Gradient Descent(54/99): loss =0.32388025942703863, w0=-0.06012075160714357, w1=0.09170755823947109\n",
      "Gradient Descent(55/99): loss =0.32376094065385935, w0=-0.06119806212588973, w1=0.09185465224623153\n",
      "Gradient Descent(56/99): loss =0.32364447564856474, w0=-0.062272967659800886, w1=0.09200748513110756\n",
      "Gradient Descent(57/99): loss =0.3235307424546255, w0=-0.06334534518034131, w1=0.09216559615814468\n",
      "Gradient Descent(58/99): loss =0.32341962742027114, w0=-0.06441507773164931, w1=0.09232855866308746\n",
      "Gradient Descent(59/99): loss =0.32331102443397075, w0=-0.06548205420904443, w1=0.09249597726333295\n",
      "Gradient Descent(60/99): loss =0.32320483424740526, w0=-0.06654616914537698, w1=0.09266748531970877\n",
      "Gradient Descent(61/99): loss =0.3231009638741282, w0=-0.06760732250441936, w1=0.09284274262540915\n",
      "Gradient Descent(62/99): loss =0.32299932605389836, w0=-0.06866541948071166, w1=0.09302143330002624\n",
      "Gradient Descent(63/99): loss =0.3228998387741708, w0=-0.069720370305433, w1=0.09320326386892212\n",
      "Gradient Descent(64/99): loss =0.3228024248414825, w0=-0.0707720900579878, w1=0.09338796151023371\n",
      "Gradient Descent(65/99): loss =0.3227070114965217, w0=-0.07182049848308036, w1=0.09357527245362257\n",
      "Gradient Descent(66/99): loss =0.3226135300675539, w0=-0.07286551981311186, w1=0.09376496051649955\n",
      "Gradient Descent(67/99): loss =0.3225219156576246, w0=-0.07390708259577572, w1=0.0939568057648958\n",
      "Gradient Descent(68/99): loss =0.32243210686158713, w0=-0.07494511952675464, w1=0.09415060328743749\n",
      "Gradient Descent(69/99): loss =0.32234404550953855, w0=-0.0759795672874404, w1=0.09434616207202877\n",
      "Gradient Descent(70/99): loss =0.3222576764337029, w0=-0.07701036638760717, w1=0.09454330397587364\n",
      "Gradient Descent(71/99): loss =0.3221729472561842, w0=-0.07803746101297344, w1=0.09474186278038454\n",
      "Gradient Descent(72/99): loss =0.3220898081953431, w0=-0.07906079887758848, w1=0.09494168332334735\n",
      "Gradient Descent(73/99): loss =0.3220082118888356, w0=-0.08008033108097691, w1=0.09514262070144881\n",
      "Gradient Descent(74/99): loss =0.32192811323158915, w0=-0.08109601196997195, w1=0.09534453953693314\n",
      "Gradient Descent(75/99): loss =0.3218494692272078, w0=-0.08210779900516277, w1=0.09554731330274766\n",
      "Gradient Descent(76/99): loss =0.32177223885147027, w0=-0.08311565263187705, w1=0.09575082370107038\n",
      "Gradient Descent(77/99): loss =0.32169638292674585, w0=-0.08411953615561445, w1=0.09595496009059219\n",
      "Gradient Descent(78/99): loss =0.32162186400628534, w0=-0.08511941562184232, w1=0.09615961895835717\n",
      "Gradient Descent(79/99): loss =0.32154864626746044, w0=-0.08611525970006038, w1=0.0963647034323539\n",
      "Gradient Descent(80/99): loss =0.321476695413131, w0=-0.08710703957203732, w1=0.09657012283140053\n",
      "Gradient Descent(81/99): loss =0.3214059785804036, w0=-0.08809472882411895, w1=0.09677579224918274\n",
      "Gradient Descent(82/99): loss =0.3213364642561262, w0=-0.08907830334350496, w1=0.09698163216958874\n",
      "Gradient Descent(83/99): loss =0.32126812219853235, w0=-0.09005774121838889, w1=0.09718756811074329\n",
      "Gradient Descent(84/99): loss =0.3212009233645047, w0=-0.0910330226418548, w1=0.09739353029537583\n",
      "Gradient Descent(85/99): loss =0.3211348398419863, w0=-0.09200412981942252, w1=0.09759945334536808\n",
      "Gradient Descent(86/99): loss =0.3210698447871097, w0=-0.09297104688013351, w1=0.09780527599851775\n",
      "Gradient Descent(87/99): loss =0.3210059123656591, w0=-0.09393375979106874, w1=0.09801094084572709\n",
      "Gradient Descent(88/99): loss =0.32094301769851535, w0=-0.09489225627519067, w1=0.09821639408698205\n",
      "Gradient Descent(89/99): loss =0.32088113681076813, w0=-0.095846525732402, w1=0.09842158530462951\n",
      "Gradient Descent(90/99): loss =0.3208202465842072, w0=-0.09679655916371514, w1=0.09862646725258871\n",
      "Gradient Descent(91/99): loss =0.32076032471293137, w0=-0.09774234909842736, w1=0.09883099566025016\n",
      "Gradient Descent(92/99): loss =0.32070134966183766, w0=-0.0986838895241985, w1=0.09903512904992144\n",
      "Gradient Descent(93/99): loss =0.3206433006277741, w0=-0.09962117581992998, w1=0.0992388285667755\n",
      "Gradient Descent(94/99): loss =0.3205861575031556, w0=-0.10055420469134546, w1=0.0994420578203456\n",
      "Gradient Descent(95/99): loss =0.3205299008418653, w0=-0.1014829741091761, w1=0.09964478273668999\n",
      "Gradient Descent(96/99): loss =0.32047451182727144, w0=-0.10240748324985555, w1=0.09984697142042301\n",
      "Gradient Descent(97/99): loss =0.32041997224221047, w0=-0.10332773243863184, w1=0.1000485940258749\n",
      "Gradient Descent(98/99): loss =0.3203662644407943, w0=-0.10424372309500643, w1=0.1002496226367035\n",
      "Gradient Descent(99/99): loss =0.3203133713219148, w0=-0.10515545768041247, w1=0.10045003115333574\n",
      "Optimizing degree 3/15, model: least_squares_GD, arguments: {'max_iters': 100}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =214244.19863310343, w0=-0.010175105052137895, w1=0.19999990929341332\n",
      "Gradient Descent(2/99): loss =251.92038946961733, w0=-0.04426475846203176, w1=0.1870516084642986\n",
      "Gradient Descent(3/99): loss =71.95038324074574, w0=0.04471501682829425, w1=0.2097995711232005\n",
      "Gradient Descent(4/99): loss =40.09982182648638, w0=0.03176562310985291, w1=0.2084657552848001\n",
      "Gradient Descent(5/99): loss =26.894504690326404, w0=0.05827988193578103, w1=0.20846122527146296\n",
      "Gradient Descent(6/99): loss =20.246168961120592, w0=0.05804454151617648, w1=0.20439419714454976\n",
      "Gradient Descent(7/99): loss =16.294646197617755, w0=0.06755122635972344, w1=0.2014247160384875\n",
      "Gradient Descent(8/99): loss =13.629944370349412, w0=0.07013728236720915, w1=0.19797879942940877\n",
      "Gradient Descent(9/99): loss =11.688989595220253, w0=0.07470801621887024, w1=0.19520311693110495\n",
      "Gradient Descent(10/99): loss =10.202059752302937, w0=0.07722622665930387, w1=0.19260558565566904\n",
      "Gradient Descent(11/99): loss =9.021702692060485, w0=0.07989558808956462, w1=0.19039053594306907\n",
      "Gradient Descent(12/99): loss =8.059333085978935, w0=0.08177631344343554, w1=0.18838243482101308\n",
      "Gradient Descent(13/99): loss =7.258227184044117, w0=0.08344237883050587, w1=0.18658956801709467\n",
      "Gradient Descent(14/99): loss =6.580273654322029, w0=0.08470002537918896, w1=0.18493850855315289\n",
      "Gradient Descent(15/99): loss =5.998842956792606, w0=0.08571821967120752, w1=0.1834082911560326\n",
      "Gradient Descent(16/99): loss =5.494704792712123, w0=0.08647512896606735, w1=0.18196391209398993\n",
      "Gradient Descent(17/99): loss =5.053576970725645, w0=0.08703305252860508, w1=0.1805874168359289\n",
      "Gradient Descent(18/99): loss =4.6645985064160636, w0=0.0874047152900335, w1=0.1792606655011642\n",
      "Gradient Descent(19/99): loss =4.319343514359954, w0=0.08762053958090696, w1=0.17797206675287036\n",
      "Gradient Descent(20/99): loss =4.011163769911857, w0=0.08769669205548379, w1=0.17671179555952898\n",
      "Gradient Descent(21/99): loss =3.734736705272301, w0=0.08765180034045678, w1=0.17547297986924323\n",
      "Gradient Descent(22/99): loss =3.4857462632845504, w0=0.0874995228585091, w1=0.17425020727570087\n",
      "Gradient Descent(23/99): loss =3.2606521619183875, w0=0.08725285785239385, w1=0.1730395634331909\n",
      "Gradient Descent(24/99): loss =3.0565197783530653, w0=0.08692254343020557, w1=0.1718380827308135\n",
      "Gradient Descent(25/99): loss =2.8708926468554687, w0=0.08651826322334047, w1=0.17064361665133718\n",
      "Gradient Descent(26/99): loss =2.7016956063551554, w0=0.08604838732159341, w1=0.169454594028701\n",
      "Gradient Descent(27/99): loss =2.547160402219846, w0=0.08552036914827102, w1=0.16826990827487973\n",
      "Gradient Descent(28/99): loss =2.4057679959992435, w0=0.08494076855701109, w1=0.16708879763268408\n",
      "Gradient Descent(29/99): loss =2.276203461133402, w0=0.08431541964294123, w1=0.16591077065687876\n",
      "Gradient Descent(30/99): loss =2.1573204556261927, w0=0.08364949682246257, w1=0.16473554089102263\n",
      "Gradient Descent(31/99): loss =2.0481130415396267, w0=0.08294760725146094, w1=0.16356298074330786\n",
      "Gradient Descent(32/99): loss =1.94769317921187, w0=0.08221385158434426, w1=0.16239308384670215\n",
      "Gradient Descent(33/99): loss =1.8552726305622833, w0=0.08145188541302223, w1=0.16122593684567058\n",
      "Gradient Descent(34/99): loss =1.7701483064596069, w0=0.08066496829372724, w1=0.16006169689906863\n",
      "Gradient Descent(35/99): loss =1.6916903179108929, w0=0.07985600893359553, w1=0.15890057428881113\n",
      "Gradient Descent(36/99): loss =1.6193321604723776, w0=0.07902760393067527, w1=0.15774281860078476\n",
      "Gradient Descent(37/99): loss =1.5525625901908275, w0=0.07818207259352877, w1=0.15658870784857207\n",
      "Gradient Descent(38/99): loss =1.4909188478838598, w0=0.07732148757246346, w1=0.15543853980119834\n",
      "Gradient Descent(39/99): loss =1.433980964156438, w0=0.07644770221678417, w1=0.15429262508108563\n",
      "Gradient Descent(40/99): loss =1.3813669357650846, w0=0.07556237484977316, w1=0.15315128163655653\n",
      "Gradient Descent(41/99): loss =1.3327286089084713, w0=0.07466699040511565, w1=0.15201483031599067\n",
      "Gradient Descent(42/99): loss =1.287748139850125, w0=0.07376287966635213, w1=0.150883591317033\n",
      "Gradient Descent(43/99): loss =1.2461349303189284, w0=0.0728512363901841, w1=0.14975788134221063\n",
      "Gradient Descent(44/99): loss =1.207622956177876, w0=0.07193313252726016, w1=0.14863801132511015\n",
      "Gradient Descent(45/99): loss =1.1719684242705153, w0=0.07100953174793713, w1=0.14752428462189385\n",
      "Gradient Descent(46/99): loss =1.138947705197692, w0=0.07008130145048237, w1=0.14641699558369278\n",
      "Gradient Descent(47/99): loss =1.108355499851392, w0=0.06914922341541616, w1=0.14531642844279316\n",
      "Gradient Descent(48/99): loss =1.0800032054588256, w0=0.06821400325131495, w1=0.14422285645834768\n",
      "Gradient Descent(49/99): loss =1.0537174531473819, w0=0.06727627876395816, w1=0.14313654127768285\n",
      "Gradient Descent(50/99): loss =1.0293387939996823, w0=0.06633662736702206, w1=0.1420577324772351\n",
      "Gradient Descent(51/99): loss =1.0067205145134168, w0=0.06539557264083652, w1=0.14098666725353096\n",
      "Gradient Descent(52/99): loss =0.985727565534583, w0=0.06445359013470352, w1=0.1399235702396742\n",
      "Gradient Descent(53/99): loss =0.9662355912664385, w0=0.06351111249842245, w1=0.1388686534268768\n",
      "Gradient Descent(54/99): loss =0.9481300470032161, w0=0.06256853401961013, w1=0.13782211617385695\n",
      "Gradient Descent(55/99): loss =0.9313053959007619, w0=0.06162621463522162, w1=0.1367841452896132\n",
      "Gradient Descent(56/99): loss =0.9156643764562157, w0=0.06068448347824216, w1=0.1357549151772873\n",
      "Gradient Descent(57/99): loss =0.9011173334884097, w0=0.059743642013810494, w1=0.13473458802865457\n",
      "Gradient Descent(58/99): loss =0.887581606338723, w0=0.05880396681297968, w1=0.13372331406030172\n",
      "Gradient Descent(59/99): loss =0.8749809687870297, w0=0.057865712006880704, w1=0.1327212317838307\n",
      "Gradient Descent(60/99): loss =0.8632451158291428, w0=0.05692911145917294, w1=0.13172846830350596\n",
      "Gradient Descent(61/99): loss =0.8523091930143694, w0=0.05599438069029901, w1=0.13074513963568057\n",
      "Gradient Descent(62/99): loss =0.8421133645130924, w0=0.055061718583162576, w1=0.12977135104512086\n",
      "Gradient Descent(63/99): loss =0.8326024164893729, w0=0.05413130889637444, w1=0.12880719739402224\n",
      "Gradient Descent(64/99): loss =0.8237253927041267, w0=0.05320332160812375, w1=0.12785276350009075\n",
      "Gradient Descent(65/99): loss =0.8154352595797616, w0=0.05227791411098948, w1=0.12690812450056688\n",
      "Gradient Descent(66/99): loss =0.8076885982247447, w0=0.051355232275577915, w1=0.12597334621950573\n",
      "Gradient Descent(67/99): loss =0.8004453211523705, w0=0.05043541139872148, w1=0.1250484855360084\n",
      "Gradient Descent(68/99): loss =0.7936684116368979, w0=0.049518577050074744, w1=0.12413359075143132\n",
      "Gradient Descent(69/99): loss =0.7873236838361443, w0=0.0486048458292656, w1=0.12322870195389185\n",
      "Gradient Descent(70/99): loss =0.7813795619758079, w0=0.047694326044281275, w1=0.12233385137864235\n",
      "Gradient Descent(71/99): loss =0.7758068770398919, w0=0.04678711832046614, w1=0.12144906376310913\n",
      "Gradient Descent(72/99): loss =0.7705786795458298, w0=0.04588331614836174, w1=0.1205743566955883\n",
      "Gradient Descent(73/99): loss =0.7656700671041197, w0=0.04498300637761158, w1=0.1197097409567628\n",
      "Gradient Descent(74/99): loss =0.7610580255719966, w0=0.04408626966326716, w1=0.11885522085335572\n",
      "Gradient Descent(75/99): loss =0.756721282710235, w0=0.043193180870053895, w1=0.11801079454336803\n",
      "Gradient Descent(76/99): loss =0.7526401733426942, w0=0.0423038094394727, w1=0.11717645435246463\n",
      "Gradient Descent(77/99): loss =0.7487965151006738, w0=0.041418219724013876, w1=0.1163521870811741\n",
      "Gradient Descent(78/99): loss =0.7451734939093344, w0=0.040536471292234866, w1=0.11553797430265743\n",
      "Gradient Descent(79/99): loss =0.7417555584421412, w0=0.039658619207992706, w1=0.11473379265087734\n",
      "Gradient Descent(80/99): loss =0.738528322832079, w0=0.03878471428671877, w1=0.11393961409906846\n",
      "Gradient Descent(81/99): loss =0.7354784769858755, w0=0.03791480333126974, w1=0.11315540622846655\n",
      "Gradient Descent(82/99): loss =0.7325937039001253, w0=0.03704892934957916, w1=0.1123811324873061\n",
      "Gradient Descent(83/99): loss =0.7298626034264751, w0=0.03618713175606289, w1=0.11161675244013895\n",
      "Gradient Descent(84/99): loss =0.7272746219773029, w0=0.03532944655849428, w1=0.11086222200756454\n",
      "Gradient Descent(85/99): loss =0.7248199877039652, w0=0.03447590653185709, w1=0.1101174936964938\n",
      "Gradient Descent(86/99): loss =0.7224896507169933, w0=0.03362654138050235, w1=0.1093825168210961\n",
      "Gradient Descent(87/99): loss =0.7202752279518932, w0=0.03278137788977576, w1=0.10865723771460116\n",
      "Gradient Descent(88/99): loss =0.7181689523156973, w0=0.031940440068143325, w1=0.10794159993214664\n",
      "Gradient Descent(89/99): loss =0.7161636257783667, w0=0.031103749280720232, w1=0.10723554444487773\n",
      "Gradient Descent(90/99): loss =0.7142525760997547, w0=0.03027132437500167, w1=0.1065390098255173\n",
      "Gradient Descent(91/99): loss =0.7124296169073273, w0=0.029443181799500368, w1=0.10585193242563468\n",
      "Gradient Descent(92/99): loss =0.7106890108623428, w0=0.028619335715913876, w1=0.10517424654484936\n",
      "Gradient Descent(93/99): loss =0.7090254356729133, w0=0.02779979810537269, w1=0.10450588459221032\n",
      "Gradient Descent(94/99): loss =0.7074339527314336, w0=0.0269845788692577, w1=0.10384677723999577\n",
      "Gradient Descent(95/99): loss =0.7059099781713967, w0=0.026173685925020077, w1=0.10319685357018023\n",
      "Gradient Descent(96/99): loss =0.704449256154769, w0=0.025367125297388653, w1=0.10255604121381598\n",
      "Gradient Descent(97/99): loss =0.7030478342159576, w0=0.02456490120530744, w1=0.10192426648357593\n",
      "Gradient Descent(98/99): loss =0.7017020405020788, w0=0.023767016144908837, w1=0.10130145449970296\n",
      "Gradient Descent(99/99): loss =0.7004084627618476, w0=0.022973470968795436, w1=0.10068752930960856\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =211946.15685824477, w0=-0.0040637346270718105, w1=0.19999990182807303\n",
      "Gradient Descent(2/99): loss =100.90301971302237, w0=0.05402142003306373, w1=0.2025676769211423\n",
      "Gradient Descent(3/99): loss =17.27623909941866, w0=0.05840210417791562, w1=0.19839561673960573\n",
      "Gradient Descent(4/99): loss =16.09514354012883, w0=0.09542880133386819, w1=0.20063731146907793\n",
      "Gradient Descent(5/99): loss =14.718376821774187, w0=0.09100035245989341, w1=0.19664502809171025\n",
      "Gradient Descent(6/99): loss =13.31253055534626, w0=0.11697975979281261, w1=0.1984367036615392\n",
      "Gradient Descent(7/99): loss =12.072399541626366, w0=0.11322275975359568, w1=0.19547352513789498\n",
      "Gradient Descent(8/99): loss =11.044224792717577, w0=0.12918830723465927, w1=0.1962113175865053\n",
      "Gradient Descent(9/99): loss =10.19314095680129, w0=0.1275184370333298, w1=0.19412460165069417\n",
      "Gradient Descent(10/99): loss =9.473592799114757, w0=0.13680970313441881, w1=0.19409821229130578\n",
      "Gradient Descent(11/99): loss =8.849321076528355, w0=0.136309402464737, w1=0.1925131872437195\n",
      "Gradient Descent(12/99): loss =8.295916092214776, w0=0.14158339691798935, w1=0.19202436980281382\n",
      "Gradient Descent(13/99): loss =7.7974775313464155, w0=0.14150162639121683, w1=0.1906894675861766\n",
      "Gradient Descent(14/99): loss =7.343578921190576, w0=0.14439126069946653, w1=0.1899267080213115\n",
      "Gradient Descent(15/99): loss =6.927129592947313, w0=0.144346827223293, w1=0.18870391400690387\n",
      "Gradient Descent(16/99): loss =6.543058062411025, w0=0.1458037504280758, w1=0.18777529073914861\n",
      "Gradient Descent(17/99): loss =6.187557220277836, w0=0.1456408591427823, w1=0.1865943026177374\n",
      "Gradient Descent(18/99): loss =5.8576267041394665, w0=0.1462195207382105, w1=0.18556181710956668\n",
      "Gradient Descent(19/99): loss =5.550816598438867, w0=0.14589380654569833, w1=0.1843891133131425\n",
      "Gradient Descent(20/99): loss =5.265062545758354, w0=0.14592197753547417, w1=0.18328951533040172\n",
      "Gradient Descent(21/99): loss =4.998589182339979, w0=0.14543790864350536, w1=0.18211091531731152\n",
      "Gradient Descent(22/99): loss =4.749842440289016, w0=0.14511316709354408, w1=0.18096714262462033\n",
      "Gradient Descent(23/99): loss =4.51744640323833, w0=0.1444931737021929, w1=0.17977839780894553\n",
      "Gradient Descent(24/99): loss =4.300170494198539, w0=0.14393721894767833, w1=0.17860570659331818\n",
      "Gradient Descent(25/99): loss =4.096906517407913, w0=0.1432075660343313, w1=0.17740749368196948\n",
      "Gradient Descent(26/99): loss =3.9066501773617444, w0=0.14249707886727753, w1=0.17621672983255576\n",
      "Gradient Descent(27/99): loss =3.7284871190225246, w0=0.1416820448050955, w1=0.17501200751558\n",
      "Gradient Descent(28/99): loss =3.5615813149007147, w0=0.1408664706025739, w1=0.17381136013797432\n",
      "Gradient Descent(29/99): loss =3.4051658075718816, w0=0.13998644311145408, w1=0.17260398754515718\n",
      "Gradient Descent(30/99): loss =3.258534858163402, w0=0.13909840913830052, w1=0.1713999405335937\n",
      "Gradient Descent(31/99): loss =3.1210374523295066, w0=0.13816970408038076, w1=0.17019396817142515\n",
      "Gradient Descent(32/99): loss =2.9920717149071017, w0=0.13723123265262646, w1=0.16899182851725272\n",
      "Gradient Descent(33/99): loss =2.8710801663999947, w0=0.1362665773983503, w1=0.16779114700389655\n",
      "Gradient Descent(34/99): loss =2.7575455924285217, w0=0.1352928628876586, w1=0.16659534931411615\n",
      "Gradient Descent(35/99): loss =2.650987463943828, w0=0.13430205873657633, w1=0.1654035277619913\n",
      "Gradient Descent(36/99): loss =2.5509587832140705, w0=0.1333038091807725, w1=0.16421781973967198\n",
      "Gradient Descent(37/99): loss =2.4570433056715735, w0=0.13229436823336507, w1=0.16303804381665749\n",
      "Gradient Descent(38/99): loss =2.368853065132841, w0=0.13127928594500263, w1=0.1618656075691885\n",
      "Gradient Descent(39/99): loss =2.2860261649766818, w0=0.1302569705433554, w1=0.16070066904372898\n",
      "Gradient Descent(40/99): loss =2.2082247909941946, w0=0.12923070650122015, w1=0.15954420690408205\n",
      "Gradient Descent(41/99): loss =2.135133418703096, w0=0.12819995843817472, w1=0.15839651885171802\n",
      "Gradient Descent(42/99): loss =2.0664571868037345, w0=0.12716673888122138, w1=0.1572583187125529\n",
      "Gradient Descent(43/99): loss =2.0019204171708878, w0=0.12613100911378933, w1=0.15612994257736745\n",
      "Gradient Descent(44/99): loss =1.9412652625081095, w0=0.12509405404320412, w1=0.1550119306174552\n",
      "Gradient Descent(45/99): loss =1.884250467476178, w0=0.12405605076370821, w1=0.15390460779656862\n",
      "Gradient Descent(46/99): loss =1.8306502302357273, w0=0.12301785791172572, w1=0.15280839279378342\n",
      "Gradient Descent(47/99): loss =1.780253154002736, w0=0.1219797309123938, w1=0.1517235769130311\n",
      "Gradient Descent(48/99): loss =1.7328612792579456, w0=0.1209422712261328, w1=0.15065048844098342\n",
      "Gradient Descent(49/99): loss =1.6882891888447802, w0=0.11990574798924855, w1=0.1495893763865113\n",
      "Gradient Descent(50/99): loss =1.646363179028849, w0=0.11887060198691352, w1=0.14854049821914575\n",
      "Gradient Descent(51/99): loss =1.6069204905988363, w0=0.1178370878419477, w1=0.14750405901766273\n",
      "Gradient Descent(52/99): loss =1.5698085947291804, w0=0.11680554188793883, w1=0.14648025856720842\n",
      "Gradient Descent(53/99): loss =1.5348845289929063, w0=0.11577619371307953, w1=0.14546925976241792\n",
      "Gradient Descent(54/99): loss =1.5020142793945404, w0=0.1147493087731616, w1=0.14447121411737276\n",
      "Gradient Descent(55/99): loss =1.4710722047562939, w0=0.11372508934497495, w1=0.14348624558552567\n",
      "Gradient Descent(56/99): loss =1.441940500153378, w0=0.11270375063096331, w1=0.14251446457687364\n",
      "Gradient Descent(57/99): loss =1.4145086964285243, w0=0.1116854688677957, w1=0.14155595987687927\n",
      "Gradient Descent(58/99): loss =1.388673193092045, w0=0.11067042207846363, w1=0.14061080652157315\n",
      "Gradient Descent(59/99): loss =1.3643368221633763, w0=0.10965876301567157, w1=0.13967906194857194\n",
      "Gradient Descent(60/99): loss =1.3414084407239282, w0=0.1086506410932715, w1=0.13876077056906094\n",
      "Gradient Descent(61/99): loss =1.319802550143117, w0=0.1076461883832203, w1=0.13785596211018314\n",
      "Gradient Descent(62/99): loss =1.2994389401080715, w0=0.10664553150595589, w1=0.1369646543925967\n",
      "Gradient Descent(63/99): loss =1.280242355739041, w0=0.10564878446913768, w1=0.13608685278958452\n",
      "Gradient Descent(64/99): loss =1.262142186207823, w0=0.1046560551850024, w1=0.1352225520140925\n",
      "Gradient Descent(65/99): loss =1.2450721733986818, w0=0.1036674418805122, w1=0.1343717361307023\n",
      "Gradient Descent(66/99): loss =1.2289701392614953, w0=0.10268303672762746, w1=0.1335343797823965\n",
      "Gradient Descent(67/99): loss =1.2137777306070774, w0=0.10170292410662392, w1=0.1327104484613616\n",
      "Gradient Descent(68/99): loss =1.1994401801857755, w0=0.10072718267606216, w1=0.13189989940739558\n",
      "Gradient Descent(69/99): loss =1.1859060829737484, w0=0.09975588458912332, w1=0.13110268198582276\n",
      "Gradient Descent(70/99): loss =1.1731271866676323, w0=0.0987890967135166, w1=0.13031873838398592\n",
      "Gradient Descent(71/99): loss =1.1610581954583001, w0=0.09782688033118109, w1=0.12954800401942207\n",
      "Gradient Descent(72/99): loss =1.149656586218891, w0=0.09686929188987628, w1=0.12879040810461187\n",
      "Gradient Descent(73/99): loss =1.1388824363016428, w0=0.09591638294263036, w1=0.12804587404777948\n",
      "Gradient Descent(74/99): loss =1.1286982621929456, w0=0.09496820063731826, w1=0.1273143199258956\n",
      "Gradient Descent(75/99): loss =1.1190688683267187, w0=0.09402478777054368, w1=0.12659565886085358\n",
      "Gradient Descent(76/99): loss =1.1099612054032177, w0=0.09308618312751121, w1=0.12588979942440323\n",
      "Gradient Descent(77/99): loss =1.1013442376038935, w0=0.09215242158667883, w1=0.12519664598292318\n",
      "Gradient Descent(78/99): loss =1.0931888181333753, w0=0.09122353437164975, w1=0.12451609904902014\n",
      "Gradient Descent(79/99): loss =1.085467572557173, w0=0.09029954917485711, w1=0.12384805559337318\n",
      "Gradient Descent(80/99): loss =1.0781547894386236, w0=0.08938049035582617, w1=0.12319240935273335\n",
      "Gradient Descent(81/99): loss =1.071226317811089, w0=0.08846637906853608, w1=0.1225490511098478\n",
      "Gradient Descent(82/99): loss =1.0646594710516555, w0=0.08755723342547515, w1=0.12191786896471489\n",
      "Gradient Descent(83/99): loss =1.058432936750766, w0=0.08665306862162175, w1=0.1212987485847189\n",
      "Gradient Descent(84/99): loss =1.0525266921984975, w0=0.085753897075495, w1=0.1206915734442678\n",
      "Gradient Descent(85/99): loss =1.046921925132664, w0=0.08485972854681605, w1=0.12009622504767356\n",
      "Gradient Descent(86/99): loss =1.0416009594168154, w0=0.08397057026117376, w1=0.1195125831412078\n",
      "Gradient Descent(87/99): loss =1.0365471853375063, w0=0.08308642702041043, w1=0.1189405259113107\n",
      "Gradient Descent(88/99): loss =1.0317449942301453, w0=0.08220730131492769, w1=0.11837993017236718\n",
      "Gradient Descent(89/99): loss =1.0271797171613188, w0=0.081333193426747, w1=0.11783067154270846\n",
      "Gradient Descent(90/99): loss =1.0228375674128807, w0=0.08046410153198633, w1=0.11729262461088849\n",
      "Gradient Descent(91/99): loss =1.0187055865293189, w0=0.07960002179692138, w1=0.11676566309175328\n",
      "Gradient Descent(92/99): loss =1.0147715937051085, w0=0.07874094847228456, w1=0.11624965997360275\n",
      "Gradient Descent(93/99): loss =1.0110241383029335, w0=0.07788687398277704, w1=0.11574448765639271\n",
      "Gradient Descent(94/99): loss =1.0074524553069395, w0=0.0770377890143124, w1=0.11525001808185567\n",
      "Gradient Descent(95/99): loss =1.004046423527586, w0=0.0761936825974416, w1=0.1147661228556928\n",
      "Gradient Descent(96/99): loss =1.0007965263862808, w0=0.07535454218833816, w1=0.11429267336247206\n",
      "Gradient Descent(97/99): loss =0.9976938151188254, w0=0.07452035374656553, w1=0.11382954087347343\n",
      "Gradient Descent(98/99): loss =0.9947298742468864, w0=0.07369110181039529, w1=0.11337659664796873\n",
      "Gradient Descent(99/99): loss =0.9918967891761896, w0=0.07286676956930127, w1=0.11293371202820673\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =318.68908215146115, w0=-0.00048388194022104536, w1=0.19999986631992347\n",
      "Gradient Descent(2/99): loss =255.10501392738564, w0=-0.042861181730986485, w1=0.1705240940853641\n",
      "Gradient Descent(3/99): loss =76.30712710717464, w0=-0.0063783996289309836, w1=0.193275739058882\n",
      "Gradient Descent(4/99): loss =28.736144855394805, w0=-0.018209680273629167, w1=0.18511071654782707\n",
      "Gradient Descent(5/99): loss =13.189003277166181, w0=-0.009226348691637966, w1=0.1795173566318898\n",
      "Gradient Descent(6/99): loss =7.288870647681852, w0=-0.009395744939131555, w1=0.1702639252634307\n",
      "Gradient Descent(7/99): loss =4.583157653697597, w0=-0.005505047686284448, w1=0.16289238676339132\n",
      "Gradient Descent(8/99): loss =3.1654960327292545, w0=-0.0031236984593443953, w1=0.15596075561136247\n",
      "Gradient Descent(9/99): loss =2.3568353183914232, w0=5.083737575843628e-06, w1=0.15026991933062564\n",
      "Gradient Descent(10/99): loss =1.8647721617160968, w0=0.0028371053123702745, w1=0.14538852220977386\n",
      "Gradient Descent(11/99): loss =1.5477126496295202, w0=0.005735668183251079, w1=0.14131673842714348\n",
      "Gradient Descent(12/99): loss =1.332395239866411, w0=0.008496409151718133, w1=0.13786444838275375\n",
      "Gradient Descent(13/99): loss =1.1791137250187527, w0=0.011173213332902325, w1=0.13494175645594653\n",
      "Gradient Descent(14/99): loss =1.0654897037879605, w0=0.013725858099805865, w1=0.13244425657320008\n",
      "Gradient Descent(15/99): loss =0.9783941169306026, w0=0.016163291396273618, w1=0.13030037925775365\n",
      "Gradient Descent(16/99): loss =0.9097970492022581, w0=0.018479928271562418, w1=0.12844770741802455\n",
      "Gradient Descent(17/99): loss =0.8545738102295662, w0=0.020679987218107813, w1=0.12683869197236405\n",
      "Gradient Descent(18/99): loss =0.8093163805403041, w0=0.022765692374608827, w1=0.1254343821128863\n",
      "Gradient Descent(19/99): loss =0.7716709710483666, w0=0.02474125236472296, w1=0.1242038558249363\n",
      "Gradient Descent(20/99): loss =0.7399572076217948, w0=0.026610602947852648, w1=0.12312189798318211\n",
      "Gradient Descent(21/99): loss =0.7129415216883906, w0=0.02837802077433639, w1=0.12216798332353553\n",
      "Gradient Descent(22/99): loss =0.6896973079639684, w0=0.03004768002365555, w1=0.1213251640379277\n",
      "Gradient Descent(23/99): loss =0.6695153935628962, w0=0.03162374171209648, w1=0.12057935640393884\n",
      "Gradient Descent(24/99): loss =0.6518446624167693, w0=0.03311026651046979, w1=0.11991873306758158\n",
      "Gradient Descent(25/99): loss =0.6362513886354756, w0=0.034511226995065335, w1=0.11933328075784527\n",
      "Gradient Descent(26/99): loss =0.6223905843174713, w0=0.0358304894625015, w1=0.118814446770911\n",
      "Gradient Descent(27/99): loss =0.6099853197894095, w0=0.03707181347923219, w1=0.11835486880995547\n",
      "Gradient Descent(28/99): loss =0.5988114944533472, w0=0.03823884536747966, w1=0.11794816115568289\n",
      "Gradient Descent(29/99): loss =0.5886864332045338, w0=0.03933511459241314, w1=0.11758874704568407\n",
      "Gradient Descent(30/99): loss =0.5794602288178853, w0=0.04036402897612183, w1=0.11727172504201742\n",
      "Gradient Descent(31/99): loss =0.571009092930411, w0=0.04132887077685729, w1=0.11699276231582928\n",
      "Gradient Descent(32/99): loss =0.5632301995717875, w0=0.042232793122760744, w1=0.11674800848406307\n",
      "Gradient Descent(33/99): loss =0.5560376524577425, w0=0.04307881739497197, w1=0.11653402566460594\n",
      "Gradient Descent(34/99): loss =0.5493593077919002, w0=0.043869831550298284, w1=0.11634773119039474\n",
      "Gradient Descent(35/99): loss =0.5431342545220326, w0=0.04460858953429719, w1=0.11618635038401033\n",
      "Gradient Descent(36/99): loss =0.5373108039812937, w0=0.04529771174635346, w1=0.11604737731790563\n",
      "Gradient Descent(37/99): loss =0.5318448770318607, w0=0.04593968651970606, w1=0.11592854198955846\n",
      "Gradient Descent(38/99): loss =0.5266987033971734, w0=0.046536872511000825, w1=0.11582778266420912\n",
      "Gradient Descent(39/99): loss =0.5218397676106193, w0=0.047091501885291515, w1=0.11574322241855249\n",
      "Gradient Descent(40/99): loss =0.5172399508278522, w0=0.047605684162744945, w1=0.11567314911503442\n",
      "Gradient Descent(41/99): loss =0.5128748289729361, w0=0.04808141059593234, w1=0.11561599819910616\n",
      "Gradient Descent(42/99): loss =0.5087230962538003, w0=0.04852055894902186, w1=0.1155703378315807\n",
      "Gradient Descent(43/99): loss =0.5047660896644299, w0=0.0489248985611606, w1=0.11553485596557067\n",
      "Gradient Descent(44/99): loss =0.5009873951803178, w0=0.04929609558775331, w1=0.11550834905162109\n",
      "Gradient Descent(45/99): loss =0.4973725203106933, w0=0.04963571832738999, w1=0.11548971211449002\n",
      "Gradient Descent(46/99): loss =0.49390862076371894, w0=0.049945242555734085, w1=0.1154779299917084\n",
      "Gradient Descent(47/99): loss =0.49058427140960414, w0=0.050226056801258835, w1=0.11547206956175025\n",
      "Gradient Descent(48/99): loss =0.4873892736425667, w0=0.05047946751013174, w1=0.115471272819583\n",
      "Gradient Descent(49/99): loss =0.4843144927604508, w0=0.05070670405894458, w1=0.11547475068165491\n",
      "Gradient Descent(50/99): loss =0.48135172018814476, w0=0.05090892358398783, w1=0.11548177742193881\n",
      "Gradient Descent(51/99): loss =0.4784935563349719, w0=0.051087215604491164, w1=0.11549168565662234\n",
      "Gradient Descent(52/99): loss =0.47573331064880237, w0=0.051242606424645495, w1=0.1155038618080505\n",
      "Gradient Descent(53/99): loss =0.4730649160509579, w0=0.051376063305422215, w1=0.11551774198923102\n",
      "Gradient Descent(54/99): loss =0.47048285543740076, w0=0.0514884984022874, w1=0.1155328082590252\n",
      "Gradient Descent(55/99): loss =0.4679820983376416, w0=0.051580772469012715, w1=0.11554858520545075\n",
      "Gradient Descent(56/99): loss =0.465558046152551, w0=0.05165369833102128, w1=0.11556463682059047\n",
      "Gradient Descent(57/99): loss =0.4632064846609106, w0=0.051708044134203296, w1=0.11558056363566871\n",
      "Gradient Descent(58/99): loss =0.46092354270412006, w0=0.051744536377000265, w1=0.11559600008910324\n",
      "Gradient Descent(59/99): loss =0.4587056561384524, w0=0.05176386273489353, w1=0.11561061210391384\n",
      "Gradient Descent(60/99): loss =0.45654953629222933, w0=0.05176667468733305, w1=0.1156240948538872\n",
      "Gradient Descent(61/99): loss =0.454452142287286, w0=0.0517535899576871, w1=0.11563617070045754\n",
      "Gradient Descent(62/99): loss =0.45241065668498665, w0=0.05172519477705286, w1=0.115646587284441\n",
      "Gradient Descent(63/99): loss =0.45042246400070624, w0=0.05168204598280163, w1=0.11565511575862375\n",
      "Gradient Descent(64/99): loss =0.44848513170025744, w0=0.05162467296259103, w1=0.11566154914880065\n",
      "Gradient Descent(65/99): loss =0.4465963933497619, w0=0.05155357945430335, w1=0.115665700832236\n",
      "Gradient Descent(66/99): loss =0.44475413363896366, w0=0.051469245211997736, w1=0.11566740312370592\n",
      "Gradient Descent(67/99): loss =0.44295637503867147, w0=0.051372127547524994, w1=0.1156665059603125\n",
      "Gradient Descent(68/99): loss =0.44120126588721076, w0=0.05126266275696856, w1=0.11566287567715633\n",
      "Gradient Descent(69/99): loss =0.4394870697296072, w0=0.051141267440564, w1=0.11565639386673836\n",
      "Gradient Descent(70/99): loss =0.4378121557576028, w0=0.05100833972422601, w1=0.11564695631564864\n",
      "Gradient Descent(71/99): loss =0.43617499021927203, w0=0.05086426039028792, w1=0.11563447201270449\n",
      "Gradient Descent(72/99): loss =0.43457412868456907, w0=0.050709393924542996, w1=0.11561886222323445\n",
      "Gradient Descent(73/99): loss =0.4330082090681131, w0=0.050544089486175774, w1=0.11560005962467737\n",
      "Gradient Descent(74/99): loss =0.4314759453232978, w0=0.05036868180668986, w1=0.11557800749908673\n",
      "Gradient Descent(75/99): loss =0.42997612173278504, w0=0.050183492023479326, w1=0.11555265897850497\n",
      "Gradient Descent(76/99): loss =0.4285075877298337, w0=0.049988828453256126, w1=0.1155239763395087\n",
      "Gradient Descent(77/99): loss =0.4270692531930311, w0=0.04978498731013679, w1=0.1154919303435261\n",
      "Gradient Descent(78/99): loss =0.4256600841639809, w0=0.04957225337280859, w1=0.11545649961979948\n",
      "Gradient Descent(79/99): loss =0.42427909894355154, w0=0.04935090060483835, w1=0.11541767008810987\n",
      "Gradient Descent(80/99): loss =0.4229253645275291, w0=0.049121192731854904, w1=0.11537543441860248\n",
      "Gradient Descent(81/99): loss =0.42159799334709225, w0=0.04888338377902934, w1=0.11532979152625215\n",
      "Gradient Descent(82/99): loss =0.42029614028348006, w0=0.04863771857199286, w1=0.11528074609769062\n",
      "Gradient Descent(83/99): loss =0.4190189999297028, w0=0.0483844332040706, w1=0.11522830814828411\n",
      "Gradient Descent(84/99): loss =0.41776580407517544, w0=0.04812375547246907, w1=0.11517249260750143\n",
      "Gradient Descent(85/99): loss =0.4165358193918041, w0=0.047855905285833715, w1=0.11511331893075223\n",
      "Gradient Descent(86/99): loss =0.4153283453024026, w0=0.047581095045390466, w1=0.11505081073600242\n",
      "Gradient Descent(87/99): loss =0.4141427120143567, w0=0.04729953000169919, w1=0.11498499546359116\n",
      "Gradient Descent(88/99): loss =0.4129782787032744, w0=0.04701140858887732, w1=0.11491590405778179\n",
      "Gradient Descent(89/99): loss =0.4118344318329495, w0=0.04671692273799613, w1=0.11484357066867831\n",
      "Gradient Descent(90/99): loss =0.41071058359938983, w0=0.04641625817121034, w1=0.11476803237323101\n",
      "Gradient Descent(91/99): loss =0.40960617048790166, w0=0.0461095946780518, w1=0.1146893289141396\n",
      "Gradient Descent(92/99): loss =0.40852065193334425, w0=0.04579710637519957, w1=0.11460750245554077\n",
      "Gradient Descent(93/99): loss =0.40745350907464745, w0=0.04547896195093059, w1=0.11452259735443979\n",
      "Gradient Descent(94/99): loss =0.4064042435955668, w0=0.045155324895356244, w1=0.11443465994691342\n",
      "Gradient Descent(95/99): loss =0.40537237664443715, w0=0.044826353717460195, w1=0.11434373834817381\n",
      "Gradient Descent(96/99): loss =0.4043574478263736, w0=0.044492202149870375, w1=0.11424988226564163\n",
      "Gradient Descent(97/99): loss =0.4033590142620065, w0=0.044153019342223127, w1=0.11415314282423054\n",
      "Gradient Descent(98/99): loss =0.4023766497073828, w0=0.04380895004390877, w1=0.11405357240309574\n",
      "Gradient Descent(99/99): loss =0.4014099437301754, w0=0.0434601347769253, w1=0.1139512244831463\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =211976.02441586746, w0=-0.0037917939181667387, w1=0.19999989408574242\n",
      "Gradient Descent(2/99): loss =314.3881707520355, w0=-0.015489719588774363, w1=0.188568952166964\n",
      "Gradient Descent(3/99): loss =72.91994467299735, w0=0.06040463756810566, w1=0.21075380498224586\n",
      "Gradient Descent(4/99): loss =38.796736288851854, w0=0.05688257020558875, w1=0.20920513091836593\n",
      "Gradient Descent(5/99): loss =26.268834686239323, w0=0.07473269244395132, w1=0.20756339320458111\n",
      "Gradient Descent(6/99): loss =20.026999018084123, w0=0.07847042763122025, w1=0.20312347405124687\n",
      "Gradient Descent(7/99): loss =16.274824420398556, w0=0.08555468195685306, w1=0.19929631397456604\n",
      "Gradient Descent(8/99): loss =13.75172096358511, w0=0.09003708634531274, w1=0.19563637643942777\n",
      "Gradient Descent(9/99): loss =11.931607003248363, w0=0.09470974712714218, w1=0.19261271029385416\n",
      "Gradient Descent(10/99): loss =10.549876259640707, w0=0.09858142783289318, w1=0.19005356268790502\n",
      "Gradient Descent(11/99): loss =9.458264402507885, w0=0.10210187390775115, w1=0.18794325457806993\n",
      "Gradient Descent(12/99): loss =8.567979578539958, w0=0.10512428802498693, w1=0.1861758464973115\n",
      "Gradient Descent(13/99): loss =7.823270056241607, w0=0.10774497148555608, w1=0.1846853571427582\n",
      "Gradient Descent(14/99): loss =7.1877149095476875, w0=0.10996886878849572, w1=0.18340442371648735\n",
      "Gradient Descent(15/99): loss =6.636648594252774, w0=0.11184192409050706, w1=0.18228322639894906\n",
      "Gradient Descent(16/99): loss =6.152798474442989, w0=0.11339493794291153, w1=0.18128065402872653\n",
      "Gradient Descent(17/99): loss =5.723689885262581, w0=0.1146637247790205, w1=0.18036547062073108\n",
      "Gradient Descent(18/99): loss =5.3400571964867565, w0=0.11567944195085994, w1=0.1795133309389586\n",
      "Gradient Descent(19/99): loss =4.994842994425277, w0=0.11647160850481082, w1=0.17870572441643337\n",
      "Gradient Descent(20/99): loss =4.682550016879228, w0=0.1170665380964516, w1=0.17792852004097437\n",
      "Gradient Descent(21/99): loss =4.398810049108014, w0=0.11748794035816162, w1=0.17717104119630003\n",
      "Gradient Descent(22/99): loss =4.140089750036436, w0=0.11775684467524733, w1=0.17642524983025998\n",
      "Gradient Descent(23/99): loss =3.903485143503807, w0=0.11789189622032249, w1=0.17568515065896564\n",
      "Gradient Descent(24/99): loss =3.6865749835540083, w0=0.11790954529753098, w1=0.17494631688791007\n",
      "Gradient Descent(25/99): loss =3.4873141501715725, w0=0.11782428733257433, w1=0.17420553089285115\n",
      "Gradient Descent(26/99): loss =3.303954862068227, w0=0.11764887132973677, w1=0.17346050533309013\n",
      "Gradient Descent(27/99): loss =3.1349875980340727, w0=0.11739450001761134, w1=0.17270966925683215\n",
      "Gradient Descent(28/99): loss =2.979096220548862, w0=0.11707100843704153, w1=0.17195200271972483\n",
      "Gradient Descent(29/99): loss =2.8351234841808126, w0=0.11668702548985391, w1=0.17118690909549322\n",
      "Gradient Descent(30/99): loss =2.7020442331378574, w0=0.11625011758049195, w1=0.17041411609865292\n",
      "Gradient Descent(31/99): loss =2.5789443540012034, w0=0.11576691643259121, w1=0.16963359895463892\n",
      "Gradient Descent(32/99): loss =2.465004077255666, w0=0.11524323229663148, w1=0.16884552059918165\n",
      "Gradient Descent(33/99): loss =2.35948459319682, w0=0.11468415414335593, w1=0.16805018504356203\n",
      "Gradient Descent(34/99): loss =2.261717214077423, w0=0.11409413819118756, w1=0.16724800093410255\n",
      "Gradient Descent(35/99): loss =2.17109450745143, w0=0.11347708605811672, w1=0.16643945303491342\n",
      "Gradient Descent(36/99): loss =2.0870629672067316, w0=0.11283641368002695, w1=0.16562507988618316\n",
      "Gradient Descent(37/99): loss =2.00911689343839, w0=0.11217511202736105, w1=0.16480545629205007\n",
      "Gradient Descent(38/99): loss =1.9367932302768724, w0=0.11149580053907009, w1=0.1639811795971618\n",
      "Gradient Descent(39/99): loss =1.8696671692162086, w0=0.1108007740956875, w1=0.16315285894473142\n",
      "Gradient Descent(40/99): loss =1.807348369506971, w0=0.11009204426504784, w1=0.16232110688786064\n",
      "Gradient Descent(41/99): loss =1.7494776804930707, w0=0.10937137547607996, w1=0.16148653286353468\n",
      "Gradient Descent(42/99): loss =1.695724276087525, w0=0.1086403167061875, w1=0.16064973814480543\n",
      "Gradient Descent(43/99): loss =1.645783130895521, w0=0.107900229205213, w1=0.15981131196877504\n",
      "Gradient Descent(44/99): loss =1.5993727822830957, w0=0.1071523107228328, w1=0.15897182860171272\n",
      "Gradient Descent(45/99): loss =1.5562333340600938, w0=0.10639761665570997, w1=0.15813184515227202\n",
      "Gradient Descent(46/99): loss =1.5161246662232177, w0=0.10563707848520613, w1=0.15729189998257193\n",
      "Gradient Descent(47/99): loss =1.4788248220101812, w0=0.1048715198354075, w1=0.15645251159734144\n",
      "Gradient Descent(48/99): loss =1.4441285488175113, w0=0.1041016704442291, w1=0.1556141779152908\n",
      "Gradient Descent(49/99): loss =1.4118459736863715, w0=0.10332817830705425, w1=0.1547773758458103\n",
      "Gradient Descent(50/99): loss =1.3818013973309633, w0=0.10255162022242685, w1=0.15394256110911625\n",
      "Gradient Descent(51/99): loss =1.3538321932761765, w0=0.1017725109424503, w1=0.15311016824992027\n",
      "Gradient Descent(52/99): loss =1.3277878007399493, w0=0.1009913111064969, w1=0.1522806108042483\n",
      "Gradient Descent(53/99): loss =1.303528801559196, w0=0.10020843411534948, w1=0.15145428158669175\n",
      "Gradient Descent(54/99): loss =1.2809260728063365, w0=0.09942425208375479, w1=0.1506315530715324\n",
      "Gradient Descent(55/99): loss =1.2598600078452502, w0=0.09863910099235149, w1=0.14981277784615135\n",
      "Gradient Descent(56/99): loss =1.2402197994838322, w0=0.09785328514484397, w1=0.14899828911915639\n",
      "Gradient Descent(57/99): loss =1.2219027796358994, w0=0.09706708102293903, w1=0.148188401268928\n",
      "Gradient Descent(58/99): loss =1.2048138105393582, w0=0.09628074061977283, w1=0.14738341042094413\n",
      "Gradient Descent(59/99): loss =1.1888647231145693, w0=0.0954944943221672, w1=0.14658359504441473\n",
      "Gradient Descent(60/99): loss =1.1739737985056127, w0=0.09470855340292093, w1=0.14578921656053323\n",
      "Gradient Descent(61/99): loss =1.1600652892423118, w0=0.09392311217632544, w1=0.14500051995610921\n",
      "Gradient Descent(62/99): loss =1.1470689768040392, w0=0.09313834986307111, w1=0.1442177343975431\n",
      "Gradient Descent(63/99): loss =1.1349197626664178, w0=0.09235443220456929, w1=0.14344107384108817\n",
      "Gradient Descent(64/99): loss =1.1235572901764115, w0=0.09157151286135118, w1=0.14267073763615717\n",
      "Gradient Descent(65/99): loss =1.1129255948354952, w0=0.09078973462552864, w1=0.1419069111191001\n",
      "Gradient Descent(66/99): loss =1.1029727807793726, w0=0.090009230473231, w1=0.14114976619543199\n",
      "Gradient Descent(67/99): loss =1.0936507214296216, w0=0.08923012447939105, w1=0.14039946190894653\n",
      "Gradient Descent(68/99): loss =1.084914782460831, w0=0.08845253261417944, w1=0.13965614499652768\n",
      "Gradient Descent(69/99): loss =1.0767235653786285, w0=0.08767656343771991, w1=0.13891995042778268\n",
      "Gradient Descent(70/99): loss =1.0690386701415915, w0=0.08690231870740692, w1=0.1381910019288769\n",
      "Gradient Descent(71/99): loss =1.0618244753850639, w0=0.08612989391014769, w1=0.13746941249016154\n",
      "Gradient Descent(72/99): loss =1.0550479349188087, w0=0.08535937873012181, w1=0.13675528485735952\n",
      "Gradient Descent(73/99): loss =1.0486783892744251, w0=0.08459085746115752, w1=0.13604871200621677\n",
      "Gradient Descent(74/99): loss =1.0426873911736154, w0=0.08382440937153553, w1=0.1353497776006428\n",
      "Gradient Descent(75/99): loss =1.037048543875542, w0=0.08306010902791958, w1=0.1346585564344583\n",
      "Gradient Descent(76/99): loss =1.0317373514415145, w0=0.08229802658415597, w1=0.1339751148569441\n",
      "Gradient Descent(77/99): loss =1.0267310800287393, w0=0.08153822803986033, w1=0.13329951118244712\n",
      "Gradient Descent(78/99): loss =1.0220086293924129, w0=0.08078077547300126, w1=0.13263179608434633\n",
      "Gradient Descent(79/99): loss =1.017550413837669, w0=0.08002572725008182, w1=0.13197201297372094\n",
      "Gradient Descent(80/99): loss =1.013338251920176, w0=0.07927313821699701, w1=0.13132019836309067\n",
      "Gradient Descent(81/99): loss =1.0093552642469839, w0=0.07852305987319694, w1=0.13067638221562045\n",
      "Gradient Descent(82/99): loss =1.0055857787779547, w0=0.07777554053140097, w1=0.13004058828019688\n",
      "Gradient Descent(83/99): loss =1.002015243073033, w0=0.07703062546477894, w1=0.12941283441279458\n",
      "Gradient Descent(84/99): loss =0.998630142972136, w0=0.07628835704323352, w1=0.12879313288455618\n",
      "Gradient Descent(85/99): loss =0.9954179272327403, w0=0.07554877486017701, w1=0.1281814906770129\n",
      "Gradient Descent(86/99): loss =0.9923669376856623, w0=0.07481191585098988, w1=0.12757790976487218\n",
      "Gradient Descent(87/99): loss =0.9894663445022422, w0=0.07407781440417238, w1=0.1269823873867963\n",
      "Gradient Descent(88/99): loss =0.9867060861963785, w0=0.073346502466051, w1=0.12639491630459146\n",
      "Gradient Descent(89/99): loss =0.984076814012825, w0=0.07261800963977318, w1=0.12581548505122087\n",
      "Gradient Descent(90/99): loss =0.9815698403790183, w0=0.07189236327921533, w1=0.12524407816804775\n",
      "Gradient Descent(91/99): loss =0.9791770911216159, w0=0.07116958857833638, w1=0.12468067643170651\n",
      "Gradient Descent(92/99): loss =0.9768910611710409, w0=0.07044970865643042, w1=0.1241252570709905\n",
      "Gradient Descent(93/99): loss =0.9747047734978219, w0=0.06973274463966515, w1=0.12357779397413594\n",
      "Gradient Descent(94/99): loss =0.9726117410434175, w0=0.06901871573923637, w1=0.12303825788687105\n",
      "Gradient Descent(95/99): loss =0.9706059314257747, w0=0.06830763932642046, w1=0.12250661660158899\n",
      "Gradient Descent(96/99): loss =0.9686817342160751, w0=0.06759953100476626, w1=0.12198283513799328\n",
      "Gradient Descent(97/99): loss =0.9668339305981399, w0=0.06689440467963359, w1=0.12146687591555294\n",
      "Gradient Descent(98/99): loss =0.9650576652358683, w0=0.06619227262525629, w1=0.12095869891809438\n",
      "Gradient Descent(99/99): loss =0.9633484201869416, w0=0.0654931455494834, w1=0.12045826185084638\n",
      "Optimizing degree 4/15, model: least_squares_GD, arguments: {'max_iters': 100}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =2977822045.739751, w0=-0.19999980893221234, w1=0.19999999986692307\n",
      "Gradient Descent(2/99): loss =5656.667728338617, w0=-0.19714781578414825, w1=0.20068611279491058\n",
      "Gradient Descent(3/99): loss =4430.069236259626, w0=-0.19569300882606036, w1=0.2013431370676733\n",
      "Gradient Descent(4/99): loss =4087.9170794597226, w0=-0.19443410228957936, w1=0.20179445336544316\n",
      "Gradient Descent(5/99): loss =3859.646071376333, w0=-0.19336726017473477, w1=0.20208267378837627\n",
      "Gradient Descent(6/99): loss =3685.9881159497777, w0=-0.19240917254128567, w1=0.2022708575660618\n",
      "Gradient Descent(7/99): loss =3544.2585924672903, w0=-0.19152960745086023, w1=0.20239376009599233\n",
      "Gradient Descent(8/99): loss =3423.2453792943447, w0=-0.19070582628083538, w1=0.202470776857143\n",
      "Gradient Descent(9/99): loss =3316.7913089871777, w0=-0.18992415389412406, w1=0.20251255024236586\n",
      "Gradient Descent(10/99): loss =3221.281350328861, w0=-0.18917532863302833, w1=0.20252505727006717\n",
      "Gradient Descent(11/99): loss =3134.457163685096, w0=-0.1884531491448891, w1=0.20251180573233055\n",
      "Gradient Descent(12/99): loss =3054.8144524615363, w0=-0.18775335060040713, w1=0.20247501935658857\n",
      "Gradient Descent(13/99): loss =2981.285041054255, w0=-0.1870729611813457, w1=0.20241625842778788\n",
      "Gradient Descent(14/99): loss =2913.0647159846744, w0=-0.1864098661198989, w1=0.20233674199294122\n",
      "Gradient Descent(15/99): loss =2849.5176425386003, w0=-0.18576252168751547, w1=0.20223751229047693\n",
      "Gradient Descent(16/99): loss =2790.121761569582, w0=-0.1851297629109267, w1=0.20211951705467002\n",
      "Gradient Descent(17/99): loss =2734.436489162021, w0=-0.18451067482020775, w1=0.20198364939577568\n",
      "Gradient Descent(18/99): loss =2682.0827790086214, w0=-0.18390450644416642, w1=0.20183076608559297\n",
      "Gradient Descent(19/99): loss =2632.7301915319104, w0=-0.18331061392800774, w1=0.2016616950920256\n",
      "Gradient Descent(20/99): loss =2586.088046660478, w0=-0.18272842345854293, w1=0.20147723797635342\n",
      "Gradient Descent(21/99): loss =2541.899040904849, w0=-0.1821574076147689, w1=0.20127817002607806\n",
      "Gradient Descent(22/99): loss =2499.9344147971683, w0=-0.18159707073767958, w1=0.20106523956414488\n",
      "Gradient Descent(23/99): loss =2459.990142617744, w0=-0.18104694027777904, w1=0.2008391671293965\n",
      "Gradient Descent(24/99): loss =2421.883830197148, w0=-0.18050656202639045, w1=0.200600644837646\n",
      "Gradient Descent(25/99): loss =2385.452127096875, w0=-0.17997549779854854, w1=0.2003503360367486\n",
      "Gradient Descent(26/99): loss =2350.5485288097166, w0=-0.17945332459763472, w1=0.20008887527258495\n",
      "Gradient Descent(27/99): loss =2317.041485556573, w0=-0.17893963461445533, w1=0.19981686853847838\n",
      "Gradient Descent(28/99): loss =2284.8127591830184, w0=-0.17843403563746685, w1=0.19953489376312128\n",
      "Gradient Descent(29/99): loss =2253.755985387116, w0=-0.1779361516053383, w1=0.19924350148823944\n",
      "Gradient Descent(30/99): loss =2223.7754088268593, w0=-0.17744562313853024, w1=0.19894321568989948\n",
      "Gradient Descent(31/99): loss =2194.784765698893, w0=-0.17696210795762624, w1=0.19863453470282944\n",
      "Gradient Descent(32/99): loss =2166.70629337873, w0=-0.17648528114312761, w1=0.19831793221344074\n",
      "Gradient Descent(33/99): loss =2139.469850388911, w0=-0.1760148352216705, w1=0.19799385829343932\n",
      "Gradient Descent(34/99): loss =2113.0121327514744, w0=-0.17555048008235732, w1=0.19766274045154877\n",
      "Gradient Descent(35/99): loss =2087.2759749551324, w0=-0.17509194273779938, w1=0.19732498468576562\n",
      "Gradient Descent(36/99): loss =2062.209725499936, w0=-0.17463896695013134, w1=0.19698097652270194\n",
      "Gradient Descent(37/99): loss =2037.766688387702, w0=-0.17419131274449717, w1=0.19663108203398877\n",
      "Gradient Descent(38/99): loss =2013.904623084088, w0=-0.17374875583257168, w1=0.19627564882249152\n",
      "Gradient Descent(39/99): loss =1990.58529644365, w0=-0.17331108696743783, w1=0.19591500697331565\n",
      "Gradient Descent(40/99): loss =1967.7740809026247, w0=-0.17287811124916716, w1=0.19554946996634046\n",
      "Gradient Descent(41/99): loss =1945.4395939357253, w0=-0.1724496473981395, w1=0.19517933554839317\n",
      "Gradient Descent(42/99): loss =1923.5533743655828, w0=-0.17202552701073628, w1=0.1948048865642322\n",
      "Gradient Descent(43/99): loss =1902.0895916240775, w0=-0.17160559380970833, w1=0.19442639174630913\n",
      "Gradient Descent(44/99): loss =1881.0247845076333, w0=-0.1711897028993441, w1=0.19404410646387427\n",
      "Gradient Descent(45/99): loss =1860.337626354238, w0=-0.17077772003359767, w1=0.19365827343242206\n",
      "Gradient Descent(46/99): loss =1840.0087139076804, w0=-0.1703695209035949, w1=0.19326912338477648\n",
      "Gradient Descent(47/99): loss =1820.020377430906, w0=-0.16996499044942548, w1=0.19287687570531872\n",
      "Gradient Descent(48/99): loss =1800.3565098918632, w0=-0.16956402219983518, w1=0.19248173902898635\n",
      "Gradient Descent(49/99): loss =1781.0024132761414, w0=-0.16916651764234134, w1=0.19208391180673856\n",
      "Gradient Descent(50/99): loss =1761.9446602854111, w0=-0.16877238562538605, w1=0.19168358283920564\n",
      "Gradient Descent(51/99): loss =1743.1709698624454, w0=-0.1683815417933948, w1=0.1912809317802293\n",
      "Gradient Descent(52/99): loss =1724.670095145176, w0=-0.1679939080550016, w1=0.19087612961196665\n",
      "Gradient Descent(53/99): loss =1706.4317225964421, w0=-0.16760941208421856, w1=0.1904693390931787\n",
      "Gradient Descent(54/99): loss =1688.446381184608, w0=-0.1672279868539469, w1=0.19006071518226272\n",
      "Gradient Descent(55/99): loss =1670.7053606052286, w0=-0.16684957020093563, w1=0.1896504054365166\n",
      "Gradient Descent(56/99): loss =1653.2006376367285, w0=-0.1664741044210747, w1=0.1892385503890507\n",
      "Gradient Descent(57/99): loss =1635.9248098150858, w0=-0.16610153589375334, w1=0.1888252839046868\n",
      "Gradient Descent(58/99): loss =1618.8710356950432, w0=-0.16573181473390805, w1=0.18841073351610782\n",
      "Gradient Descent(59/99): loss =1602.0329810392498, w0=-0.16536489447031844, w1=0.18799502074144883\n",
      "Gradient Descent(60/99): loss =1585.404770343167, w0=-0.16500073174867838, w1=0.1875782613844474\n",
      "Gradient Descent(61/99): loss =1568.9809431630933, w0=-0.16463928605796385, w1=0.1871605658182022\n",
      "Gradient Descent(62/99): loss =1552.7564147681576, w0=-0.16428051947863392, w1=0.18674203925352376\n",
      "Gradient Descent(63/99): loss =1536.726440685192, w0=-0.16392439645123366, w1=0.18632278199279734\n",
      "Gradient Descent(64/99): loss =1520.88658474859, w0=-0.1635708835640104, w1=0.1859028896702209\n",
      "Gradient Descent(65/99): loss =1505.232690306049, w0=-0.16321994935820833, w1=0.1854824534792239\n",
      "Gradient Descent(66/99): loss =1489.760854266043, w0=-0.1628715641497645, w1=0.18506156038782187\n",
      "Gradient Descent(67/99): loss =1474.4674037042198, w0=-0.16252569986619259, w1=0.18464029334261248\n",
      "Gradient Descent(68/99): loss =1459.3488747741922, w0=-0.16218232989750594, w1=0.1842187314620737\n",
      "Gradient Descent(69/99): loss =1444.4019936935829, w0=-0.16184142896009798, w1=0.183796950219782\n",
      "Gradient Descent(70/99): loss =1429.6236595990067, w0=-0.16150297297256322, w1=0.1833750216181289\n",
      "Gradient Descent(71/99): loss =1415.0109290843118, w0=-0.16116693894250836, w1=0.18295301435307798\n",
      "Gradient Descent(72/99): loss =1400.5610022548376, w0=-0.16083330486346545, w1=0.18253099397046932\n",
      "Gradient Descent(73/99): loss =1386.2712101471614, w0=-0.16050204962108064, w1=0.18210902301434698\n",
      "Gradient Descent(74/99): loss =1372.1390033787588, w0=-0.16017315290781078, w1=0.18168716116775527\n",
      "Gradient Descent(75/99): loss =1358.1619419055326, w0=-0.15984659514541621, w1=0.18126546538642163\n",
      "Gradient Descent(76/99): loss =1344.337685777297, w0=-0.15952235741459078, w1=0.1808439900257187\n",
      "Gradient Descent(77/99): loss =1330.663986792252, w0=-0.159200421391121, w1=0.18042278696127342\n",
      "Gradient Descent(78/99): loss =1317.138680961315, w0=-0.15888076928801265, w1=0.18000190570356966\n",
      "Gradient Descent(79/99): loss =1303.7596817020708, w0=-0.1585633838030678, w1=0.1795813935068687\n",
      "Gradient Descent(80/99): loss =1290.5249736900462, w0=-0.1582482480714367, w1=0.17916129547275356\n",
      "Gradient Descent(81/99): loss =1277.4326073022414, w0=-0.1579353456227067, w1=0.1787416546485848\n",
      "Gradient Descent(82/99): loss =1264.4806935942897, w0=-0.1576246603421274, w1=0.17832251212113778\n",
      "Gradient Descent(83/99): loss =1251.6673997584592, w0=-0.15731617643560336, w1=0.17790390710567658\n",
      "Gradient Descent(84/99): loss =1238.990945014965, w0=-0.15700987839811706, w1=0.17748587703070423\n",
      "Gradient Descent(85/99): loss =1226.4495968937606, w0=-0.15670575098527315, w1=0.17706845761861534\n",
      "Gradient Descent(86/99): loss =1214.0416678682564, w0=-0.1564037791876814, w1=0.1766516829624645\n",
      "Gradient Descent(87/99): loss =1201.7655123062204, w0=-0.1561039482079195, w1=0.17623558559905123\n",
      "Gradient Descent(88/99): loss =1189.6195237066013, w0=-0.15580624343984018, w1=0.17582019657851136\n",
      "Gradient Descent(89/99): loss =1177.6021321940725, w0=-0.1555106504500061, w1=0.17540554553059395\n",
      "Gradient Descent(90/99): loss =1165.7118022459276, w0=-0.15521715496105654, w1=0.17499166072779274\n",
      "Gradient Descent(91/99): loss =1153.9470306284868, w0=-0.15492574283682564, w1=0.17457856914549189\n",
      "Gradient Descent(92/99): loss =1142.3063445223856, w0=-0.15463640006904897, w1=0.17416629651927723\n",
      "Gradient Descent(93/99): loss =1130.7882998182401, w0=-0.15434911276550878, w1=0.17375486739955545\n",
      "Gradient Descent(94/99): loss =1119.3914795659477, w0=-0.15406386713948236, w1=0.17334430520361638\n",
      "Gradient Descent(95/99): loss =1108.114492562592, w0=-0.15378064950036968, w1=0.172934632265266\n",
      "Gradient Descent(96/99): loss =1096.9559720653867, w0=-0.15349944624538747, w1=0.17252586988215116\n",
      "Gradient Descent(97/99): loss =1085.914574617449, w0=-0.15322024385222746, w1=0.17211803836089012\n",
      "Gradient Descent(98/99): loss =1074.9889789753918, w0=-0.15294302887258546, w1=0.17171115706011758\n",
      "Gradient Descent(99/99): loss =1064.177885128821, w0=-0.15266778792647642, w1=0.17130524443154643\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =2977362013.6304355, w0=-0.19999982163680083, w1=0.19999999983797034\n",
      "Gradient Descent(2/99): loss =3060.831651207874, w0=-0.19743047007962675, w1=0.20103661857519367\n",
      "Gradient Descent(3/99): loss =2639.991926750826, w0=-0.1957272080850618, w1=0.20119881661378544\n",
      "Gradient Descent(4/99): loss =2566.62353715349, w0=-0.19404197143874882, w1=0.20139475240122523\n",
      "Gradient Descent(5/99): loss =2506.948264155589, w0=-0.19253957480421974, w1=0.20148902470413396\n",
      "Gradient Descent(6/99): loss =2456.2277084300135, w0=-0.19112242579611483, w1=0.20154509167179427\n",
      "Gradient Descent(7/99): loss =2412.420633800117, w0=-0.1898026243095456, w1=0.20155167208245225\n",
      "Gradient Descent(8/99): loss =2374.050930826077, w0=-0.18855777070971175, w1=0.20152295129019007\n",
      "Gradient Descent(9/99): loss =2339.980129993337, w0=-0.18738180549327665, w1=0.20146130635372078\n",
      "Gradient Descent(10/99): loss =2309.3186995162528, w0=-0.1862651301124536, w1=0.20137190650558867\n",
      "Gradient Descent(11/99): loss =2281.3692937979067, w0=-0.18520132102596978, w1=0.20125779368755428\n",
      "Gradient Descent(12/99): loss =2255.5844444644918, w0=-0.184184248421645, w1=0.2011220090987745\n",
      "Gradient Descent(13/99): loss =2231.5339686878874, w0=-0.18320886400516673, w1=0.2009669981464008\n",
      "Gradient Descent(14/99): loss =2208.8796079943913, w0=-0.1822707053457431, w1=0.20079495611585724\n",
      "Gradient Descent(15/99): loss =2187.3552240525646, w0=-0.1813659214221088, w1=0.20060778606806337\n",
      "Gradient Descent(16/99): loss =2166.7513010048497, w0=-0.1804911376471461, w1=0.2004071761201113\n",
      "Gradient Descent(17/99): loss =2146.9028016287007, w0=-0.1796434036375151, w1=0.20019461743423994\n",
      "Gradient Descent(18/99): loss =2127.67964223125, w0=-0.17882012811173598, w1=0.19997143428852437\n",
      "Gradient Descent(19/99): loss =2108.979216232709, w0=-0.17801903238882577, w1=0.19973880294599833\n",
      "Gradient Descent(20/99): loss =2090.7205226333294, w0=-0.17723810806182158, w1=0.1994977698353788\n",
      "Gradient Descent(21/99): loss =2072.839552964815, w0=-0.17647558210251077, w1=0.19924926634267887\n",
      "Gradient Descent(22/99): loss =2055.285665834367, w0=-0.17572988660004146, w1=0.1989941219869446\n",
      "Gradient Descent(23/99): loss =2038.0187369211935, w0=-0.17499963291920265, w1=0.1987330758519241\n",
      "Gradient Descent(24/99): loss =2021.0069181190922, w0=-0.17428358936220045, w1=0.1984667866995001\n",
      "Gradient Descent(25/99): loss =2004.2248753459262, w0=-0.17358066186829177, w1=0.19819584189235143\n",
      "Gradient Descent(26/99): loss =1987.6524025876668, w0=-0.1728898772660722, w1=0.1979207653029651\n",
      "Gradient Descent(27/99): loss =1971.273331725392, w0=-0.17221036871556394, w1=0.1976420243264218\n",
      "Gradient Descent(28/99): loss =1955.0746749348082, w0=-0.17154136302338763, w1=0.19736003610686934\n",
      "Gradient Descent(29/99): loss =1939.045949980543, w0=-0.17088216957063002, w1=0.19707517306811181\n",
      "Gradient Descent(30/99): loss =1923.178649354229, w0=-0.17023217063172746, w1=0.19678776782843793\n",
      "Gradient Descent(31/99): loss =1907.4658225538558, w0=-0.1695908128970974, w1=0.19649811756933938\n",
      "Gradient Descent(32/99): loss =1891.9017473620809, w0=-0.168957600039718, w1=0.19620648791966758\n",
      "Gradient Descent(33/99): loss =1876.481671137726, w0=-0.16833208618908455, w1=0.19591311640953343\n",
      "Gradient Descent(34/99): loss =1861.2016071882954, w0=-0.1677138701952897, w1=0.19561821554205774\n",
      "Gradient Descent(35/99): loss =1846.0581744787328, w0=-0.16710259058229113, w1=0.19532197552560895\n",
      "Gradient Descent(36/99): loss =1831.0484714378786, w0=-0.16649792110322, w1=0.19502456670435933\n",
      "Gradient Descent(37/99): loss =1816.1699765952103, w0=-0.16589956682231122, w1=0.19472614172073896\n",
      "Gradient Descent(38/99): loss =1801.4204703306186, w0=-0.16530726065804208, w1=0.19442683743960318\n",
      "Gradient Descent(39/99): loss =1786.7979732393146, w0=-0.16472076033063654, w1=0.19412677666059214\n",
      "Gradient Descent(40/99): loss =1772.300697573043, w0=-0.16413984566445405, w1=0.19382606964220092\n",
      "Gradient Descent(41/99): loss =1757.927008973229, w0=-0.16356431620212694, w1=0.19352481545845054\n",
      "Gradient Descent(42/99): loss =1743.6753963052238, w0=-0.16299398909278875, w1=0.19322310320671798\n",
      "Gradient Descent(43/99): loss =1729.5444478696845, w0=-0.16242869722148046, w1=0.1929210130832117\n",
      "Gradient Descent(44/99): loss =1715.5328326344961, w0=-0.16186828755093566, w1=0.19261861734074015\n",
      "Gradient Descent(45/99): loss =1701.6392854196167, w0=-0.16131261965052163, w1=0.19231598114178783\n",
      "Gradient Descent(46/99): loss =1687.8625951946601, w0=-0.16076156439022432, w1=0.19201316331846252\n",
      "Gradient Descent(47/99): loss =1674.2015958279126, w0=-0.16021500278027817, w1=0.1917102170495903\n",
      "Gradient Descent(48/99): loss =1660.6551587662814, w0=-0.15967282493940693, w1=0.19140719046409066\n",
      "Gradient Descent(49/99): loss =1647.2221872364696, w0=-0.15913492917670918, w1=0.19110412717874872\n",
      "Gradient Descent(50/99): loss =1633.9016116448176, w0=-0.1586012211740298, w1=0.19080106677759914\n",
      "Gradient Descent(51/99): loss =1620.6923859219023, w0=-0.15807161325724, w1=0.1904980452393356\n",
      "Gradient Descent(52/99): loss =1607.5934846119508, w0=-0.15754602374623417, w1=0.19019509531844792\n",
      "Gradient Descent(53/99): loss =1594.6039005496352, w0=-0.15702437637466612, w1=0.1898922468851569\n",
      "Gradient Descent(54/99): loss =1581.7226430002675, w0=-0.15650659977151243, w1=0.1895895272286557\n",
      "Gradient Descent(55/99): loss =1568.9487361657407, w0=-0.15599262699748503, w1=0.18928696132766748\n",
      "Gradient Descent(56/99): loss =1556.2812179792932, w0=-0.15548239513013656, w1=0.188984572091886\n",
      "Gradient Descent(57/99): loss =1543.7191391284932, w0=-0.15497584489222277, w1=0.1886823805774727\n",
      "Gradient Descent(58/99): loss =1531.261562258687, w0=-0.15447292031852047, w1=0.18838040617943297\n",
      "Gradient Descent(59/99): loss =1518.9075613192645, w0=-0.15397356845685725, w1=0.18807866680338403\n",
      "Gradient Descent(60/99): loss =1506.6562210230852, w0=-0.15347773909959986, w1=0.18777717901895052\n",
      "Gradient Descent(61/99): loss =1494.5066363956457, w0=-0.1529853845422802, w1=0.18747595819677784\n",
      "Gradient Descent(62/99): loss =1482.4579123955543, w0=-0.15249645936641826, w1=0.1871750186309353\n",
      "Gradient Descent(63/99): loss =1470.5091635917408, w0=-0.15201092024393692, w1=0.1868743736482868\n",
      "Gradient Descent(64/99): loss =1458.659513885898, w0=-0.1515287257608592, w1=0.18657403570623415\n",
      "Gradient Descent(65/99): loss =1446.908096271094, w0=-0.15104983625823917, w1=0.18627401648008474\n",
      "Gradient Descent(66/99): loss =1435.2540526193798, w0=-0.15057421368850793, w1=0.18597432694115829\n",
      "Gradient Descent(67/99): loss =1423.6965334926995, w0=-0.1501018214856196, w1=0.1856749774266265\n",
      "Gradient Descent(68/99): loss =1412.2346979726544, w0=-0.14963262444756142, w1=0.18537597770197095\n",
      "Gradient Descent(69/99): loss =1400.8677135055316, w0=-0.14916658862995094, w1=0.1850773370168485\n",
      "Gradient Descent(70/99): loss =1389.594755759805, w0=-0.14870368124958366, w1=0.18477906415506795\n",
      "Gradient Descent(71/99): loss =1378.415008493864, w0=-0.1482438705969186, w1=0.18448116747930537\n",
      "Gradient Descent(72/99): loss =1367.3276634321976, w0=-0.14778712595659907, w1=0.18418365497111786\n",
      "Gradient Descent(73/99): loss =1356.3319201486195, w0=-0.14733341753520382, w1=0.183886534266755\n",
      "Gradient Descent(74/99): loss =1345.4269859553972, w0=-0.14688271639550932, w1=0.18358981268921348\n",
      "Gradient Descent(75/99): loss =1334.6120757974152, w0=-0.1464349943966211, w1=0.1832934972769325\n",
      "Gradient Descent(76/99): loss =1323.8864121506049, w0=-0.1459902241393999, w1=0.18299759480948496\n",
      "Gradient Descent(77/99): loss =1313.2492249241134, w0=-0.1455483789166685, w1=0.18270211183058116\n",
      "Gradient Descent(78/99): loss =1302.699751365698, w0=-0.14510943266773893, w1=0.18240705466866808\n",
      "Gradient Descent(79/99): loss =1292.2372359699932, w0=-0.14467335993684755, w1=0.18211242945537728\n",
      "Gradient Descent(80/99): loss =1281.86093038934, w0=-0.1442401358351277, w1=0.18181824214204687\n",
      "Gradient Descent(81/99): loss =1271.570093346924, w0=-0.14380973600578745, w1=0.18152449851451968\n",
      "Gradient Descent(82/99): loss =1261.363990552008, w0=-0.14338213659219387, w1=0.1812312042063981\n",
      "Gradient Descent(83/99): loss =1251.241894617113, w0=-0.1429573142085952, w1=0.1809383647109166\n",
      "Gradient Descent(84/99): loss =1241.2030849769558, w0=-0.14253524591323877, w1=0.18064598539157656\n",
      "Gradient Descent(85/99): loss =1231.2468478090943, w0=-0.14211590918366723, w1=0.18035407149167232\n",
      "Gradient Descent(86/99): loss =1221.3724759561028, w0=-0.14169928189399622, w1=0.18006262814282376\n",
      "Gradient Descent(87/99): loss =1211.5792688492613, w0=-0.14128534229399636, w1=0.17977166037261894\n",
      "Gradient Descent(88/99): loss =1201.866532433628, w0=-0.1408740689898193, w1=0.17948117311145928\n",
      "Gradient Descent(89/99): loss =1192.2335790944587, w0=-0.14046544092622318, w1=0.17919117119869007\n",
      "Gradient Descent(90/99): loss =1182.6797275849144, w0=-0.14005943737016624, w1=0.17890165938809036\n",
      "Gradient Descent(91/99): loss =1173.204302954998, w0=-0.13965603789565031, w1=0.17861264235278881\n",
      "Gradient Descent(92/99): loss =1163.8066364816843, w0=-0.13925522236970633, w1=0.1783241246896651\n",
      "Gradient Descent(93/99): loss =1154.4860656001845, w0=-0.13885697093942453, w1=0.17803611092329003\n",
      "Gradient Descent(94/99): loss =1145.2419338363293, w0=-0.13846126401994094, w1=0.17774860550945223\n",
      "Gradient Descent(95/99): loss =1136.073590740032, w0=-0.13806808228329936, w1=0.17746161283831455\n",
      "Gradient Descent(96/99): loss =1126.9803918197774, w0=-0.13767740664811637, w1=0.17717513723723816\n",
      "Gradient Descent(97/99): loss =1117.9616984781394, w0=-0.13728921826998208, w1=0.17688918297330927\n",
      "Gradient Descent(98/99): loss =1109.0168779482751, w0=-0.1369034985325367, w1=0.1766037542555992\n",
      "Gradient Descent(99/99): loss =1100.1453032313887, w0=-0.13652022903916747, w1=0.17631885523718563\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =252980.65810003458, w0=-9.208692474643732e-06, w1=0.1999999988749488\n",
      "Gradient Descent(2/99): loss =1340.0939599553528, w0=0.0007048402057819006, w1=0.19767687868346076\n",
      "Gradient Descent(3/99): loss =598.1520880394413, w0=-0.009284696508448737, w1=0.19599113420707226\n",
      "Gradient Descent(4/99): loss =293.5630072128052, w0=-0.009201624156692208, w1=0.19513750207497496\n",
      "Gradient Descent(5/99): loss =163.94050223477825, w0=-0.015050577048205348, w1=0.193501551098555\n",
      "Gradient Descent(6/99): loss =111.26492306054041, w0=-0.016307491330062582, w1=0.1922765217592228\n",
      "Gradient Descent(7/99): loss =88.06018400262376, w0=-0.0197867660576933, w1=0.19082088604460887\n",
      "Gradient Descent(8/99): loss =76.18353323977706, w0=-0.021363250028591168, w1=0.1895447904171037\n",
      "Gradient Descent(9/99): loss =68.84044311135995, w0=-0.02361811503730727, w1=0.1882126257294246\n",
      "Gradient Descent(10/99): loss =63.49190758720905, w0=-0.025052888410732596, w1=0.186961084660704\n",
      "Gradient Descent(11/99): loss =59.1627446577272, w0=-0.026611202489756805, w1=0.1857036831881209\n",
      "Gradient Descent(12/99): loss =55.448664195631366, w0=-0.027767641449831328, w1=0.184485170398706\n",
      "Gradient Descent(13/99): loss =52.1668229682282, w0=-0.028867369075500457, w1=0.1832734535634089\n",
      "Gradient Descent(14/99): loss =49.22041684574183, w0=-0.02973300718835968, w1=0.1820827709247612\n",
      "Gradient Descent(15/99): loss =46.5512999678185, w0=-0.03049435572893017, w1=0.1809010169253346\n",
      "Gradient Descent(16/99): loss =44.11924981078397, w0=-0.031096278704612155, w1=0.17973251432733456\n",
      "Gradient Descent(17/99): loss =41.89426727050519, w0=-0.031591134241295324, w1=0.17857262296871665\n",
      "Gradient Descent(18/99): loss =39.852432362943716, w0=-0.03196583422464809, w1=0.17742257215614013\n",
      "Gradient Descent(19/99): loss =37.97407905942613, w0=-0.03224450296977579, w1=0.1762805808073487\n",
      "Gradient Descent(20/99): loss =36.24257720212136, w0=-0.032427928357557735, w1=0.17514698904004172\n",
      "Gradient Descent(21/99): loss =34.643657615432524, w0=-0.0325295115393761, w1=0.1740211464440899\n",
      "Gradient Descent(22/99): loss =33.164905036371735, w0=-0.03255413435146213, w1=0.1729031807250133\n",
      "Gradient Descent(23/99): loss =31.7954221769703, w0=-0.0325107355648345, w1=0.17179290326562208\n",
      "Gradient Descent(24/99): loss =30.52556551805487, w0=-0.03240496125354026, w1=0.17069041494797194\n",
      "Gradient Descent(25/99): loss =29.34675011190982, w0=-0.03224356970546002, w1=0.1695957188329016\n",
      "Gradient Descent(26/99): loss =28.251292230671396, w0=-0.03203189035717809, w1=0.16850892560378378\n",
      "Gradient Descent(27/99): loss =27.232285392474463, w0=-0.03177537591150317, w1=0.1674301151543354\n",
      "Gradient Descent(28/99): loss =26.283498299538504, w0=-0.031478735303312806, w1=0.16635940514319275\n",
      "Gradient Descent(29/99): loss =25.399291148123723, w0=-0.031146490122582155, w1=0.16529690083909485\n",
      "Gradient Descent(30/99): loss =24.57454537845259, w0=-0.030782688703074792, w1=0.16424271743280777\n",
      "Gradient Descent(31/99): loss =23.80460450313773, w0=-0.030391114372724803, w1=0.16319696155754193\n",
      "Gradient Descent(32/99): loss =23.08522357349189, w0=-0.029975200111016027, w1=0.16215973830737626\n",
      "Gradient Descent(33/99): loss =22.412525780597264, w0=-0.0295381141739114, w1=0.16113114434636142\n",
      "Gradient Descent(34/99): loss =21.78296484163206, w0=-0.029082742968391445, w1=0.16011127006258322\n",
      "Gradient Descent(35/99): loss =21.19329221571484, w0=-0.0286117325868693, w1=0.15910019704024836\n",
      "Gradient Descent(36/99): loss =20.640528340190702, w0=-0.02812749395024437, w1=0.15809799878138026\n",
      "Gradient Descent(37/99): loss =20.12193726857453, w0=-0.027632227634005708, w1=0.1571047398852443\n",
      "Gradient Descent(38/99): loss =19.6350041957133, w0=-0.02712793539881461, w1=0.15612047641660837\n",
      "Gradient Descent(39/99): loss =19.17741545891868, w0=-0.02661643791752335, w1=0.15514525578355318\n",
      "Gradient Descent(40/99): loss =18.747040672878178, w0=-0.026099387278643797, w1=0.1541791170596467\n",
      "Gradient Descent(41/99): loss =18.341916716987157, w0=-0.025578281059119835, w1=0.153222091158002\n",
      "Gradient Descent(42/99): loss =17.960233338902917, w0=-0.02505447403213791, w1=0.1522742011738703\n",
      "Gradient Descent(43/99): loss =17.60032017616871, w0=-0.02452918989592134, w1=0.15133546268435338\n",
      "Gradient Descent(44/99): loss =17.260635027622403, w0=-0.024003531687472518, w1=0.15040588411052544\n",
      "Gradient Descent(45/99): loss =16.939753231172585, w0=-0.023478491737611764, w1=0.149485467065156\n",
      "Gradient Descent(46/99): loss =16.63635802470007, w0=-0.022954960731949182, w1=0.14857420671989477\n",
      "Gradient Descent(47/99): loss =16.349231783688104, w0=-0.022433736202167657, w1=0.14767209216327598\n",
      "Gradient Descent(48/99): loss =16.077248043156004, w0=-0.02191553032307094, w1=0.14677910675978906\n",
      "Gradient Descent(49/99): loss =15.81936422323838, w0=-0.02140097715333541, w1=0.14589522849898853\n",
      "Gradient Descent(50/99): loss =15.57461498767552, w0=-0.02089063930111838, w1=0.14502043033741777\n",
      "Gradient Descent(51/99): loss =15.342106172927695, w0=-0.0203850140855466, w1=0.14415468052899244\n",
      "Gradient Descent(52/99): loss =15.121009232845683, w0=-0.019884539209553943, w1=0.1432979429444714\n",
      "Gradient Descent(53/99): loss =14.910556150041662, w0=-0.019389597989499973, w1=0.1424501773783241\n",
      "Gradient Descent(54/99): loss =14.710034770472895, w0=-0.018900524166542792, w1=0.14161133984315147\n",
      "Gradient Descent(55/99): loss =14.518784522415569, w0=-0.01841760633394906, w1=0.14078138285110708\n",
      "Gradient Descent(56/99): loss =14.336192485076145, w0=-0.017941092006279868, w1=0.13996025568247875\n",
      "Gradient Descent(57/99): loss =14.16168977565447, w0=-0.017471191358468394, w1=0.13914790464139962\n",
      "Gradient Descent(58/99): loss =13.994748226810515, w0=-0.017008080658872902, w1=0.13834427329894783\n",
      "Gradient Descent(59/99): loss =13.834877329256704, w0=-0.01655190542008064, w1=0.13754930272385807\n",
      "Gradient Descent(60/99): loss =13.681621416650916, w0=-0.016102783288895556, w1=0.13676293170119175\n",
      "Gradient Descent(61/99): loss =13.534557072145091, w0=-0.015660806695906963, w1=0.13598509693931501\n",
      "Gradient Descent(62/99): loss =13.393290737885652, w0=-0.015226045283346085, w1=0.13521573326558647\n",
      "Gradient Descent(63/99): loss =13.257456510496343, w0=-0.014798548128751548, w1=0.134454773811163\n",
      "Gradient Descent(64/99): loss =13.126714107125933, w0=-0.014378345780602082, w1=0.133702150185351\n",
      "Gradient Descent(65/99): loss =13.000746988036461, w0=-0.013965452120935987, w1=0.1329577926399325\n",
      "Gradient Descent(66/99): loss =12.879260622959187, w0=-0.013559866068824365, w1=0.13222163022389768\n",
      "Gradient Descent(67/99): loss =12.76198088957288, w0=-0.013161573137535812, w1=0.1314935909290122\n",
      "Gradient Descent(68/99): loss =12.64865259347652, w0=-0.012770546857240028, w1=0.13077360182664022\n",
      "Gradient Descent(69/99): loss =12.539038099947172, w0=-0.012386750074191662, w1=0.13006158919623728\n",
      "Gradient Descent(70/99): loss =12.432916068605639, w0=-0.01201013613648267, w1=0.12935747864591501\n",
      "Gradient Descent(71/99): loss =12.330080282865932, w0=-0.011640649975664523, w1=0.128661195225469\n",
      "Gradient Descent(72/99): loss =12.230338566728745, w0=-0.011278229092808269, w1=0.1279726635322481\n",
      "Gradient Descent(73/99): loss =12.13351178209973, w0=-0.010922804456892646, w1=0.12729180781023003\n",
      "Gradient Descent(74/99): loss =12.039432900378705, w0=-0.01057430132278193, w1=0.12661855204265468\n",
      "Gradient Descent(75/99): loss =11.94794614258014, w0=-0.010232639975474724, w1=0.12595282003855207\n",
      "Gradient Descent(76/99): loss =11.858906182713797, w0=-0.009897736406768005, w1=0.12529453551348846\n",
      "Gradient Descent(77/99): loss =11.772177409582149, w0=-0.009569502929985488, w1=0.12464362216483964\n",
      "Gradient Descent(78/99): loss =11.687633242541612, w0=-0.009247848737962193, w1=0.12400000374188726\n",
      "Gradient Descent(79/99): loss =11.605155497130948, w0=-0.008932680409055838, w1=0.12336360411101992\n",
      "Gradient Descent(80/99): loss =11.52463379679699, w0=-0.008623902365567284, w1=0.12273434731630811\n",
      "Gradient Descent(81/99): loss =11.445965027245848, w0=-0.008321417288594818, w1=0.12211215763570904\n",
      "Gradient Descent(82/99): loss =11.369052830221506, w0=-0.008025126493017789, w1=0.1214969596331449\n",
      "Gradient Descent(83/99): loss =11.293807133764304, w0=-0.007734930266002341, w1=0.12088867820668636\n",
      "Gradient Descent(84/99): loss =11.220143716231336, w0=-0.007450728172143334, w1=0.12028723863306144\n",
      "Gradient Descent(85/99): loss =11.147983801572163, w0=-0.007172419328100411, w1=0.11969256660869869\n",
      "Gradient Descent(86/99): loss =11.077253683546362, w0=-0.006899902649350716, w1=0.11910458828750328\n",
      "Gradient Descent(87/99): loss =11.007884376747759, w0=-0.006633077071464298, w1=0.118523230315554\n",
      "Gradient Descent(88/99): loss =10.939811292463446, w0=-0.006371841748109462, w1=0.11794841986289985\n",
      "Gradient Descent(89/99): loss =10.872973937546199, w0=-0.006116096227812618, w1=0.11738008465262516\n",
      "Gradient Descent(90/99): loss =10.807315634617211, w0=-0.005865740611329423, w1=0.11681815298734388\n",
      "Gradient Descent(91/99): loss =10.74278326204358, w0=-0.005620675691329922, w1=0.1162625537732745\n",
      "Gradient Descent(92/99): loss =10.679327012251973, w0=-0.0053808030759589275, w1=0.11571321654203967\n",
      "Gradient Descent(93/99): loss =10.616900167048485, w0=-0.005146025297702941, w1=0.11517007147032646\n",
      "Gradient Descent(94/99): loss =10.555458888713792, w0=-0.00491624590887571, w1=0.11463304939753623\n",
      "Gradient Descent(95/99): loss =10.49496202573492, w0=-0.004691369564924976, w1=0.114102081841546\n",
      "Gradient Descent(96/99): loss =10.435370932119563, w0=-0.004471302096662535, w1=0.11357710101269655\n",
      "Gradient Descent(97/99): loss =10.376649299317071, w0=-0.004255950572427422, w1=0.11305803982611665\n",
      "Gradient Descent(98/99): loss =10.318762999842406, w0=-0.004045223351107424, w1=0.11254483191248613\n",
      "Gradient Descent(99/99): loss =10.261679941765996, w0=-0.00383903012686637, w1=0.11203741162733595\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =2977549289.025725, w0=-0.19999980523360172, w1=0.199999999839907\n",
      "Gradient Descent(2/99): loss =5652.274487070868, w0=-0.19688735295467621, w1=0.20088574344790872\n",
      "Gradient Descent(3/99): loss =4074.938418342041, w0=-0.19546071073742197, w1=0.2014738975732278\n",
      "Gradient Descent(4/99): loss =3687.7944853523736, w0=-0.19426545304345397, w1=0.2017621140665103\n",
      "Gradient Descent(5/99): loss =3457.142203405868, w0=-0.19327314342416616, w1=0.20184754941606198\n",
      "Gradient Descent(6/99): loss =3292.9377664962535, w0=-0.1923798378227929, w1=0.20182059001832095\n",
      "Gradient Descent(7/99): loss =3165.146949138963, w0=-0.19155337181573223, w1=0.20172784307629996\n",
      "Gradient Descent(8/99): loss =3060.2251724909393, w0=-0.19076938508804697, w1=0.20159393621547708\n",
      "Gradient Descent(9/99): loss =2970.917042712779, w0=-0.19001524576670004, w1=0.20143158710666198\n",
      "Gradient Descent(10/99): loss =2892.950041339641, w0=-0.18928283516119643, w1=0.2012474107734972\n",
      "Gradient Descent(11/99): loss =2823.639249435556, w0=-0.18856729016077717, w1=0.20104489138639503\n",
      "Gradient Descent(12/99): loss =2761.19968447913, w0=-0.1878655923930912, w1=0.20082595318082316\n",
      "Gradient Descent(13/99): loss =2704.3803300807062, w0=-0.18717592396487975, w1=0.20059176136268816\n",
      "Gradient Descent(14/99): loss =2652.2609447857953, w0=-0.18649721219198998, w1=0.20034312858502046\n",
      "Gradient Descent(15/99): loss =2604.1356742160015, w0=-0.18582885121222334, w1=0.20008071508854816\n",
      "Gradient Descent(16/99): loss =2559.4443452714477, w0=-0.18517051591118558, w1=0.1998051236595905\n",
      "Gradient Descent(17/99): loss =2517.730533708235, w0=-0.184522040749504, w1=0.19951694130628858\n",
      "Gradient Descent(18/99): loss =2478.614982161641, w0=-0.1838833405729101, w1=0.19921675467370273\n",
      "Gradient Descent(19/99): loss =2441.778025227286, w0=-0.18325436019442334, w1=0.19890515301579145\n",
      "Gradient Descent(20/99): loss =2406.947445392976, w0=-0.18263504343593914, w1=0.1985827257494096\n",
      "Gradient Descent(21/99): loss =2373.8897106404875, w0=-0.18202531536145974, w1=0.1982500580819405\n",
      "Gradient Descent(22/99): loss =2342.4033970260257, w0=-0.18142507332044705, w1=0.1979077263869331\n",
      "Gradient Descent(23/99): loss =2312.314081203575, w0=-0.1808341837575333, w1=0.19755629407131065\n",
      "Gradient Descent(24/99): loss =2283.4702637152463, w0=-0.18025248267047903, w1=0.19719630820902914\n",
      "Gradient Descent(25/99): loss =2255.7400444490236, w0=-0.17967977825512194, w1=0.19682829698686807\n",
      "Gradient Descent(26/99): loss =2229.0083669960627, w0=-0.17911585474090316, w1=0.19645276790278043\n",
      "Gradient Descent(27/99): loss =2203.174706605045, w0=-0.1785604767499214, w1=0.19607020661540125\n",
      "Gradient Descent(28/99): loss =2178.1511126885152, w0=-0.17801339374460803, w1=0.19568107633325235\n",
      "Gradient Descent(29/99): loss =2153.8605402616854, w0=-0.1774743442915664, w1=0.19528581763729627\n",
      "Gradient Descent(30/99): loss =2130.2354203733953, w0=-0.1769430599815692, w1=0.19488484864213382\n",
      "Gradient Descent(31/99): loss =2107.2164304731314, w0=-0.17641926892241253, w1=0.1944785654148612\n",
      "Gradient Descent(32/99): loss =2084.7514334871044, w0=-0.1759026987725927, w1=0.19406734258419236\n",
      "Gradient Descent(33/99): loss =2062.7945601953347, w0=-0.17539307931706388, w1=0.19365153408488114\n",
      "Gradient Descent(34/99): loss =2041.3054139527221, w0=-0.17489014460703464, w1=0.19323147399335316\n",
      "Gradient Descent(35/99): loss =2020.248380284608, w0=-0.17439363469774702, w1=0.19280747741970214\n",
      "Gradient Descent(36/99): loss =1999.5920266745784, w0=-0.17390329702422533, w1=0.19237984142891154\n",
      "Gradient Descent(37/99): loss =1979.3085801247792, w0=-0.1734188874570608, w1=0.19194884597048747\n",
      "Gradient Descent(38/99): loss =1959.3734719288432, w0=-0.17294017107980914, w1=0.19151475480081775\n",
      "Gradient Descent(39/99): loss =1939.7649406412843, w0=-0.17246692272749778, w1=0.19107781638668697\n",
      "Gradient Descent(40/99): loss =1920.4636855187753, w0=-0.17199892732274652, w1=0.19063826478164614\n",
      "Gradient Descent(41/99): loss =1901.4525637961692, w0=-0.1715359800425591, w1=0.1901963204695072\n",
      "Gradient Descent(42/99): loss =1882.7163260802033, w0=-0.17107788634525342, w1=0.18975219117123668\n",
      "Gradient Descent(43/99): loss =1864.2413849261677, w0=-0.17062446188346142, w1=0.18930607261306756\n",
      "Gradient Descent(44/99): loss =1846.0156123297263, w0=-0.1701755323257721, w1=0.18885814925482525\n",
      "Gradient Descent(45/99): loss =1828.0281624371405, w0=-0.16973093310648027, w1=0.18840859497834636\n",
      "Gradient Descent(46/99): loss =1810.2693162669011, w0=-0.16929050912007754, w1=0.18795757373652167\n",
      "Gradient Descent(47/99): loss =1792.730345657026, w0=-0.16885411437458783, w1=0.18750524016396336\n",
      "Gradient Descent(48/99): loss =1775.4033940154695, w0=-0.16842161161560587, w1=0.18705174015062584\n",
      "Gradient Descent(49/99): loss =1758.281371764604, w0=-0.1679928719309279, w1=0.18659721137992666\n",
      "Gradient Descent(50/99): loss =1741.3578646419837, w0=-0.16756777434394882, w1=0.1861417838330493\n",
      "Gradient Descent(51/99): loss =1724.6270532546475, w0=-0.16714620540251818, w1=0.18568558026117982\n",
      "Gradient Descent(52/99): loss =1708.0836424879908, w0=-0.166728058768675, w1=0.1852287166274525\n",
      "Gradient Descent(53/99): loss =1691.7227995475023, w0=-0.16631323481359325, w1=0.1847713025203674\n",
      "Gradient Descent(54/99): loss =1675.5400995655166, w0=-0.1659016402211483, w1=0.18431344154040505\n",
      "Gradient Descent(55/99): loss =1659.53147783928, w0=-0.16549318760273485, w1=0.18385523166150852\n",
      "Gradient Descent(56/99): loss =1643.6931878835271, w0=-0.1650877951253128, w1=0.18339676556903495\n",
      "Gradient Descent(57/99): loss =1628.0217645824928, w0=-0.16468538615411263, w1=0.18293813097570435\n",
      "Gradient Descent(58/99): loss =1612.5139918153734, w0=-0.16428588891097856, w1=0.18247941091699396\n",
      "Gradient Descent(59/99): loss =1597.1668740068155, w0=-0.16388923614895612, w1=0.1820206840273455\n",
      "Gradient Descent(60/99): loss =1581.9776111218787, w0=-0.16349536484342647, w1=0.18156202479847197\n",
      "Gradient Descent(61/99): loss =1566.9435766843142, w0=-0.16310421589984422, w1=0.1811035038209713\n",
      "Gradient Descent(62/99): loss =1552.0622984488766, w0=-0.16271573387793858, w1=0.1806451880103773\n",
      "Gradient Descent(63/99): loss =1537.3314414038161, w0=-0.16232986673208283, w1=0.18018714081870432\n",
      "Gradient Descent(64/99): loss =1522.7487928195221, w0=-0.16194656556741702, w1=0.1797294224324718\n",
      "Gradient Descent(65/99): loss =1508.3122490940875, w0=-0.16156578441121755, w1=0.17927208995812793\n",
      "Gradient Descent(66/99): loss =1494.019804177134, w0=-0.1611874799989408, w1=0.1788151975957288\n",
      "Gradient Descent(67/99): loss =1479.869539379985, w0=-0.16081161157432122, w1=0.17835879680167005\n",
      "Gradient Descent(68/99): loss =1465.8596144037012, w0=-0.16043814070287402, w1=0.17790293644121266\n",
      "Gradient Descent(69/99): loss =1451.988259437172, w0=-0.1600670310981365, w1=0.17744766293149267\n",
      "Gradient Descent(70/99): loss =1438.2537681953336, w0=-0.15969824845997543, w1=0.17699302037565595\n",
      "Gradient Descent(71/99): loss =1424.6544917835986, w0=-0.1593317603242918, w1=0.17653905068871428\n",
      "Gradient Descent(72/99): loss =1411.1888332883123, w0=-0.15896753592346313, w1=0.17608579371567706\n",
      "Gradient Descent(73/99): loss =1397.855243005361, w0=-0.15860554605688004, w1=0.1756332873424736\n",
      "Gradient Descent(74/99): loss =1384.6522142296883, w0=-0.15824576297095183, w1=0.17518156760014517\n",
      "Gradient Descent(75/99): loss =1371.578279537872, w0=-0.15788816024797933, w1=0.17473066876275184\n",
      "Gradient Descent(76/99): loss =1358.6320075042192, w0=-0.15753271270331706, w1=0.17428062343940845\n",
      "Gradient Descent(77/99): loss =1345.8119997979506, w0=-0.1571793962902733, w1=0.17383146266083438\n",
      "Gradient Descent(78/99): loss =1333.1168886155908, w0=-0.15682818801222306, w1=0.17338321596077572\n",
      "Gradient Descent(79/99): loss =1320.5453344080522, w0=-0.15647906584143667, w1=0.1729359114526328\n",
      "Gradient Descent(80/99): loss =1308.096023866972, w0=-0.15613200864415333, w1=0.1724895759016034\n",
      "Gradient Descent(81/99): loss =1295.7676681390851, w0=-0.1557869961114565, w1=0.17204423479263026\n",
      "Gradient Descent(82/99): loss =1283.5590012411988, w0=-0.1554440086955337, w1=0.17159991239442154\n",
      "Gradient Descent(83/99): loss =1271.4687786517027, w0=-0.15510302755092947, w1=0.1711566318197951\n",
      "Gradient Descent(84/99): loss =1259.4957760574193, w0=-0.15476403448042447, w1=0.1707144150825792\n",
      "Gradient Descent(85/99): loss =1247.638788237193, w0=-0.1544270118851978, w1=0.17027328315128745\n",
      "Gradient Descent(86/99): loss =1235.896628065855, w0=-0.1540919427189521, w1=0.16983325599977048\n",
      "Gradient Descent(87/99): loss =1224.2681256241879, w0=-0.15375881044570272, w1=0.16939435265503328\n",
      "Gradient Descent(88/99): loss =1212.7521274022522, w0=-0.15342759900095282, w1=0.16895659124239437\n",
      "Gradient Descent(89/99): loss =1201.3474955849574, w0=-0.15309829275599535, w1=0.16851998902815146\n",
      "Gradient Descent(90/99): loss =1190.0531074101218, w0=-0.15277087648510168, w1=0.16808456245990672\n",
      "Gradient Descent(91/99): loss =1178.8678545904124, w0=-0.15244533533537338, w1=0.1676503272046953\n",
      "Gradient Descent(92/99): loss =1167.79064279165, w0=-0.15212165479905015, w1=0.1672172981850507\n",
      "Gradient Descent(93/99): loss =1156.8203911607814, w0=-0.15179982068808184, w1=0.16678548961313205\n",
      "Gradient Descent(94/99): loss =1145.956031897756, w0=-0.15147981911078698, w1=0.1663549150230299\n",
      "Gradient Descent(95/99): loss =1135.1965098660926, w0=-0.15116163645043323, w1=0.16592558730136026\n",
      "Gradient Descent(96/99): loss =1124.54078223769, w0=-0.15084525934558748, w1=0.16549751871624824\n",
      "Gradient Descent(97/99): loss =1113.987818167857, w0=-0.1505306746720952, w1=0.16507072094479744\n",
      "Gradient Descent(98/99): loss =1103.5365984970979, w0=-0.15021786952655877, w1=0.16464520509913402\n",
      "Gradient Descent(99/99): loss =1093.1861154765647, w0=-0.14990683121119533, w1=0.1642209817511091\n",
      "Optimizing degree 5/15, model: least_squares_GD, arguments: {'max_iters': 100}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =43218900829390.0, w0=-0.1999999999892096, w1=0.19999999999998086\n",
      "Gradient Descent(2/99): loss =1276112.2854198029, w0=-0.1998473015995932, w1=0.20014960219358024\n",
      "Gradient Descent(3/99): loss =1164266.3087042335, w0=-0.19973138201353233, w1=0.2002672884094066\n",
      "Gradient Descent(4/99): loss =1105591.9146303202, w0=-0.19962588643563717, w1=0.20036983447829795\n",
      "Gradient Descent(5/99): loss =1058424.665448339, w0=-0.19952806813277854, w1=0.2004609236127702\n",
      "Gradient Descent(6/99): loss =1019280.141526874, w0=-0.19943645742937002, w1=0.2005433594934515\n",
      "Gradient Descent(7/99): loss =986263.8444815519, w0=-0.19935008622698153, w1=0.20061896068003207\n",
      "Gradient Descent(8/99): loss =958040.9334078027, w0=-0.19926822606205308, w1=0.20068892640328218\n",
      "Gradient Descent(9/99): loss =933622.4323712689, w0=-0.19919029545941358, w1=0.20075407674854365\n",
      "Gradient Descent(10/99): loss =912261.5223321406, w0=-0.19911581680029342, w1=0.2008149970382357\n",
      "Gradient Descent(11/99): loss =893386.3970508941, w0=-0.1990443918647625, w1=0.20087212350839\n",
      "Gradient Descent(12/99): loss =876553.9473149644, w0=-0.19897568563791415, w1=0.2009257944772793\n",
      "Gradient Descent(13/99): loss =861417.0396391049, w0=-0.19890941451928495, w1=0.20097628144724453\n",
      "Gradient Descent(14/99): loss =847701.0090914072, w0=-0.19884533726440434, w1=0.20102380850395496\n",
      "Gradient Descent(15/99): loss =835186.5274644446, w0=-0.19878324780750709, w1=0.20106856480952956\n",
      "Gradient Descent(16/99): loss =823696.9554929154, w0=-0.1987229694664073, w1=0.20111071294149682\n",
      "Gradient Descent(17/99): loss =813088.8946921354, w0=-0.19866435020504783, w1=0.20115039466867402\n",
      "Gradient Descent(18/99): loss =803245.0524245094, w0=-0.19860725872729304, w1=0.20118773509642532\n",
      "Gradient Descent(19/99): loss =794068.7997132185, w0=-0.1985515812361218, w1=0.20122284573834584\n",
      "Gradient Descent(20/99): loss =785479.9817838158, w0=-0.19849721873249668, w1=0.20125582685543855\n",
      "Gradient Descent(21/99): loss =777411.6655054638, w0=-0.19844408475615757, w1=0.20128676927786107\n",
      "Gradient Descent(22/99): loss =769807.59446289, w0=-0.19839210349088002, w1=0.20131575584943845\n",
      "Gradient Descent(23/99): loss =762620.183435863, w0=-0.19834120817193607, w1=0.2013428625895657\n",
      "Gradient Descent(24/99): loss =755808.9275954278, w0=-0.19829133974516452, w1=0.20136815963858892\n",
      "Gradient Descent(25/99): loss =749339.1330985492, w0=-0.1982424457361999, w1=0.20139171203430173\n",
      "Gradient Descent(26/99): loss =743180.8986009901, w0=-0.19819447929568385, w1=0.20141358035484794\n",
      "Gradient Descent(27/99): loss =737308.2939957534, w0=-0.1981473983921431, w1=0.20143382125477402\n",
      "Gradient Descent(28/99): loss =731698.6951395683, w0=-0.1981011651289823, w1=0.20145248791486906\n",
      "Gradient Descent(29/99): loss =726332.2426540437, w0=-0.1980557451659436, w1=0.20146963042194357\n",
      "Gradient Descent(30/99): loss =721191.399928291, w0=-0.1980111072286003, w1=0.20148529609132593\n",
      "Gradient Descent(31/99): loss =716260.590809248, w0=-0.19796722269211367, w1=0.20149952974227234\n",
      "Gradient Descent(32/99): loss =711525.901577719, w0=-0.1979240652276927, w1=0.20151237393447952\n",
      "Gradient Descent(33/99): loss =706974.8349859463, w0=-0.19788161050203878, w1=0.2015238691723124\n",
      "Gradient Descent(34/99): loss =702596.1066054642, w0=-0.19783983592159587, w1=0.201534054082111\n",
      "Gradient Descent(35/99): loss =698379.4756708819, w0=-0.19779872041471477, w1=0.20154296556694468\n",
      "Gradient Descent(36/99): loss =694315.6041312513, w0=-0.19775824424591906, w1=0.2015506389423854\n",
      "Gradient Descent(37/99): loss =690395.9388296788, w0=-0.1977183888573675, w1=0.2015571080562293\n",
      "Gradient Descent(38/99): loss =686612.6126944734, w0=-0.19767913673336962, w1=0.20156240539457942\n",
      "Gradient Descent(39/99): loss =682958.3615951202, w0=-0.19764047128445236, w1=0.20156656217628255\n",
      "Gradient Descent(40/99): loss =679426.4541348479, w0=-0.1976023767480168, w1=0.20156960843737465\n",
      "Gradient Descent(41/99): loss =676010.632150223, w0=-0.19756483810307918, w1=0.20157157310691104\n",
      "Gradient Descent(42/99): loss =672705.0600916172, w0=-0.19752784099697496, w1=0.20157248407533365\n",
      "Gradient Descent(43/99): loss =669504.2817857291, w0=-0.1974913716822302, w1=0.20157236825634162\n",
      "Gradient Descent(44/99): loss =666403.1833476878, w0=-0.19745541696207775, w1=0.2015712516430811\n",
      "Gradient Descent(45/99): loss =663396.9612275397, w0=-0.197419964143329, w1=0.201569159359345\n",
      "Gradient Descent(46/99): loss =660481.0945534752, w0=-0.19738500099550707, w1=0.20156611570637062\n",
      "Gradient Descent(47/99): loss =657651.3210796432, w0=-0.19735051571531442, w1=0.2015621442057379\n",
      "Gradient Descent(48/99): loss =654903.6161656907, w0=-0.1973164968956475, w1=0.20155726763880014\n",
      "Gradient Descent(49/99): loss =652234.1743132959, w0=-0.19728293349849088, w1=0.20155150808301994\n",
      "Gradient Descent(50/99): loss =649639.3928656228, w0=-0.19724981483112367, w1=0.20154488694553332\n",
      "Gradient Descent(51/99): loss =647115.8575421539, w0=-0.19721713052515696, w1=0.20153742499422336\n",
      "Gradient Descent(52/99): loss =644660.3295362375, w0=-0.19718487051799302, w1=0.2015291423865495\n",
      "Gradient Descent(53/99): loss =642269.7339479795, w0=-0.19715302503635884, w1=0.20152005869634856\n",
      "Gradient Descent(54/99): loss =639941.1493626197, w0=-0.19712158458161846, w1=0.2015101929387979\n",
      "Gradient Descent(55/99): loss =637671.7984155072, w0=-0.19709053991661293, w1=0.20149956359370935\n",
      "Gradient Descent(56/99): loss =635459.0392105209, w0=-0.19705988205381425, w1=0.20148818862730378\n",
      "Gradient Descent(57/99): loss =633300.3574800747, w0=-0.19702960224461114, w1=0.20147608551259938\n",
      "Gradient Descent(58/99): loss =631193.3593925306, w0=-0.19699969196957218, w1=0.2014632712485334\n",
      "Gradient Descent(59/99): loss =629135.7649275587, w0=-0.19697014292955425, w1=0.20144976237792364\n",
      "Gradient Descent(60/99): loss =627125.4017521976, w0=-0.19694094703754386, w1=0.2014355750043663\n",
      "Gradient Descent(61/99): loss =625160.1995405578, w0=-0.1969120964111357, w1=0.20142072480815598\n",
      "Gradient Descent(62/99): loss =623238.1846885916, w0=-0.1968835833655665, w1=0.20140522706130692\n",
      "Gradient Descent(63/99): loss =621357.4753824555, w0=-0.1968554004072346, w1=0.20138909664174498\n",
      "Gradient Descent(64/99): loss =619516.2769849014, w0=-0.1968275402276454, w1=0.2013723480467353\n",
      "Gradient Descent(65/99): loss =617712.8777090952, w0=-0.196799995697732, w1=0.2013549954056032\n",
      "Gradient Descent(66/99): loss =615945.6445534228, w0=-0.19677275986250692, w1=0.20133705249180137\n",
      "Gradient Descent(67/99): loss =614213.0194743596, w0=-0.1967458259360082, w1=0.2013185327343714\n",
      "Gradient Descent(68/99): loss =612513.5157773714, w0=-0.19671918729650703, w1=0.20129944922884335\n",
      "Gradient Descent(69/99): loss =610845.7147083464, w0=-0.19669283748195002, w1=0.20127981474761372\n",
      "Gradient Descent(70/99): loss =609208.262230148, w0=-0.1966667701856121, w1=0.20125964174983788\n",
      "Gradient Descent(71/99): loss =607599.8659706323, w0=-0.19664097925193963, w1=0.20123894239087084\n",
      "Gradient Descent(72/99): loss =606019.2923300191, w0=-0.19661545867256622, w1=0.20121772853128678\n",
      "Gradient Descent(73/99): loss =604465.3637367852, w0=-0.19659020258248602, w1=0.20119601174550528\n",
      "Gradient Descent(74/99): loss =602936.9560423639, w0=-0.19656520525637114, w1=0.20117380333005028\n",
      "Gradient Descent(75/99): loss =601432.9960458619, w0=-0.19654046110502185, w1=0.20115111431146512\n",
      "Gradient Descent(76/99): loss =599952.459140875, w0=-0.19651596467193957, w1=0.20112795545390558\n",
      "Gradient Descent(77/99): loss =598494.3670771284, w0=-0.19649171063001378, w1=0.20110433726643057\n",
      "Gradient Descent(78/99): loss =597057.7858303493, w0=-0.19646769377831536, w1=0.2010802700100093\n",
      "Gradient Descent(79/99): loss =595641.8235742865, w0=-0.19644390903898956, w1=0.2010557637042615\n",
      "Gradient Descent(80/99): loss =594245.6287492702, w0=-0.19642035145424255, w1=0.20103082813394638\n",
      "Gradient Descent(81/99): loss =592868.3882221336, w0=-0.19639701618341654, w1=0.2010054728552147\n",
      "Gradient Descent(82/99): loss =591509.3255326953, w0=-0.19637389850014844, w1=0.20097970720163746\n",
      "Gradient Descent(83/99): loss =590167.6992223033, w0=-0.19635099378960827, w1=0.2009535402900232\n",
      "Gradient Descent(84/99): loss =588842.801240291, w0=-0.19632829754581307, w1=0.20092698102603546\n",
      "Gradient Descent(85/99): loss =587533.9554244062, w0=-0.19630580536901357, w1=0.20090003810962076\n",
      "Gradient Descent(86/99): loss =586240.5160515651, w0=-0.19628351296314975, w1=0.20087272004025725\n",
      "Gradient Descent(87/99): loss =584961.8664554632, w0=-0.19626141613337328, w1=0.20084503512203244\n",
      "Gradient Descent(88/99): loss =583697.417707818, w0=-0.19623951078363375, w1=0.20081699146855914\n",
      "Gradient Descent(89/99): loss =582446.6073601613, w0=-0.1962177929143265, w1=0.20078859700773677\n",
      "Gradient Descent(90/99): loss =581208.8982432974, w0=-0.19619625862, w1=0.2007598594863657\n",
      "Gradient Descent(91/99): loss =579983.7773216915, w0=-0.19617490408712057, w1=0.20073078647462142\n",
      "Gradient Descent(92/99): loss =578770.7546001863, w0=-0.19615372559189265, w1=0.20070138537039442\n",
      "Gradient Descent(93/99): loss =577569.3620806143, w0=-0.19613271949813296, w1=0.20067166340350232\n",
      "Gradient Descent(94/99): loss =576379.1527659425, w0=-0.19611188225519677, w1=0.20064162763977914\n",
      "Gradient Descent(95/99): loss =575199.6997097691, w0=-0.19609121039595478, w1=0.20061128498504724\n",
      "Gradient Descent(96/99): loss =574030.5951090481, w0=-0.19607070053481918, w1=0.20058064218897667\n",
      "Gradient Descent(97/99): loss =572871.4494380583, w0=-0.19605034936581733, w1=0.20054970584883636\n",
      "Gradient Descent(98/99): loss =571721.8906217008, w0=-0.1960301536607121, w1=0.20051848241314152\n",
      "Gradient Descent(99/99): loss =570581.5632463377, w0=-0.1960101102671671, w1=0.20048697818520114\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =43218686513967.53, w0=-0.19999999998907553, w1=0.19999999999997925\n",
      "Gradient Descent(2/99): loss =690639.2354127778, w0=-0.19991271796596685, w1=0.20008474172259919\n",
      "Gradient Descent(3/99): loss =647108.602458564, w0=-0.19984861723033356, w1=0.2001100797873922\n",
      "Gradient Descent(4/99): loss =632271.3971813674, w0=-0.19978671110570342, w1=0.2001247933239167\n",
      "Gradient Descent(5/99): loss =620612.6117072871, w0=-0.1997279776570201, w1=0.20013016790333177\n",
      "Gradient Descent(6/99): loss =611358.9835526696, w0=-0.19967159949616237, w1=0.20012863358098235\n",
      "Gradient Descent(7/99): loss =603971.0165777184, w0=-0.19961737539616842, w1=0.20012138723704417\n",
      "Gradient Descent(8/99): loss =598032.5274177499, w0=-0.19956504505501954, w1=0.2001093315675037\n",
      "Gradient Descent(9/99): loss =593220.8790071283, w0=-0.1995144130984829, w1=0.20009312198488066\n",
      "Gradient Descent(10/99): loss =589285.6302875801, w0=-0.1994653074793173, w1=0.2000732854081595\n",
      "Gradient Descent(11/99): loss =586032.2503012308, w0=-0.19941758062830597, w1=0.20005025901577442\n",
      "Gradient Descent(12/99): loss =583309.5673165896, w0=-0.1993711037147871, w1=0.20002441624278913\n",
      "Gradient Descent(13/99): loss =581000.0644459161, w0=-0.19932576399361474, w1=0.19999608064508856\n",
      "Gradient Descent(14/99): loss =579012.3630904446, w0=-0.1992814623780672, w1=0.1999655350106234\n",
      "Gradient Descent(15/99): loss =577275.3944236119, w0=-0.19923811156246027, w1=0.1999330276545669\n",
      "Gradient Descent(16/99): loss =575733.8761292546, w0=-0.1991956344431973, w1=0.19989877720533128\n",
      "Gradient Descent(17/99): loss =574344.7999172121, w0=-0.1991539627891937, w1=0.1998629764478537\n",
      "Gradient Descent(18/99): loss =573074.7026365295, w0=-0.19911303610521133, w1=0.1998257955359842\n",
      "Gradient Descent(19/99): loss =571897.5453785593, w0=-0.19907280065440733, w1=0.19978738473999122\n",
      "Gradient Descent(20/99): loss =570793.0646254153, w0=-0.19903320861365376, w1=0.19974787682852152\n",
      "Gradient Descent(21/99): loss =569745.4900835204, w0=-0.19899421734133418, w1=0.19970738914925637\n",
      "Gradient Descent(22/99): loss =568742.5474727327, w0=-0.1989557887411816, w1=0.19966602545403686\n",
      "Gradient Descent(23/99): loss =567774.6828279922, w0=-0.19891788870861504, w1=0.19962387750357877\n",
      "Gradient Descent(24/99): loss =566834.4590384399, w0=-0.19888048664822272, w1=0.19958102648024245\n",
      "Gradient Descent(25/99): loss =565916.0863364927, w0=-0.19884355505277865, w1=0.19953754423274536\n",
      "Gradient Descent(26/99): loss =565015.0569767844, w0=-0.19880706913559193, w1=0.19949349437328973\n",
      "Gradient Descent(27/99): loss =564127.8609668527, w0=-0.19877100650915747, w1=0.19944893324486404\n",
      "Gradient Descent(28/99): loss =563251.7648561534, w0=-0.19873534690405711, w1=0.19940391077422928\n",
      "Gradient Descent(29/99): loss =562384.6395884408, w0=-0.19870007192288894, w1=0.19935847122419034\n",
      "Gradient Descent(30/99): loss =561524.8265309789, w0=-0.19866516482470864, w1=0.1993126538571041\n",
      "Gradient Descent(31/99): loss =560671.033211214, w0=-0.19863061033607096, w1=0.19926649352013998\n",
      "Gradient Descent(32/99): loss =559822.2521713934, w0=-0.1985963944852773, w1=0.19922002116155296\n",
      "Gradient Descent(33/99): loss =558977.6978139161, w0=-0.1985625044568825, w1=0.19917326428612625\n",
      "Gradient Descent(34/99): loss =558136.7572477268, w0=-0.1985289284638986, w1=0.19912624735697235\n",
      "Gradient Descent(35/99): loss =557298.9520311106, w0=-0.19849565563546656, w1=0.19907899215002867\n",
      "Gradient Descent(36/99): loss =556463.9083948653, w0=-0.19846267591805544, w1=0.19903151806683286\n",
      "Gradient Descent(37/99): loss =555631.3340656238, w0=-0.19842997998849773, w1=0.19898384241050227\n",
      "Gradient Descent(38/99): loss =554801.0002260675, w0=-0.19839755917738666, w1=0.1989359806292591\n",
      "Gradient Descent(39/99): loss =553972.727473237, w0=-0.1983654054015497, w1=0.19888794653132985\n",
      "Gradient Descent(40/99): loss =553146.3748886298, w0=-0.1983335111044755, w1=0.19883975247459462\n",
      "Gradient Descent(41/99): loss =552321.8315302965, w0=-0.19830186920371431, w1=0.19879140953396357\n",
      "Gradient Descent(42/99): loss =551499.0098100639, w0=-0.19827047304439532, w1=0.19874292764910562\n",
      "Gradient Descent(43/99): loss =550677.8403380475, w0=-0.19823931635811257, w1=0.19869431575484522\n",
      "Gradient Descent(44/99): loss =549858.2679092215, w0=-0.19820839322652467, w1=0.19864558189626896\n",
      "Gradient Descent(45/99): loss =549040.2483789395, w0=-0.19817769804909566, w1=0.198596733330343\n",
      "Gradient Descent(46/99): loss =548223.7462303719, w0=-0.19814722551447592, w1=0.19854777661563036\n",
      "Gradient Descent(47/99): loss =547408.7326805404, w0=-0.19811697057508387, w1=0.19849871769150865\n",
      "Gradient Descent(48/99): loss =546595.184205576, w0=-0.19808692842450445, w1=0.1984495619481245\n",
      "Gradient Descent(49/99): loss =545783.0813923097, w0=-0.1980570944773671, w1=0.19840031428817465\n",
      "Gradient Descent(50/99): loss =544972.408043886, w0=-0.19802746435140792, w1=0.19835097918147543\n",
      "Gradient Descent(51/99): loss =544163.1504831233, w0=-0.19799803385145717, w1=0.1983015607131687\n",
      "Gradient Descent(52/99): loss =543355.2970097985, w0=-0.19796879895512465, w1=0.19825206262631256\n",
      "Gradient Descent(53/99): loss =542548.837477778, w0=-0.19793975579998335, w1=0.19820248835951707\n",
      "Gradient Descent(54/99): loss =541743.7629654247, w0=-0.1979109006720766, w1=0.19815284108020687\n",
      "Gradient Descent(55/99): loss =540940.0655186526, w0=-0.1978822299955944, w1=0.19810312371402494\n",
      "Gradient Descent(56/99): loss =540137.7379505199, w0=-0.19785374032358416, w1=0.19805333897083016\n",
      "Gradient Descent(57/99): loss =539336.7736848585, w0=-0.19782542832957656, w1=0.1980034893676887\n",
      "Gradient Descent(58/99): loss =538537.1666341897, w0=-0.19779729080002226, w1=0.19795357724921198\n",
      "Gradient Descent(59/99): loss =537738.9111043385, w0=-0.1977693246274475, w1=0.19790360480555236\n",
      "Gradient Descent(60/99): loss =536942.00171985, w0=-0.1977415268042475, w1=0.19785357408833115\n",
      "Gradient Descent(61/99): loss =536146.4333655962, w0=-0.19771389441704632, w1=0.19780348702474096\n",
      "Gradient Descent(62/99): loss =535352.2011410166, w0=-0.19768642464156058, w1=0.19775334543003656\n",
      "Gradient Descent(63/99): loss =534559.3003241776, w0=-0.1976591147379116, w1=0.19770315101860242\n",
      "Gradient Descent(64/99): loss =533767.7263435125, w0=-0.19763196204633682, w1=0.19765290541376354\n",
      "Gradient Descent(65/99): loss =532977.4747555276, w0=-0.19760496398325808, w1=0.19760261015648647\n",
      "Gradient Descent(66/99): loss =532188.5412271798, w0=-0.197578118037668, w1=0.1975522667130999\n",
      "Gradient Descent(67/99): loss =531400.9215218801, w0=-0.19755142176780138, w1=0.1975018764821492\n",
      "Gradient Descent(68/99): loss =530614.6114883506, w0=-0.19752487279806163, w1=0.19745144080048618\n",
      "Gradient Descent(69/99): loss =529829.6070516888, w0=-0.19749846881617622, w1=0.19740096094868248\n",
      "Gradient Descent(70/99): loss =529045.9042061807, w0=-0.19747220757055767, w1=0.19735043815584566\n",
      "Gradient Descent(71/99): loss =528263.4990094552, w0=-0.1974460868678497, w1=0.19729987360390727\n",
      "Gradient Descent(72/99): loss =527482.387577724, w0=-0.19742010457064013, w1=0.19724926843144402\n",
      "Gradient Descent(73/99): loss =526702.5660818391, w0=-0.19739425859532442, w1=0.1971986237370862\n",
      "Gradient Descent(74/99): loss =525924.0307440213, w0=-0.19736854691010558, w1=0.19714794058256085\n",
      "Gradient Descent(75/99): loss =525146.7778350963, w0=-0.19734296753311753, w1=0.1970972199954122\n",
      "Gradient Descent(76/99): loss =524370.8036721512, w0=-0.19731751853066073, w1=0.19704646297143597\n",
      "Gradient Descent(77/99): loss =523596.10461650806, w0=-0.19729219801553982, w1=0.19699567047686103\n",
      "Gradient Descent(78/99): loss =522822.6770719597, w0=-0.19726700414549433, w1=0.1969448434503068\n",
      "Gradient Descent(79/99): loss =522050.51748321013, w0=-0.19724193512171456, w1=0.19689398280454232\n",
      "Gradient Descent(80/99): loss =521279.6223344896, w0=-0.19721698918743524, w1=0.19684308942806952\n",
      "Gradient Descent(81/99): loss =520509.98814829066, w0=-0.19719216462660075, w1=0.19679216418655074\n",
      "Gradient Descent(82/99): loss =519741.6114842277, w0=-0.19716745976259592, w1=0.19674120792409772\n",
      "Gradient Descent(83/99): loss =518974.4889379779, w0=-0.19714287295703772, w1=0.1966902214644383\n",
      "Gradient Descent(84/99): loss =518208.6171403029, w0=-0.19711840260862262, w1=0.19663920561197404\n",
      "Gradient Descent(85/99): loss =517443.99275612994, w0=-0.19709404715202597, w1=0.19658816115274114\n",
      "Gradient Descent(86/99): loss =516680.6124836872, w0=-0.19706980505684935, w1=0.19653708885528537\n",
      "Gradient Descent(87/99): loss =515918.4730536875, w0=-0.1970456748266127, w1=0.19648598947146062\n",
      "Gradient Descent(88/99): loss =515157.57122854644, w0=-0.19702165499778818, w1=0.19643486373715918\n",
      "Gradient Descent(89/99): loss =514397.9038016398, w0=-0.19699774413887275, w1=0.1963837123729815\n",
      "Gradient Descent(90/99): loss =513639.46759659, w0=-0.19697394084949738, w1=0.1963325360848518\n",
      "Gradient Descent(91/99): loss =512882.2594665804, w0=-0.19695024375957015, w1=0.19628133556458538\n",
      "Gradient Descent(92/99): loss =512126.27629369387, w0=-0.1969266515284518, w1=0.1962301114904128\n",
      "Gradient Descent(93/99): loss =511371.51498827734, w0=-0.19690316284416098, w1=0.1961788645274654\n",
      "Gradient Descent(94/99): loss =510617.97248832346, w0=-0.1968797764226085, w1=0.1961275953282264\n",
      "Gradient Descent(95/99): loss =509865.64575887437, w0=-0.1968564910068581, w1=0.19607630453295066\n",
      "Gradient Descent(96/99): loss =509114.53179144184, w0=-0.19683330536641275, w1=0.196024992770057\n",
      "Gradient Descent(97/99): loss =508364.62760345003, w0=-0.19681021829652526, w1=0.19597366065649513\n",
      "Gradient Descent(98/99): loss =507615.93023768783, w0=-0.19678722861753162, w1=0.19592230879809017\n",
      "Gradient Descent(99/99): loss =506868.4367617815, w0=-0.19676433517420613, w1=0.19587093778986678\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =203919134.7457476, w0=-5.476852500685414e-08, w1=0.19999999999229326\n",
      "Gradient Descent(2/99): loss =354164.9952755338, w0=-0.00720218977296672, w1=0.1988237603039746\n",
      "Gradient Descent(3/99): loss =26239.179554415263, w0=-0.010963211936783346, w1=0.19873289241983202\n",
      "Gradient Descent(4/99): loss =19339.839809660683, w0=-0.015427249781884485, w1=0.19848501367907795\n",
      "Gradient Descent(5/99): loss =18791.3112738788, w0=-0.019522575975007766, w1=0.19825286339731285\n",
      "Gradient Descent(6/99): loss =18404.488634395097, w0=-0.02367815221288933, w1=0.19801570651786904\n",
      "Gradient Descent(7/99): loss =18046.164116787004, w0=-0.027632210968566554, w1=0.19777771456208076\n",
      "Gradient Descent(8/99): loss =17706.40764055226, w0=-0.031564812134206095, w1=0.19753908093057337\n",
      "Gradient Descent(9/99): loss =17380.426696936327, w0=-0.03535928565114775, w1=0.19729931474142123\n",
      "Gradient Descent(10/99): loss =17065.395384996627, w0=-0.03909926230800026, w1=0.19705807051923152\n",
      "Gradient Descent(11/99): loss =16759.595150630754, w0=-0.042733091662148906, w1=0.1968146872222137\n",
      "Gradient Descent(12/99): loss =16461.922970550673, w0=-0.046299795476869056, w1=0.19656875192287618\n",
      "Gradient Descent(13/99): loss =16171.64252723792, w0=-0.049777293449959285, w1=0.19631980347212216\n",
      "Gradient Descent(14/99): loss =15888.224650430331, w0=-0.05318419019142773, w1=0.19606755737401296\n",
      "Gradient Descent(15/99): loss =15611.273890163422, w0=-0.056511622783925256, w1=0.19581175112181154\n",
      "Gradient Descent(16/99): loss =15340.475941981287, w0=-0.05976887464670422, w1=0.1955522314165523\n",
      "Gradient Descent(17/99): loss =15075.57307316775, w0=-0.06295290588981585, w1=0.19528887612392082\n",
      "Gradient Descent(18/99): loss =14816.344921937032, w0=-0.06606872167938717, w1=0.19502162713655322\n",
      "Gradient Descent(19/99): loss =14562.598919082815, w0=-0.06911579639485677, w1=0.1947504513656528\n",
      "Gradient Descent(20/99): loss =14314.162433637563, w0=-0.07209714305345116, w1=0.19447535232342245\n",
      "Gradient Descent(21/99): loss =14070.878472902104, w0=-0.07501328301138789, w1=0.19419635055904125\n",
      "Gradient Descent(22/99): loss =13832.602094966924, w0=-0.0778662749472497, w1=0.19391348713859202\n",
      "Gradient Descent(23/99): loss =13599.198231306244, w0=-0.08065704405151172, w1=0.19362681371857862\n",
      "Gradient Descent(24/99): loss =13370.53986742401, w0=-0.083387168712409, w1=0.19333639313500525\n",
      "Gradient Descent(25/99): loss =13146.506826076193, w0=-0.08605770116203572, w1=0.19304229428462535\n",
      "Gradient Descent(26/99): loss =12926.984752684659, w0=-0.08866995899438053, w1=0.19274459182913187\n",
      "Gradient Descent(27/99): loss =12711.864380147117, w0=-0.09122500617842107, w1=0.1924433635094694\n",
      "Gradient Descent(28/99): loss =12501.040915520167, w0=-0.09372400374711728, w1=0.19213868969097822\n",
      "Gradient Descent(29/99): loss =12294.413567968002, w0=-0.09616798243416612, w1=0.1918306519332456\n",
      "Gradient Descent(30/99): loss =12091.885153124527, w0=-0.09855799835788974, w1=0.1915193326042417\n",
      "Gradient Descent(31/99): loss =11893.361775081157, w0=-0.10089503408652796, w1=0.19120481411243592\n",
      "Gradient Descent(32/99): loss =11698.752557585001, w0=-0.10318006811559448, w1=0.19088717863531965\n",
      "Gradient Descent(33/99): loss =11507.969421038879, w0=-0.10541403230886072, w1=0.19056650770851605\n",
      "Gradient Descent(34/99): loss =11320.926891838526, w0=-0.10759784324835929, w1=0.19024288205517112\n",
      "Gradient Descent(35/99): loss =11137.541940270776, w0=-0.10973238431498708, w1=0.18991638137118247\n",
      "Gradient Descent(36/99): loss =10957.733840019931, w0=-0.11181852013667591, w1=0.18958708422902176\n",
      "Gradient Descent(37/99): loss =10781.424046229784, w0=-0.11385708936685257, w1=0.18925506797221475\n",
      "Gradient Descent(38/99): loss =10608.536088228697, w0=-0.11584891159898233, w1=0.18892040866944756\n",
      "Gradient Descent(39/99): loss =10438.995474689913, w0=-0.11779478473784916, w1=0.1885831810703205\n",
      "Gradient Descent(40/99): loss =10272.729608899816, w0=-0.11969548851917056, w1=0.18824345859150385\n",
      "Gradient Descent(41/99): loss =10109.667712574923, w0=-0.12155178381225523, w1=0.18790131330675813\n",
      "Gradient Descent(42/99): loss =9949.740756768228, w0=-0.12336441457839226, w1=0.18755681595252552\n",
      "Gradient Descent(43/99): loss =9792.881398792779, w0=-0.12513410795274157, w1=0.18721003593679578\n",
      "Gradient Descent(44/99): loss =9639.023924216299, w0=-0.1268615754590209, w1=0.1868610413558448\n",
      "Gradient Descent(45/99): loss =9488.104193194033, w0=-0.12854751337781667, w1=0.18650989901306714\n",
      "Gradient Descent(46/99): loss =9340.059590512006, w0=-0.1301926035864235, w1=0.18615667444160858\n",
      "Gradient Descent(47/99): loss =9194.828978838928, w0=-0.13179751400741255, w1=0.1858014319280438\n",
      "Gradient Descent(48/99): loss =9052.352654762977, w0=-0.13336289924365272, w1=0.18544423453767375\n",
      "Gradient Descent(49/99): loss =8912.572307267863, w0=-0.13488940102463803, w1=0.18508514414011348\n",
      "Gradient Descent(50/99): loss =8775.430978357666, w0=-0.13637764871796326, w1=0.18472422143533043\n",
      "Gradient Descent(51/99): loss =8640.873025589937, w0=-0.13782825974355042, w1=0.18436152597948918\n",
      "Gradient Descent(52/99): loss =8508.844086314966, w0=-0.13924184000269169, w1=0.18399711621062928\n",
      "Gradient Descent(53/99): loss =8379.291043451625, w0=-0.1406189842521659, w1=0.18363104947386705\n",
      "Gradient Descent(54/99): loss =8252.161992656685, w0=-0.14196027647335727, w1=0.183263382046118\n",
      "Gradient Descent(55/99): loss =8127.406210766043, w0=-0.14326629020687548, w1=0.182894169160198\n",
      "Gradient Descent(56/99): loss =8004.97412540447, w0=-0.14453758887529936, w1=0.1825234650283014\n",
      "Gradient Descent(57/99): loss =7884.817285675123, w0=-0.14577472608194006, w1=0.18215132286479985\n",
      "Gradient Descent(58/99): loss =7766.888333852346, w0=-0.14697824589618344, w1=0.18177779490836818\n",
      "Gradient Descent(59/99): loss =7651.140978011459, w0=-0.1481486831207636, w1=0.18140293244342456\n",
      "Gradient Descent(60/99): loss =7537.529965537489, w0=-0.14928656354614656, w1=0.1810267858208994\n",
      "Gradient Descent(61/99): loss =7426.011057462172, w0=-0.15039240419052077, w1=0.18064940447834077\n",
      "Gradient Descent(62/99): loss =7316.541003583959, w0=-0.1514667135281296, w1=0.18027083695937693\n",
      "Gradient Descent(63/99): loss =7209.077518331198, w0=-0.15250999170572688, w1=0.17989113093255293\n",
      "Gradient Descent(64/99): loss =7103.579257332261, w0=-0.1535227307487463, w1=0.17951033320956467\n",
      "Gradient Descent(65/99): loss =7000.0057946604375, w0=-0.15450541475745447, w1=0.1791284897629113\n",
      "Gradient Descent(66/99): loss =6898.317600723984, w0=-0.15545852009411487, w1=0.17874564574298996\n",
      "Gradient Descent(67/99): loss =6798.476020774404, w0=-0.15638251556158744, w1=0.17836184549465472\n",
      "Gradient Descent(68/99): loss =6700.443254008418, w0=-0.15727786257409274, w1=0.1779771325732627\n",
      "Gradient Descent(69/99): loss =6604.18233324052, w0=-0.15814501532058417, w1=0.17759154976022895\n",
      "Gradient Descent(70/99): loss =6509.657105125085, w0=-0.15898442092128412, w1=0.17720513907811156\n",
      "Gradient Descent(71/99): loss =6416.832210908236, w0=-0.15979651957779595, w1=0.17681794180524715\n",
      "Gradient Descent(72/99): loss =6325.6730676908255, w0=-0.16058174471723577, w1=0.17642999848995652\n",
      "Gradient Descent(73/99): loss =6236.145850185244, w0=-0.16134052313074973, w1=0.17604134896433915\n",
      "Gradient Descent(74/99): loss =6148.217472949609, w0=-0.1620732751067816, w1=0.17565203235767413\n",
      "Gradient Descent(75/99): loss =6061.855573083643, w0=-0.16278041455940898, w1=0.1752620871094448\n",
      "Gradient Descent(76/99): loss =5977.028493371714, w0=-0.16346234915205343, w1=0.17487155098200297\n",
      "Gradient Descent(77/99): loss =5893.705265858766, w0=-0.16411948041683927, w1=0.17448046107288803\n",
      "Gradient Descent(78/99): loss =5811.855595845977, w0=-0.16475220386985942, w1=0.17408885382681544\n",
      "Gradient Descent(79/99): loss =5731.449846293225, w0=-0.16536090912258483, w1=0.1736967650473482\n",
      "Gradient Descent(80/99): loss =5652.45902261614, w0=-0.1659459799896382, w1=0.17330422990826452\n",
      "Gradient Descent(81/99): loss =5574.854757866145, w0=-0.1665077945931351, w1=0.1729112829646337\n",
      "Gradient Descent(82/99): loss =5498.609298282123, w0=-0.1670467254637825, w1=0.17251795816361207\n",
      "Gradient Descent(83/99): loss =5423.695489202924, w0=-0.1675631396389095, w1=0.17212428885496983\n",
      "Gradient Descent(84/99): loss =5350.086761330444, w0=-0.16805739875759426, w1=0.17173030780135956\n",
      "Gradient Descent(85/99): loss =5277.757117333126, w0=-0.16852985915303825, w1=0.17133604718833587\n",
      "Gradient Descent(86/99): loss =5206.681118780351, w0=-0.16898087194232947, w1=0.17094153863413603\n",
      "Gradient Descent(87/99): loss =5136.833873398437, w0=-0.16941078311372615, w1=0.17054681319923015\n",
      "Gradient Descent(88/99): loss =5068.191022639208, w0=-0.1698199336115838, w1=0.1701519013956497\n",
      "Gradient Descent(89/99): loss =5000.7287295525775, w0=-0.17020865941903965, w1=0.16975683319610202\n",
      "Gradient Descent(90/99): loss =4934.4236669547745, w0=-0.17057729163856195, w1=0.1693616380428788\n",
      "Gradient Descent(91/99): loss =4869.253005884064, w0=-0.1709261565704636, w1=0.16896634485656575\n",
      "Gradient Descent(92/99): loss =4805.1944043362755, w0=-0.17125557578947379, w1=0.16857098204456017\n",
      "Gradient Descent(93/99): loss =4742.225996272499, w0=-0.17156586621945497, w1=0.16817557750940323\n",
      "Gradient Descent(94/99): loss =4680.326380891698, w0=-0.17185734020634713, w1=0.16778015865693313\n",
      "Gradient Descent(95/99): loss =4619.474612161117, w0=-0.17213030558941608, w1=0.16738475240426506\n",
      "Gradient Descent(96/99): loss =4559.650188597686, w0=-0.17238506577087787, w1=0.16698938518760395\n",
      "Gradient Descent(97/99): loss =4500.833043293773, w0=-0.17262191978396688, w1=0.16659408296989495\n",
      "Gradient Descent(98/99): loss =4443.003534180825, w0=-0.1728411623595111, w1=0.16619887124831734\n",
      "Gradient Descent(99/99): loss =4386.142434524719, w0=-0.17304308399107438, w1=0.16580377506162658\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =43218867554022.24, w0=-0.1999999999889907, w1=0.1999999999999769\n",
      "Gradient Descent(2/99): loss =1222290.612271354, w0=-0.19984890003643557, w1=0.2001700376734688\n",
      "Gradient Descent(3/99): loss =1083608.2526220446, w0=-0.19974623547155973, w1=0.20028697175802604\n",
      "Gradient Descent(4/99): loss =1022735.8687697119, w0=-0.19965532853182824, w1=0.2003805251083569\n",
      "Gradient Descent(5/99): loss =977021.1819851807, w0=-0.19957332761445531, w1=0.20045890320000526\n",
      "Gradient Descent(6/99): loss =939918.6974322366, w0=-0.19949787477986355, w1=0.20052691243853815\n",
      "Gradient Descent(7/99): loss =909013.6146239633, w0=-0.19942762787155724, w1=0.20058724107491127\n",
      "Gradient Descent(8/99): loss =882846.206716681, w0=-0.1993616615318697, w1=0.20064149252242336\n",
      "Gradient Descent(9/99): loss =860389.220345128, w0=-0.19929928527051244, w1=0.20069067783503297\n",
      "Gradient Descent(10/99): loss =840888.3309664557, w0=-0.19923995765090413, w1=0.20073547233398883\n",
      "Gradient Descent(11/99): loss =823778.2706989684, w0=-0.19918324308470642, w1=0.20077635332074642\n",
      "Gradient Descent(12/99): loss =808628.6406163183, w0=-0.19912878631546313, w1=0.20081367637591888\n",
      "Gradient Descent(13/99): loss =795106.8677108135, w0=-0.19907629539249308, w1=0.20084771894635825\n",
      "Gradient Descent(14/99): loss =782952.3388658223, w0=-0.1990255292802164, w1=0.20087870605356892\n",
      "Gradient Descent(15/99): loss =771958.0875213846, w0=-0.1989762883518913, w1=0.20090682598754647\n",
      "Gradient Descent(16/99): loss =761957.6689654144, w0=-0.1989284068700643, w1=0.20093224024701742\n",
      "Gradient Descent(17/99): loss =752815.6430498714, w0=-0.19888174693331637, w1=0.20095509008107235\n",
      "Gradient Descent(18/99): loss =744420.5918210573, w0=-0.1988361935529859, w1=0.20097550096020322\n",
      "Gradient Descent(19/99): loss =736679.9366793297, w0=-0.1987916506239149, w1=0.20099358574236625\n",
      "Gradient Descent(20/99): loss =729516.0459929217, w0=-0.19874803761406198, w1=0.20100944698678824\n",
      "Gradient Descent(21/99): loss =722863.2775738669, w0=-0.19870528683805647, w1=0.2010231786911032\n",
      "Gradient Descent(22/99): loss =716665.7054509191, w0=-0.19866334120823353, w1=0.20103486762509742\n",
      "Gradient Descent(23/99): loss =710875.352833615, w0=-0.19862215237781736, w1=0.2010445943738226\n",
      "Gradient Descent(24/99): loss =705450.8035415773, w0=-0.19858167920713168, w1=0.20105243416602284\n",
      "Gradient Descent(25/99): loss =700356.099467797, w0=-0.19854188649644644, w1=0.20105845754071\n",
      "Gradient Descent(26/99): loss =695559.8565583152, w0=-0.1985027439392228, w1=0.20106273088971485\n",
      "Gradient Descent(27/99): loss =691034.549512165, w0=-0.19846422525771049, w1=0.20106531690395962\n",
      "Gradient Descent(28/99): loss =686755.9281107814, w0=-0.1984263074895109, w1=0.2010662749442047\n",
      "Gradient Descent(29/99): loss =682702.537270004, w0=-0.19838897039916423, w1=0.20106566135202963\n",
      "Gradient Descent(30/99): loss =678855.3196030085, w0=-0.1983521959932904, w1=0.20106352971315986\n",
      "Gradient Descent(31/99): loss =675197.2842070906, w0=-0.19831596812149524, w1=0.20105993108253045\n",
      "Gradient Descent(32/99): loss =671713.2290426454, w0=-0.19828027214829289, w1=0.20105491417842017\n",
      "Gradient Descent(33/99): loss =668389.5070120746, w0=-0.1982450946838093, w1=0.2010485255514145\n",
      "Gradient Descent(34/99): loss =665213.8279190633, w0=-0.1982104233631131, w1=0.2010408097327422\n",
      "Gradient Descent(35/99): loss =662175.0900723472, w0=-0.19817624666574596, w1=0.20103180936558532\n",
      "Gradient Descent(36/99): loss =659263.2365198991, w0=-0.19814255376845677, w1=0.2010215653222282\n",
      "Gradient Descent(37/99): loss =656469.1318511568, w0=-0.19810933442533224, w1=0.20101011680933356\n",
      "Gradient Descent(38/99): loss =653784.4562530122, w0=-0.19807657887050553, w1=0.2009975014631828\n",
      "Gradient Descent(39/99): loss =651201.6140986157, w0=-0.19804427773944444, w1=0.20098375543636046\n",
      "Gradient Descent(40/99): loss =648713.6548225296, w0=-0.19801242200550362, w1=0.20096891347708312\n",
      "Gradient Descent(41/99): loss =646314.2042180857, w0=-0.19798100292899304, w1=0.20095300900215016\n",
      "Gradient Descent(42/99): loss =643997.4046031513, w0=-0.19795001201648646, w1=0.20093607416431822\n",
      "Gradient Descent(43/99): loss =641757.8625540234, w0=-0.19791944098848632, w1=0.20091813991476176\n",
      "Gradient Descent(44/99): loss =639590.6031153352, w0=-0.19788928175388779, w1=0.20089923606116994\n",
      "Gradient Descent(45/99): loss =637491.0295659688, w0=-0.19785952638995566, w1=0.2008793913219417\n",
      "Gradient Descent(46/99): loss =635454.887963537, w0=-0.19783016712675386, w1=0.20085863337686932\n",
      "Gradient Descent(47/99): loss =633478.2358089398, w0=-0.19780119633515378, w1=0.2008369889146422\n",
      "Gradient Descent(48/99): loss =631557.4142718405, w0=-0.19777260651770368, w1=0.20081448367745747\n",
      "Gradient Descent(49/99): loss =629689.0235011986, w0=-0.19774439030177038, w1=0.2007911425029845\n",
      "Gradient Descent(50/99): loss =627869.9006151413, w0=-0.19771654043447118, w1=0.20076698936389956\n",
      "Gradient Descent(51/99): loss =626097.100023463, w0=-0.1976890497790027, w1=0.2007420474051815\n",
      "Gradient Descent(52/99): loss =624367.8757859997, w0=-0.1976619113120469, w1=0.20071633897933733\n",
      "Gradient Descent(53/99): loss =622679.6657523554, w0=-0.197635118121995, w1=0.20068988567970833\n",
      "Gradient Descent(54/99): loss =621030.0772642873, w0=-0.19760866340777955, w1=0.20066270837199288\n",
      "Gradient Descent(55/99): loss =619416.8742325107, w0=-0.1975825404781476, w1=0.2006348272241081\n",
      "Gradient Descent(56/99): loss =617837.9654255549, w0=-0.19755674275124008, w1=0.2006062617345021\n",
      "Gradient Descent(57/99): loss =616291.3938303904, w0=-0.19753126375437163, w1=0.200577030759018\n",
      "Gradient Descent(58/99): loss =614775.3269633437, w0=-0.19750609712392772, w1=0.2005471525364032\n",
      "Gradient Descent(59/99): loss =613288.0480259244, w0=-0.197481236605314, w1=0.2005166447125491\n",
      "Gradient Descent(60/99): loss =611827.9478139321, w0=-0.19745667605290873, w1=0.2004855243635402\n",
      "Gradient Descent(61/99): loss =610393.5173000031, w0=-0.1974324094299811, w1=0.2004538080175854\n",
      "Gradient Descent(62/99): loss =608983.3408198694, w0=-0.1974084308085481, w1=0.20042151167589875\n",
      "Gradient Descent(63/99): loss =607596.0898013082, w0=-0.19738473436915094, w1=0.20038865083259239\n",
      "Gradient Descent(64/99): loss =606230.5169821979, w0=-0.19736131440053806, w1=0.2003552404936395\n",
      "Gradient Descent(65/99): loss =604885.4510706017, w0=-0.19733816529924736, w1=0.20032129519496153\n",
      "Gradient Descent(66/99): loss =603559.7918052862, w0=-0.19731528156908335, w1=0.20028682901968967\n",
      "Gradient Descent(67/99): loss =602252.5053799743, w0=-0.19729265782048916, w1=0.20025185561464778\n",
      "Gradient Descent(68/99): loss =600962.6201987275, w0=-0.19727028876981476, w1=0.20021638820610038\n",
      "Gradient Descent(69/99): loss =599689.2229335156, w0=-0.19724816923848504, w1=0.2001804396148064\n",
      "Gradient Descent(70/99): loss =598431.4548581712, w0=-0.19722629415207266, w1=0.20014402227041705\n",
      "Gradient Descent(71/99): loss =597188.5084356274, w0=-0.1972046585392812, w1=0.20010714822525347\n",
      "Gradient Descent(72/99): loss =595959.6241377982, w0=-0.19718325753084523, w1=0.20006982916749727\n",
      "Gradient Descent(73/99): loss =594744.0874795037, w0=-0.1971620863583537, w1=0.20003207643382556\n",
      "Gradient Descent(74/99): loss =593541.2262496852, w0=-0.197141140353004, w1=0.1999939010215194\n",
      "Gradient Descent(75/99): loss =592350.4079248235, w0=-0.19712041494429325, w1=0.19995531360007332\n",
      "Gradient Descent(76/99): loss =591171.0372508145, w0=-0.19709990565865373, w1=0.19991632452233146\n",
      "Gradient Descent(77/99): loss =590002.5539809092, w0=-0.19707960811803935, w1=0.19987694383517443\n",
      "Gradient Descent(78/99): loss =588844.4307583545, w0=-0.19705951803846927, w1=0.19983718128977954\n",
      "Gradient Descent(79/99): loss =587696.17113345, w0=-0.19703963122853516, w1=0.1997970463514756\n",
      "Gradient Descent(80/99): loss =586557.3077055458, w0=-0.19701994358787753, w1=0.19975654820921235\n",
      "Gradient Descent(81/99): loss =585427.4003813392, w0=-0.197000451105637, w1=0.199715695784663\n",
      "Gradient Descent(82/99): loss =584306.0347415371, w0=-0.19698114985888537, w1=0.19967449774097776\n",
      "Gradient Descent(83/99): loss =583192.8205085609, w0=-0.19696203601104142, w1=0.19963296249120488\n",
      "Gradient Descent(84/99): loss =582087.3901085731, w0=-0.19694310581027577, w1=0.19959109820639465\n",
      "Gradient Descent(85/99): loss =580989.3973215951, w0=-0.1969243555879091, w1=0.1995489128234013\n",
      "Gradient Descent(86/99): loss =579898.5160139925, w0=-0.19690578175680717, w1=0.19950641405239644\n",
      "Gradient Descent(87/99): loss =578814.4389479704, w0=-0.19688738080977639, w1=0.19946360938410734\n",
      "Gradient Descent(88/99): loss =577736.8766631827, w0=-0.19686914931796287, w1=0.19942050609679204\n",
      "Gradient Descent(89/99): loss =576665.5564258393, w0=-0.196851083929258, w1=0.1993771112629633\n",
      "Gradient Descent(90/99): loss =575600.2212410744, w0=-0.19683318136671282, w1=0.19933343175587218\n",
      "Gradient Descent(91/99): loss =574540.6289246058, w0=-0.19681543842696395, w1=0.1992894742557616\n",
      "Gradient Descent(92/99): loss =573486.551229984, w0=-0.19679785197867264, w1=0.19924524525589984\n",
      "Gradient Descent(93/99): loss =572437.7730280013, w0=-0.19678041896097928, w1=0.19920075106840332\n",
      "Gradient Descent(94/99): loss =571394.0915350383, w0=-0.1967631363819746, w1=0.19915599782985713\n",
      "Gradient Descent(95/99): loss =570355.3155873467, w0=-0.1967460013171893, w1=0.19911099150674216\n",
      "Gradient Descent(96/99): loss =569321.2649584784, w0=-0.1967290109081034, w1=0.19906573790067641\n",
      "Gradient Descent(97/99): loss =568291.7697172065, w0=-0.19671216236067599, w1=0.19902024265347804\n",
      "Gradient Descent(98/99): loss =567266.6696235223, w0=-0.19669545294389687, w1=0.1989745112520573\n",
      "Gradient Descent(99/99): loss =566245.8135603725, w0=-0.19667887998836064, w1=0.1989285490331443\n",
      "Optimizing degree 6/15, model: least_squares_GD, arguments: {'max_iters': 100}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =6.546610502276288e+17, w0=-0.19999999999999932, w1=0.20000000000000004\n",
      "Gradient Descent(2/99): loss =333601104.79941964, w0=-0.19998807980586122, w1=0.20001607634069474\n",
      "Gradient Descent(3/99): loss =319710268.5279273, w0=-0.19997777729592925, w1=0.20003044515181684\n",
      "Gradient Descent(4/99): loss =310406740.76511985, w0=-0.19996789993208314, w1=0.20004378504433948\n",
      "Gradient Descent(5/99): loss =302162258.903154, w0=-0.19995834281140318, w1=0.20005635085089052\n",
      "Gradient Descent(6/99): loss =294769258.80998164, w0=-0.19994905622336656, w1=0.20006830876344522\n",
      "Gradient Descent(7/99): loss =288107014.1331901, w0=-0.19994000929675482, w1=0.2000797679075654\n",
      "Gradient Descent(8/99): loss =282078788.233332, w0=-0.19993117875165461, w1=0.20009080241350152\n",
      "Gradient Descent(9/99): loss =276603982.18714905, w0=-0.1999225454607625, w1=0.2001014643557955\n",
      "Gradient Descent(10/99): loss =271614689.91062015, w0=-0.19991409313488506, w1=0.20011179161149173\n",
      "Gradient Descent(11/99): loss =267053234.32232103, w0=-0.19990580766177526, w1=0.20012181272727908\n",
      "Gradient Descent(12/99): loss =262870278.08150885, w0=-0.1998976766909539, w1=0.20013154997723573\n",
      "Gradient Descent(13/99): loss =259023350.3513492, w0=-0.19988968933694823, w1=0.2001410213099965\n",
      "Gradient Descent(14/99): loss =255475685.07456934, w0=-0.1998818359523475, w1=0.20015024160751482\n",
      "Gradient Descent(15/99): loss =252195295.58480397, w0=-0.19987410794754668, w1=0.2001592235132236\n",
      "Gradient Descent(16/99): loss =249154230.32168785, w0=-0.19986649764404243, w1=0.2001679779881118\n",
      "Gradient Descent(17/99): loss =246327968.59995463, w0=-0.19985899815294947, w1=0.20017651469277695\n",
      "Gradient Descent(18/99): loss =243694925.65004334, w0=-0.19985160327309404, w1=0.20018484225652342\n",
      "Gradient Descent(19/99): loss =241236043.66437984, w0=-0.19984430740468773, w1=0.20019296847185522\n",
      "Gradient Descent(20/99): loss =238934451.13275146, w0=-0.19983710547564915, w1=0.2002009004386915\n",
      "Gradient Descent(21/99): loss =236775176.87549493, w0=-0.1998299928783556, w1=0.20020864467393693\n",
      "Gradient Descent(22/99): loss =234744908.26959142, w0=-0.19982296541510203, w1=0.20021620719660693\n",
      "Gradient Descent(23/99): loss =232831785.48588172, w0=-0.1998160192508978, w1=0.20022359359528363\n",
      "Gradient Descent(24/99): loss =231025225.31503546, w0=-0.1998091508724922, w1=0.2002308090824985\n",
      "Gradient Descent(25/99): loss =229315769.50065857, w0=-0.19980235705271698, w1=0.20023785853922738\n",
      "Gradient Descent(26/99): loss =227694953.5262333, w0=-0.19979563481938595, w1=0.2002447465517596\n",
      "Gradient Descent(27/99): loss =226155192.59647962, w0=-0.19978898142811463, w1=0.2002514774425832\n",
      "Gradient Descent(28/99): loss =224689682.1708916, w0=-0.19978239433851994, w1=0.20025805529650578\n",
      "Gradient Descent(29/99): loss =223292310.89034766, w0=-0.199775871193341, w1=0.20026448398293506\n",
      "Gradient Descent(30/99): loss =221957584.11872524, w0=-0.19976940980008934, w1=0.20027076717503134\n",
      "Gradient Descent(31/99): loss =220680556.62414834, w0=-0.19976300811489212, w1=0.20027690836628997\n",
      "Gradient Descent(32/99): loss =219456773.16684458, w0=-0.1997566642282406, w1=0.20028291088499636\n",
      "Gradient Descent(33/99): loss =218282215.956033, w0=-0.19975037635239498, w1=0.2002887779069078\n",
      "Gradient Descent(34/99): loss =217153258.09714484, w0=-0.19974414281023237, w1=0.20029451246644892\n",
      "Gradient Descent(35/99): loss =216066622.28076565, w0=-0.19973796202535243, w1=0.20030011746665383\n",
      "Gradient Descent(36/99): loss =215019344.07200566, w0=-0.19973183251328194, w1=0.2003055956880458\n",
      "Gradient Descent(37/99): loss =214008739.24817353, w0=-0.19972575287363964, w1=0.20031094979661146\n",
      "Gradient Descent(38/99): loss =213032374.70719817, w0=-0.1997197217831421, w1=0.20031618235099918\n",
      "Gradient Descent(39/99): loss =212088042.5320615, w0=-0.1997137379893471, w1=0.20032129580904912\n",
      "Gradient Descent(40/99): loss =211173736.84969485, w0=-0.1997078003050446, w1=0.20032629253374437\n",
      "Gradient Descent(41/99): loss =210287633.1681204, w0=-0.19970190760321685, w1=0.20033117479865803\n",
      "Gradient Descent(42/99): loss =209428069.91445553, w0=-0.19969605881250074, w1=0.20033594479295855\n",
      "Gradient Descent(43/99): loss =208593531.9298028, w0=-0.19969025291309234, w1=0.20034060462602626\n",
      "Gradient Descent(44/99): loss =207782635.705957, w0=-0.19968448893304286, w1=0.20034515633172545\n",
      "Gradient Descent(45/99): loss =206994116.17393416, w0=-0.1996787659449011, w1=0.20034960187236958\n",
      "Gradient Descent(46/99): loss =206226814.87618417, w0=-0.1996730830626632, w1=0.20035394314241198\n",
      "Gradient Descent(47/99): loss =205479669.37345555, w0=-0.1996674394389954, w1=0.20035818197188932\n",
      "Gradient Descent(48/99): loss =204751703.75402263, w0=-0.19966183426270018, w1=0.2003623201296414\n",
      "Gradient Descent(49/99): loss =204042020.12770826, w0=-0.19965626675639953, w1=0.20036635932632793\n",
      "Gradient Descent(50/99): loss =203349791.00011504, w0=-0.19965073617441195, w1=0.2003703012172597\n",
      "Gradient Descent(51/99): loss =202674252.4339119, w0=-0.1996452418008037, w1=0.20037414740505977\n",
      "Gradient Descent(52/99): loss =202014697.9141665, w0=-0.1996397829475959, w1=0.20037789944216827\n",
      "Gradient Descent(53/99): loss =201370472.84368157, w0=-0.19963435895311227, w1=0.2003815588332028\n",
      "Gradient Descent(54/99): loss =200740969.60224974, w0=-0.19962896918045336, w1=0.20038512703718495\n",
      "Gradient Descent(55/99): loss =200125623.11082196, w0=-0.19962361301608558, w1=0.2003886054696427\n",
      "Gradient Descent(56/99): loss =199523906.8478645, w0=-0.1996182898685335, w1=0.20039199550459705\n",
      "Gradient Descent(57/99): loss =198935329.27077964, w0=-0.19961299916716646, w1=0.20039529847644047\n",
      "Gradient Descent(58/99): loss =198359430.60025755, w0=-0.1996077403610708, w1=0.20039851568171457\n",
      "Gradient Descent(59/99): loss =197795779.92986462, w0=-0.19960251291799982, w1=0.2004016483807929\n",
      "Gradient Descent(60/99): loss =197243972.6271482, w0=-0.19959731632339536, w1=0.200404697799475\n",
      "Gradient Descent(61/99): loss =196703627.99607217, w0=-0.19959215007947417, w1=0.20040766513049685\n",
      "Gradient Descent(62/99): loss =196174387.17376375, w0=-0.1995870137043742, w1=0.20041055153496276\n",
      "Gradient Descent(63/99): loss =195655911.23738462, w0=-0.19958190673135598, w1=0.20041335814370334\n",
      "Gradient Descent(64/99): loss =195147879.49945033, w0=-0.19957682870805415, w1=0.20041608605856354\n",
      "Gradient Descent(65/99): loss =194649987.9722023, w0=-0.199571779195776, w1=0.20041873635362492\n",
      "Gradient Descent(66/99): loss =194161947.98363233, w0=-0.19956675776884278, w1=0.20042131007636568\n",
      "Gradient Descent(67/99): loss =193683484.9295974, w0=-0.19956176401397088, w1=0.200423808248762\n",
      "Gradient Descent(68/99): loss =193214337.14805308, w0=-0.19955679752969005, w1=0.20042623186833372\n",
      "Gradient Descent(69/99): loss =192754254.90290126, w0=-0.1995518579257957, w1=0.20042858190913773\n",
      "Gradient Descent(70/99): loss =192302999.4662389, w0=-0.19954694482283306, w1=0.20043085932271135\n",
      "Gradient Descent(71/99): loss =191860342.28895712, w0=-0.19954205785161117, w1=0.20043306503896893\n",
      "Gradient Descent(72/99): loss =191426064.25067288, w0=-0.19953719665274414, w1=0.2004351999670538\n",
      "Gradient Descent(73/99): loss =190999954.9809292, w0=-0.1995323608762184, w1=0.20043726499614817\n",
      "Gradient Descent(74/99): loss =190581812.24440295, w0=-0.199527550180984, w1=0.20043926099624287\n",
      "Gradient Descent(75/99): loss =190171441.3836391, w0=-0.19952276423456833, w1=0.20044118881886958\n",
      "Gradient Descent(76/99): loss =189768654.81347638, w0=-0.19951800271271092, w1=0.20044304929779697\n",
      "Gradient Descent(77/99): loss =189373271.5619499, w0=-0.199513265299018, w1=0.20044484324969294\n",
      "Gradient Descent(78/99): loss =188985116.85297376, w0=-0.19950855168463558, w1=0.20044657147475475\n",
      "Gradient Descent(79/99): loss =188604021.72660908, w0=-0.19950386156793984, w1=0.20044823475730858\n",
      "Gradient Descent(80/99): loss =188229822.69314736, w0=-0.19949919465424396, w1=0.20044983386638013\n",
      "Gradient Descent(81/99): loss =187862361.41761813, w0=-0.19949455065552013, w1=0.2004513695562379\n",
      "Gradient Descent(82/99): loss =187501484.4316946, w0=-0.19948992929013606, w1=0.20045284256691046\n",
      "Gradient Descent(83/99): loss =187147042.87027398, w0=-0.19948533028260512, w1=0.20045425362467895\n",
      "Gradient Descent(84/99): loss =186798892.23027655, w0=-0.1994807533633492, w1=0.20045560344254637\n",
      "Gradient Descent(85/99): loss =186456892.14948958, w0=-0.19947619826847363, w1=0.2004568927206847\n",
      "Gradient Descent(86/99): loss =186120906.20347065, w0=-0.19947166473955355, w1=0.20045812214686087\n",
      "Gradient Descent(87/99): loss =185790801.7187499, w0=-0.19946715252343086, w1=0.2004592923968429\n",
      "Gradient Descent(88/99): loss =185466449.6007474, w0=-0.19946266137202137, w1=0.20046040413478713\n",
      "Gradient Descent(89/99): loss =185147724.17497563, w0=-0.1994581910421315, w1=0.2004614580136075\n",
      "Gradient Descent(90/99): loss =184834503.0402495, w0=-0.19945374129528398, w1=0.2004624546753279\n",
      "Gradient Descent(91/99): loss =184526666.93275848, w0=-0.1994493118975521, w1=0.20046339475141808\n",
      "Gradient Descent(92/99): loss =184224099.59996092, w0=-0.19944490261940193, w1=0.20046427886311477\n",
      "Gradient Descent(93/99): loss =183926687.68338382, w0=-0.19944051323554224, w1=0.20046510762172773\n",
      "Gradient Descent(94/99): loss =183634320.60948238, w0=-0.19943614352478164, w1=0.20046588162893242\n",
      "Gradient Descent(95/99): loss =183346890.4878226, w0=-0.19943179326989244, w1=0.2004666014770493\n",
      "Gradient Descent(96/99): loss =183064292.01590326, w0=-0.19942746225748098, w1=0.20046726774931098\n",
      "Gradient Descent(97/99): loss =182786422.39001834, w0=-0.19942315027786411, w1=0.20046788102011737\n",
      "Gradient Descent(98/99): loss =182513181.22161168, w0=-0.19941885712495147, w1=0.20046844185527973\n",
      "Gradient Descent(99/99): loss =182244470.45863855, w0=-0.1994145825961331, w1=0.20046895081225416\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =6.546608748815652e+17, w0=-0.19999999999999932, w1=0.2\n",
      "Gradient Descent(2/99): loss =191563361.18774396, w0=-0.19999396508359527, w1=0.20000476788369334\n",
      "Gradient Descent(3/99): loss =182308259.53408882, w0=-0.19998946126700332, w1=0.20000546865633972\n",
      "Gradient Descent(4/99): loss =178172768.64913464, w0=-0.1999851551567891, w1=0.20000490160711137\n",
      "Gradient Descent(5/99): loss =175010511.83539212, w0=-0.19998104086363075, w1=0.200003391794287\n",
      "Gradient Descent(6/99): loss =172585911.78803593, w0=-0.19997708143002244, w1=0.20000114359542523\n",
      "Gradient Descent(7/99): loss =170723339.12730473, w0=-0.19997325700015703, w1=0.19999828521676954\n",
      "Gradient Descent(8/99): loss =169289267.6233731, w0=-0.1999695497932944, w1=0.19999490931722327\n",
      "Gradient Descent(9/99): loss =168182026.6812327, w0=-0.19996594507704782, w1=0.1999910879723113\n",
      "Gradient Descent(10/99): loss =167324142.66308233, w0=-0.19996243018685794, w1=0.19998688034437823\n",
      "Gradient Descent(11/99): loss =166656549.7054454, w0=-0.19995899419668825, w1=0.19998233644178195\n",
      "Gradient Descent(12/99): loss =166134198.8820889, w0=-0.19995562763368707, w1=0.19997749922011174\n",
      "Gradient Descent(13/99): loss =165722721.93496555, w0=-0.19995232226912665, w1=0.19997240588431708\n",
      "Gradient Descent(14/99): loss =165395893.20592496, w0=-0.1999490709469046, w1=0.1999670887960825\n",
      "Gradient Descent(15/99): loss =165133697.07962492, w0=-0.19994586743996845, w1=0.199961576165298\n",
      "Gradient Descent(16/99): loss =164920855.4901856, w0=-0.19994270632765038, w1=0.19995589260983554\n",
      "Gradient Descent(17/99): loss =164745705.3935232, w0=-0.19993958288995703, w1=0.19995005962507426\n",
      "Gradient Descent(18/99): loss =164599342.69335803, w0=-0.19993649301603977, w1=0.19994409598546786\n",
      "Gradient Descent(19/99): loss =164474969.17624822, w0=-0.1999334331247633, w1=0.1999380180915267\n",
      "Gradient Descent(20/99): loss =164367394.19882375, w0=-0.19993040009569643, w1=0.19993184027122599\n",
      "Gradient Descent(21/99): loss =164272654.38893524, w0=-0.19992739120912623, w1=0.19992557504254005\n",
      "Gradient Descent(22/99): loss =164187723.3719394, w0=-0.19992440409390272, w1=0.19991923334243686\n",
      "Gradient Descent(23/99): loss =164110290.18762943, w0=-0.19992143668208726, w1=0.19991282472676414\n",
      "Gradient Descent(24/99): loss =164038590.1288363, w0=-0.19991848716951502, w1=0.19990635754479458\n",
      "Gradient Descent(25/99): loss =163971275.59158307, w0=-0.19991555398149977, w1=0.19989983909167497\n",
      "Gradient Descent(26/99): loss =163907317.46793306, w0=-0.19991263574301002, w1=0.19989327574159244\n",
      "Gradient Descent(27/99): loss =163845929.855485, w0=-0.19990973125273345, w1=0.19988667306410499\n",
      "Gradient Descent(28/99): loss =163786512.56824794, w0=-0.19990683946052215, w1=0.1998800359257694\n",
      "Gradient Descent(29/99): loss =163728607.23888674, w0=-0.19990395944777778, w1=0.1998733685789272\n",
      "Gradient Descent(30/99): loss =163671863.79843104, w0=-0.19990109041039303, w1=0.1998666747392733\n",
      "Gradient Descent(31/99): loss =163616014.879773, w0=-0.19989823164391593, w1=0.1998599576536251\n",
      "Gradient Descent(32/99): loss =163560856.27158284, w0=-0.1998953825306467, w1=0.19985322015913135\n",
      "Gradient Descent(33/99): loss =163506231.99229532, w0=-0.19989254252841543, w1=0.1998464647350021\n",
      "Gradient Descent(34/99): loss =163452022.89200246, w0=-0.19988971116082083, w1=0.19983969354770495\n",
      "Gradient Descent(35/99): loss =163398137.94832394, w0=-0.1998868880087399, w1=0.19983290849045332\n",
      "Gradient Descent(36/99): loss =163344507.619471, w0=-0.1998840727029424, w1=0.1998261112177074\n",
      "Gradient Descent(37/99): loss =163291078.7682539, w0=-0.19988126491766608, w1=0.19981930317531824\n",
      "Gradient Descent(38/99): loss =163237810.78572664, w0=-0.19987846436502774, w1=0.199812485626865\n",
      "Gradient Descent(39/99): loss =163184672.63093513, w0=-0.19987567079016053, w1=0.1998056596766665\n",
      "Gradient Descent(40/99): loss =163131640.57025298, w0=-0.19987288396698344, w1=0.19979882628988696\n",
      "Gradient Descent(41/99): loss =163078696.45095688, w0=-0.19987010369452018, w1=0.1997919863101029\n",
      "Gradient Descent(42/99): loss =163025826.3827858, w0=-0.19986732979369604, w1=0.1997851404746518\n",
      "Gradient Descent(43/99): loss =162973019.7310668, w0=-0.19986456210455036, w1=0.19977828942804307\n",
      "Gradient Descent(44/99): loss =162920268.34776956, w0=-0.1998618004838107, w1=0.19977143373367548\n",
      "Gradient Descent(45/99): loss =162867565.984264, w0=-0.19985904480278124, w1=0.1997645738840752\n",
      "Gradient Descent(46/99): loss =162814907.84284556, w0=-0.19985629494550497, w1=0.1997577103098414\n",
      "Gradient Descent(47/99): loss =162762290.23422515, w0=-0.19985355080716358, w1=0.19975084338746227\n",
      "Gradient Descent(48/99): loss =162709710.31595215, w0=-0.1998508122926844, w1=0.19974397344614436\n",
      "Gradient Descent(49/99): loss =162657165.89264244, w0=-0.19984807931552748, w1=0.1997371007737798\n",
      "Gradient Descent(50/99): loss =162604655.26340124, w0=-0.1998453517966292, w1=0.1997302256221601\n",
      "Gradient Descent(51/99): loss =162552177.1052995, w0=-0.19984262966348226, w1=0.19972334821153198\n",
      "Gradient Descent(52/99): loss =162499730.38437393, w0=-0.19983991284933406, w1=0.19971646873457774\n",
      "Gradient Descent(53/99): loss =162447314.28765133, w0=-0.19983720129248841, w1=0.19970958735989353\n",
      "Gradient Descent(54/99): loss =162394928.17123178, w0=-0.19983449493569708, w1=0.19970270423502845\n",
      "Gradient Descent(55/99): loss =162342571.5206312, w0=-0.19983179372562918, w1=0.19969581948913995\n",
      "Gradient Descent(56/99): loss =162290243.92049116, w0=-0.19982909761240877, w1=0.19968893323531417\n",
      "Gradient Descent(57/99): loss =162237945.03143832, w0=-0.1998264065492116, w1=0.19968204557259345\n",
      "Gradient Descent(58/99): loss =162185674.57240868, w0=-0.19982372049191324, w1=0.19967515658774776\n",
      "Gradient Descent(59/99): loss =162133432.3071422, w0=-0.1998210393987821, w1=0.1996682663568228\n",
      "Gradient Descent(60/99): loss =162081218.03386503, w0=-0.1998183632302115, w1=0.19966137494649255\n",
      "Gradient Descent(61/99): loss =162029031.57740298, w0=-0.19981569194848556, w1=0.19965448241524117\n",
      "Gradient Descent(62/99): loss =161976872.78315538, w0=-0.19981302551757493, w1=0.19964758881439582\n",
      "Gradient Descent(63/99): loss =161924741.51248643, w0=-0.199810363902958, w1=0.19964069418902905\n",
      "Gradient Descent(64/99): loss =161872637.63920522, w0=-0.19980770707146464, w1=0.1996337985787474\n",
      "Gradient Descent(65/99): loss =161820561.0468686, w0=-0.19980505499113954, w1=0.1996269020183805\n",
      "Gradient Descent(66/99): loss =161768511.62671897, w0=-0.19980240763112242, w1=0.199620004538583\n",
      "Gradient Descent(67/99): loss =161716489.2761064, w0=-0.1997997649615432, w1=0.19961310616636105\n",
      "Gradient Descent(68/99): loss =161664493.89727703, w0=-0.19979712695342994, w1=0.1996062069255318\n",
      "Gradient Descent(69/99): loss =161612525.3964471, w0=-0.1997944935786282, w1=0.19959930683712526\n",
      "Gradient Descent(70/99): loss =161560583.6830881, w0=-0.19979186480973005, w1=0.19959240591973548\n",
      "Gradient Descent(71/99): loss =161508668.66938066, w0=-0.1997892406200117, w1=0.1995855041898272\n",
      "Gradient Descent(72/99): loss =161456780.2697913, w0=-0.1997866209833786, w1=0.1995786016620041\n",
      "Gradient Descent(73/99): loss =161404918.40074697, w0=-0.19978400587431702, w1=0.19957169834924302\n",
      "Gradient Descent(74/99): loss =161353082.98038185, w0=-0.1997813952678513, w1=0.19956479426309864\n",
      "Gradient Descent(75/99): loss =161301273.92834187, w0=-0.19977878913950603, w1=0.1995578894138826\n",
      "Gradient Descent(76/99): loss =161249491.16563094, w0=-0.19977618746527262, w1=0.1995509838108197\n",
      "Gradient Descent(77/99): loss =161197734.61449063, w0=-0.19977359022157956, w1=0.1995440774621846\n",
      "Gradient Descent(78/99): loss =161146004.19830376, w0=-0.19977099738526607, w1=0.19953717037542126\n",
      "Gradient Descent(79/99): loss =161094299.84152043, w0=-0.19976840893355854, w1=0.19953026255724732\n",
      "Gradient Descent(80/99): loss =161042621.4695959, w0=-0.1997658248440495, w1=0.19952335401374527\n",
      "Gradient Descent(81/99): loss =160990969.00894076, w0=-0.19976324509467894, w1=0.19951644475044222\n",
      "Gradient Descent(82/99): loss =160939342.38687932, w0=-0.19976066966371728, w1=0.19950953477237957\n",
      "Gradient Descent(83/99): loss =160887741.5316159, w0=-0.19975809852975035, w1=0.19950262408417388\n",
      "Gradient Descent(84/99): loss =160836166.37220615, w0=-0.1997555316716656, w1=0.19949571269007013\n",
      "Gradient Descent(85/99): loss =160784616.83853108, w0=-0.19975296906863976, w1=0.19948880059398827\n",
      "Gradient Descent(86/99): loss =160733092.86127537, w0=-0.19975041070012758, w1=0.19948188779956377\n",
      "Gradient Descent(87/99): loss =160681594.37190774, w0=-0.19974785654585156, w1=0.19947497431018332\n",
      "Gradient Descent(88/99): loss =160630121.30266353, w0=-0.19974530658579262, w1=0.19946806012901577\n",
      "Gradient Descent(89/99): loss =160578673.58652842, w0=-0.19974276080018152, w1=0.1994611452590394\n",
      "Gradient Descent(90/99): loss =160527251.15722337, w0=-0.19974021916949097, w1=0.19945422970306562\n",
      "Gradient Descent(91/99): loss =160475853.94919068, w0=-0.1997376816744283, w1=0.1994473134637597\n",
      "Gradient Descent(92/99): loss =160424481.89758164, w0=-0.19973514829592875, w1=0.199440396543659\n",
      "Gradient Descent(93/99): loss =160373134.93824205, w0=-0.19973261901514922, w1=0.1994334789451887\n",
      "Gradient Descent(94/99): loss =160321813.00770316, w0=-0.19973009381346232, w1=0.19942656067067585\n",
      "Gradient Descent(95/99): loss =160270516.04316613, w0=-0.199727572672451, w1=0.1994196417223614\n",
      "Gradient Descent(96/99): loss =160219243.98249564, w0=-0.19972505557390338, w1=0.1994127221024108\n",
      "Gradient Descent(97/99): loss =160167996.76420468, w0=-0.1997225424998079, w1=0.1994058018129233\n",
      "Gradient Descent(98/99): loss =160116774.3274471, w0=-0.19972003343234868, w1=0.19939888085594015\n",
      "Gradient Descent(99/99): loss =160065576.61200625, w0=-0.19971752835390133, w1=0.19939195923345154\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =182137264730.05484, w0=-1.9528564876303278e-10, w1=0.19999999999995718\n",
      "Gradient Descent(2/99): loss =32240312.0100456, w0=-0.0021298607529316297, w1=0.19944044127245347\n",
      "Gradient Descent(3/99): loss =1340216.6754373366, w0=-0.0033936378385032656, w1=0.19941462617965566\n",
      "Gradient Descent(4/99): loss =1233459.5848002667, w0=-0.004744017958847298, w1=0.1993571416259307\n",
      "Gradient Descent(5/99): loss =1227312.8155982732, w0=-0.00606812655466462, w1=0.1992987691772559\n",
      "Gradient Descent(6/99): loss =1222152.5869920424, w0=-0.007399139493115272, w1=0.19923910778982318\n",
      "Gradient Descent(7/99): loss =1217252.4118557097, w0=-0.008716270425682458, w1=0.19917915719153984\n",
      "Gradient Descent(8/99): loss =1212499.8098753993, w0=-0.01003314931174854, w1=0.1991194821527859\n",
      "Gradient Descent(9/99): loss =1207842.8489900935, w0=-0.011340767403669289, w1=0.19906031546105246\n",
      "Gradient Descent(10/99): loss =1203253.2993176063, w0=-0.012645143920876905, w1=0.1990017596815329\n",
      "Gradient Descent(11/99): loss =1198714.843888528, w0=-0.013942400803266128, w1=0.19894380550828625\n",
      "Gradient Descent(12/99): loss =1194217.507276093, w0=-0.015235229506473361, w1=0.1988864167010875\n",
      "Gradient Descent(13/99): loss =1189754.986027118, w0=-0.01652199024559689, w1=0.19882952986801475\n",
      "Gradient Descent(14/99): loss =1185323.1276293697, w0=-0.017803905114810215, w1=0.19877308527171167\n",
      "Gradient Descent(15/99): loss =1180919.135236472, w0=-0.019080301022753873, w1=0.1987170222957833\n",
      "Gradient Descent(16/99): loss =1176541.0597907123, w0=-0.02035174792417187, w1=0.198661289841693\n",
      "Gradient Descent(17/99): loss =1172187.5154882432, w0=-0.021617986308740015, w1=0.19860584225632283\n",
      "Gradient Descent(18/99): loss =1167857.4843344688, w0=-0.02287929463062257, w1=0.1985506423938513\n",
      "Gradient Descent(19/99): loss =1163550.2010495167, w0=-0.024135587792968496, w1=0.1984956587957955\n",
      "Gradient Descent(20/99): loss =1159265.0709692517, w0=-0.025387012311797335, w1=0.19844086625944432\n",
      "Gradient Descent(21/99): loss =1155001.6198851222, w0=-0.026633554797851754, w1=0.1983862440446189\n",
      "Gradient Descent(22/99): loss =1150759.4575799713, w0=-0.027875300763474015, w1=0.19833177569142668\n",
      "Gradient Descent(23/99): loss =1146538.2548144714, w0=-0.02911226488741852, w1=0.19827744791233975\n",
      "Gradient Descent(24/99): loss =1142337.7263886025, w0=-0.03034450349262057, w1=0.19822325025719653\n",
      "Gradient Descent(25/99): loss =1138157.6200848483, w0=-0.031572041110900194, w1=0.19816917443203605\n",
      "Gradient Descent(26/99): loss =1133997.7084291207, w0=-0.0327949193633455, w1=0.1981152139952542\n",
      "Gradient Descent(27/99): loss =1129857.7831093248, w0=-0.034013165271489496, w1=0.19806136393645477\n",
      "Gradient Descent(28/99): loss =1125737.6507541123, w0=-0.035226812516521634, w1=0.1980076204442383\n",
      "Gradient Descent(29/99): loss =1121637.1299571602, w0=-0.03643588781473239, w1=0.19795398064336522\n",
      "Gradient Descent(30/99): loss =1117556.0489870342, w0=-0.03764042018411605, w1=0.1979004424296333\n",
      "Gradient Descent(31/99): loss =1113494.2441071956, w0=-0.038840435119098904, w1=0.19784700430426552\n",
      "Gradient Descent(32/99): loss =1109451.5582586683, w0=-0.04003595864844529, w1=0.19779366526073144\n",
      "Gradient Descent(33/99): loss =1105427.8400575782, w0=-0.04122701486740291, w1=0.19774042467955263\n",
      "Gradient Descent(34/99): loss =1101422.9429946057, w0=-0.0424136277302525, w1=0.19768728225233692\n",
      "Gradient Descent(35/99): loss =1097436.724806427, w0=-0.043595820021571884, w1=0.19763423791456824\n",
      "Gradient Descent(36/99): loss =1093469.0469652961, w0=-0.04477361416050522, w1=0.19758129179528577\n",
      "Gradient Descent(37/99): loss =1089519.774267984, w0=-0.04594703178477395, w1=0.19752844417399254\n",
      "Gradient Descent(38/99): loss =1085588.7744969851, w0=-0.047116094121232424, w1=0.19747569544763102\n",
      "Gradient Descent(39/99): loss =1081675.9181420568, w0=-0.04828082182787506, w1=0.19742304610292383\n",
      "Gradient Descent(40/99): loss =1077781.0781676273, w0=-0.04944123517200929, w1=0.19737049669486614\n",
      "Gradient Descent(41/99): loss =1073904.1298183782, w0=-0.050597353978760966, w1=0.19731804782899381\n",
      "Gradient Descent(42/99): loss =1070044.9504547985, w0=-0.051749197722331365, w1=0.19726570014748124\n",
      "Gradient Descent(43/99): loss =1066203.419413663, w0=-0.052896785516840454, w1=0.19721345431781748\n",
      "Gradient Descent(44/99): loss =1062379.4178885499, w0=-0.054040136167117225, w1=0.19716131102389778\n",
      "Gradient Descent(45/99): loss =1058572.8288270296, w0=-0.05517926817509024, w1=0.1971092709588427\n",
      "Gradient Descent(46/99): loss =1054783.5368414856, w0=-0.05631419977084383, w1=0.19705733481935356\n",
      "Gradient Descent(47/99): loss =1051011.4281313103, w0=-0.057444948923650895, w1=0.1970055033012106\n",
      "Gradient Descent(48/99): loss =1047256.3904144977, w0=-0.058571533362756445, w1=0.19695377709575299\n",
      "Gradient Descent(49/99): loss =1043518.3128671263, w0=-0.059693970588828325, w1=0.1969021568871072\n",
      "Gradient Descent(50/99): loss =1039797.0860694186, w0=-0.060812277888939614, w1=0.1968506433500437\n",
      "Gradient Descent(51/99): loss =1036092.6019573401, w0=-0.06192647234697125, w1=0.19679923714832023\n",
      "Gradient Descent(52/99): loss =1032404.7537788743, w0=-0.06303657085501796, w1=0.19674793893342596\n",
      "Gradient Descent(53/99): loss =1028733.4360542377, w0=-0.06414259012237736, w1=0.19669674934363948\n",
      "Gradient Descent(54/99): loss =1025078.5445394714, w0=-0.06524454668456514, w1=0.19664566900334196\n",
      "Gradient Descent(55/99): loss =1021439.9761928982, w0=-0.06634245691093506, w1=0.1965946985225305\n",
      "Gradient Descent(56/99): loss =1017817.6291440293, w0=-0.0674363370119886, w1=0.19654383849649265\n",
      "Gradient Descent(57/99): loss =1014211.4026646097, w0=-0.06852620304579968, w1=0.19649308950560718\n",
      "Gradient Descent(58/99): loss =1010621.197141487, w0=-0.06961207092404868, w1=0.19644245211524547\n",
      "Gradient Descent(59/99): loss =1007046.9140510899, w0=-0.07069395641744636, w1=0.1963919268757513\n",
      "Gradient Descent(60/99): loss =1003488.4559352975, w0=-0.07177187516078243, w1=0.19634151432248229\n",
      "Gradient Descent(61/99): loss =999945.7263785591, w0=-0.07284584265752564, w1=0.1962912149758989\n",
      "Gradient Descent(62/99): loss =996418.6299860941, w0=-0.07391587428409416, w1=0.1962410293416901\n",
      "Gradient Descent(63/99): loss =992907.0723630801, w0=-0.07498198529378126, w1=0.19619095791092683\n",
      "Gradient Descent(64/99): loss =989410.9600947106, w0=-0.07604419082040191, w1=0.19614100116023647\n",
      "Gradient Descent(65/99): loss =985930.2007270537, w0=-0.07710250588166646, w1=0.19609115955199236\n",
      "Gradient Descent(66/99): loss =982464.7027486219, w0=-0.07815694538232139, w1=0.19604143353451442\n",
      "Gradient Descent(67/99): loss =979014.3755726074, w0=-0.07920752411707009, w1=0.19599182354227707\n",
      "Gradient Descent(68/99): loss =975579.1295197234, w0=-0.08025425677330003, w1=0.1959423299961218\n",
      "Gradient Descent(69/99): loss =972158.8758016009, w0=-0.08129715793363022, w1=0.19589295330347245\n",
      "Gradient Descent(70/99): loss =968753.5265047124, w0=-0.08233624207829783, w1=0.19584369385855124\n",
      "Gradient Descent(71/99): loss =965362.9945747786, w0=-0.08337152358739665, w1=0.19579455204259452\n",
      "Gradient Descent(72/99): loss =961987.1938016334, w0=-0.08440301674298145, w1=0.19574552822406716\n",
      "Gradient Descent(73/99): loss =958626.0388045209, w0=-0.08543073573104937, w1=0.1956966227588748\n",
      "Gradient Descent(74/99): loss =955279.4450177934, w0=-0.08645469464340913, w1=0.19564783599057373\n",
      "Gradient Descent(75/99): loss =951947.3286770055, w0=-0.08747490747944742, w1=0.1955991682505774\n",
      "Gradient Descent(76/99): loss =948629.6068053675, w0=-0.08849138814780114, w1=0.19555061985836011\n",
      "Gradient Descent(77/99): loss =945326.1972005498, w0=-0.08950415046794315, w1=0.19550219112165682\n",
      "Gradient Descent(78/99): loss =942037.0184218328, w0=-0.09051320817168865, w1=0.19545388233665972\n",
      "Gradient Descent(79/99): loss =938761.989777566, w0=-0.09151857490462836, w1=0.19540569378821107\n",
      "Gradient Descent(80/99): loss =935501.0313129444, w0=-0.09252026422749454, w1=0.19535762574999244\n",
      "Gradient Descent(81/99): loss =932254.0637980814, w0=-0.09351828961746475, w1=0.1953096784847103\n",
      "Gradient Descent(82/99): loss =929021.0087163668, w0=-0.09451266446940837, w1=0.1952618522442781\n",
      "Gradient Descent(83/99): loss =925801.7882531037, w0=-0.0955034020970801, w1=0.195214147269995\n",
      "Gradient Descent(84/99): loss =922596.3252844098, w0=-0.0964905157342643, w1=0.19516656379272085\n",
      "Gradient Descent(85/99): loss =919404.5433663796, w0=-0.09747401853587392, w1=0.19511910203304808\n",
      "Gradient Descent(86/99): loss =916226.3667245008, w0=-0.09845392357900715, w1=0.19507176220147038\n",
      "Gradient Descent(87/99): loss =913061.7202433047, w0=-0.09943024386396472, w1=0.19502454449854795\n",
      "Gradient Descent(88/99): loss =909910.5294562582, w0=-0.10040299231523094, w1=0.19497744911506995\n",
      "Gradient Descent(89/99): loss =906772.7205358843, w0=-0.1013721817824204, w1=0.19493047623221382\n",
      "Gradient Descent(90/99): loss =903648.2202840996, w0=-0.10233782504119313, w1=0.19488362602170173\n",
      "Gradient Descent(91/99): loss =900536.9561227706, w0=-0.10329993479413994, w1=0.1948368986459542\n",
      "Gradient Descent(92/99): loss =897438.8560844769, w0=-0.10425852367164021, w1=0.194790294258241\n",
      "Gradient Descent(93/99): loss =894353.8488034818, w0=-0.1052136042326934, w1=0.19474381300282947\n",
      "Gradient Descent(94/99): loss =891281.8635068919, w0=-0.10616518896572642, w1=0.19469745501513022\n",
      "Gradient Descent(95/99): loss =888222.8300060176, w0=-0.10711329028937801, w1=0.19465122042184022\n",
      "Gradient Descent(96/99): loss =885176.6786879168, w0=-0.1080579205532615, w1=0.1946051093410836\n",
      "Gradient Descent(97/99): loss =882143.3405071169, w0=-0.10899909203870738, w1=0.19455912188255012\n",
      "Gradient Descent(98/99): loss =879122.7469775217, w0=-0.10993681695948666, w1=0.1945132581476312\n",
      "Gradient Descent(99/99): loss =876114.8301644825, w0=-0.11087110746251624, w1=0.19446751822955388\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =6.546610466392608e+17, w0=-0.1999999999999993, w1=0.2\n",
      "Gradient Descent(2/99): loss =323223181.8660235, w0=-0.19998845914181135, w1=0.20001789005600984\n",
      "Gradient Descent(3/99): loss =305551030.6544995, w0=-0.19997933718594738, w1=0.20003271603936312\n",
      "Gradient Descent(4/99): loss =295796066.35700315, w0=-0.19997067345996392, w1=0.20004599457491828\n",
      "Gradient Descent(5/99): loss =287554370.4484728, w0=-0.19996247546948467, w1=0.20005827337589147\n",
      "Gradient Descent(6/99): loss =280276569.8651219, w0=-0.1999546121665413, w1=0.20006982334239254\n",
      "Gradient Descent(7/99): loss =273785535.55413526, w0=-0.19994703733305424, w1=0.20008080061246467\n",
      "Gradient Descent(8/99): loss =267958791.389645, w0=-0.1999397116865505, w1=0.20009130195024247\n",
      "Gradient Descent(9/99): loss =262699557.7791511, w0=-0.1999326054396364, w1=0.20010139115748798\n",
      "Gradient Descent(10/99): loss =257929495.8492312, w0=-0.19992569370414393, w1=0.20011111231138734\n",
      "Gradient Descent(11/99): loss =253584421.73083693, w0=-0.19991895549778282, w1=0.20012049722478617\n",
      "Gradient Descent(12/99): loss =249611195.63193092, w0=-0.19991237290343386, w1=0.20012956983728908\n",
      "Gradient Descent(13/99): loss =245965387.64418867, w0=-0.1999059305372461, w1=0.20013834889297433\n",
      "Gradient Descent(14/99): loss =242609505.55941492, w0=-0.19989961513523002, w1=0.20014684960884563\n",
      "Gradient Descent(15/99): loss =239511637.734143, w0=-0.19989341522642584, w1=0.20015508473494176\n",
      "Gradient Descent(16/99): loss =236644405.3602546, w0=-0.19988732086568722, w1=0.20016306524113248\n",
      "Gradient Descent(17/99): loss =233984147.04137474, w0=-0.19988132341193493, w1=0.20017080077165122\n",
      "Gradient Descent(18/99): loss =231510278.8940455, w0=-0.19987541534205158, w1=0.2001782999530618\n",
      "Gradient Descent(19/99): loss =229204788.08274254, w0=-0.1998695900935072, w1=0.2001855706082761\n",
      "Gradient Descent(20/99): loss =227051828.41936138, w0=-0.19986384193055662, w1=0.20019261990925535\n",
      "Gradient Descent(21/99): loss =225037394.53603947, w0=-0.1998581658300362, w1=0.20019945448886603\n",
      "Gradient Descent(22/99): loss =223149056.95914134, w0=-0.19985255738361507, w1=0.20020608052490693\n",
      "Gradient Descent(23/99): loss =221375744.72837016, w0=-0.19984701271395822, w1=0.20021250380471817\n",
      "Gradient Descent(24/99): loss =219707565.41848662, w0=-0.19984152840271233, w1=0.20021872977590904\n",
      "Gradient Descent(25/99): loss =218135654.82207116, w0=-0.19983610142857558, w1=0.2002247635869265\n",
      "Gradient Descent(26/99): loss =216652050.35188916, w0=-0.1998307291139922, w1=0.2002306101200235\n",
      "Gradient Descent(27/99): loss =215249583.57589862, w0=-0.19982540907923735, w1=0.20023627401842856\n",
      "Gradient Descent(28/99): loss =213921788.32090992, w0=-0.19982013920284442, w1=0.20024175970901387\n",
      "Gradient Descent(29/99): loss =212662821.55668408, w0=-0.19981491758748066, w1=0.20024707142141682\n",
      "Gradient Descent(30/99): loss =211467394.86312446, w0=-0.19980974253050746, w1=0.20025221320433262\n",
      "Gradient Descent(31/99): loss =210330714.73542324, w0=-0.19980461249857104, w1=0.20025718893952527\n",
      "Gradient Descent(32/99): loss =209248430.3299205, w0=-0.19979952610566235, w1=0.20026200235398217\n",
      "Gradient Descent(33/99): loss =208216587.52261275, w0=-0.19979448209416484, w1=0.20026665703054558\n",
      "Gradient Descent(34/99): loss =207231588.36179245, w0=-0.19978947931847624, w1=0.20027115641728524\n",
      "Gradient Descent(35/99): loss =206290155.16054165, w0=-0.19978451673084863, w1=0.2002755038358236\n",
      "Gradient Descent(36/99): loss =205389298.60440987, w0=-0.19977959336914092, w1=0.20027970248878352\n",
      "Gradient Descent(37/99): loss =204526289.35269096, w0=-0.1997747083462206, w1=0.20028375546649607\n",
      "Gradient Descent(38/99): loss =203698632.69431153, w0=-0.1997698608407879, w1=0.20028766575308057\n",
      "Gradient Descent(39/99): loss =202904045.8861141, w0=-0.19976505008942774, w1=0.20029143623198792\n",
      "Gradient Descent(40/99): loss =202140437.85564926, w0=-0.19976027537972083, w1=0.20029506969108246\n",
      "Gradient Descent(41/99): loss =201405890.99530032, w0=-0.19975553604426968, w1=0.20029856882732397\n",
      "Gradient Descent(42/99): loss =200698644.8115151, w0=-0.1997508314555145, w1=0.20030193625110085\n",
      "Gradient Descent(43/99): loss =200017081.22380838, w0=-0.1997461610212314, w1=0.20030517449025687\n",
      "Gradient Descent(44/99): loss =199359711.33418375, w0=-0.19974152418062038, w1=0.20030828599384684\n",
      "Gradient Descent(45/99): loss =198725163.50961536, w0=-0.19973692040090282, w1=0.20031127313565075\n",
      "Gradient Descent(46/99): loss =198112172.63903242, w0=-0.19973234917436003, w1=0.20031413821747124\n",
      "Gradient Descent(47/99): loss =197519570.44237366, w0=-0.19972781001575263, w1=0.2003168834722357\n",
      "Gradient Descent(48/99): loss =196946276.72321758, w0=-0.19972330246007014, w1=0.20031951106692092\n",
      "Gradient Descent(49/99): loss =196391291.4685814, w0=-0.19971882606056593, w1=0.20032202310531558\n",
      "Gradient Descent(50/99): loss =195853687.7100755, w0=-0.19971438038703945, w1=0.20032442163063396\n",
      "Gradient Descent(51/99): loss =195332605.06981716, w0=-0.19970996502433275, w1=0.2003267086279926\n",
      "Gradient Descent(52/99): loss =194827243.9226811, w0=-0.1997055795710126, w1=0.20032888602675972\n",
      "Gradient Descent(53/99): loss =194336860.11363012, w0=-0.19970122363821372, w1=0.20033095570278658\n",
      "Gradient Descent(54/99): loss =193860760.17525807, w0=-0.19969689684862163, w1=0.20033291948052856\n",
      "Gradient Descent(55/99): loss =193398296.99629417, w0=-0.19969259883557677, w1=0.20033477913506295\n",
      "Gradient Descent(56/99): loss =192948865.89687803, w0=-0.19968832924228402, w1=0.20033653639401008\n",
      "Gradient Descent(57/99): loss =192511901.0708717, w0=-0.19968408772111382, w1=0.20033819293936322\n",
      "Gradient Descent(58/99): loss =192086872.35948408, w0=-0.19967987393298287, w1=0.20033975040923271\n",
      "Gradient Descent(59/99): loss =191673282.3240579, w0=-0.19967568754680418, w1=0.20034121039950906\n",
      "Gradient Descent(60/99): loss =191270663.58908853, w0=-0.19967152823899761, w1=0.2003425744654493\n",
      "Gradient Descent(61/99): loss =190878576.42938253, w0=-0.1996673956930531, w1=0.200343844123191\n",
      "Gradient Descent(62/99): loss =190496606.57788575, w0=-0.19966328959913973, w1=0.20034502085119726\n",
      "Gradient Descent(63/99): loss =190124363.23298404, w0=-0.1996592096537551, w1=0.2003461060916367\n",
      "Gradient Descent(64/99): loss =189761477.246189, w0=-0.19965515555940994, w1=0.20034710125170135\n",
      "Gradient Descent(65/99): loss =189407599.47297862, w0=-0.19965112702434326, w1=0.20034800770486588\n",
      "Gradient Descent(66/99): loss =189062399.27124518, w0=-0.1996471237622648, w1=0.20034882679209068\n",
      "Gradient Descent(67/99): loss =188725563.1333275, w0=-0.19964314549212092, w1=0.2003495598229719\n",
      "Gradient Descent(68/99): loss =188396793.43896034, w0=-0.19963919193788163, w1=0.2003502080768408\n",
      "Gradient Descent(69/99): loss =188075807.31771338, w0=-0.19963526282834565, w1=0.20035077280381502\n",
      "Gradient Descent(70/99): loss =187762335.61058575, w0=-0.19963135789696204, w1=0.20035125522580383\n",
      "Gradient Descent(71/99): loss =187456121.9214339, w0=-0.19962747688166593, w1=0.20035165653746986\n",
      "Gradient Descent(72/99): loss =187156921.7498185, w0=-0.19962361952472707, w1=0.20035197790714918\n",
      "Gradient Descent(73/99): loss =186864501.69763687, w0=-0.19961978557260954, w1=0.20035222047773163\n",
      "Gradient Descent(74/99): loss =186578638.74268582, w0=-0.19961597477584161, w1=0.20035238536750358\n",
      "Gradient Descent(75/99): loss =186299119.5729186, w0=-0.19961218688889448, w1=0.20035247367095455\n",
      "Gradient Descent(76/99): loss =186025739.97578815, w0=-0.199608421670069, w1=0.20035248645954956\n",
      "Gradient Descent(77/99): loss =185758304.2775937, w0=-0.1996046788813895, w1=0.20035242478246884\n",
      "Gradient Descent(78/99): loss =185496624.8282288, w0=-0.19960095828850416, w1=0.2003522896673163\n",
      "Gradient Descent(79/99): loss =185240521.52718675, w0=-0.1995972596605911, w1=0.2003520821207983\n",
      "Gradient Descent(80/99): loss =184989821.38705713, w0=-0.19959358277026967, w1=0.20035180312937415\n",
      "Gradient Descent(81/99): loss =184744358.13111466, w0=-0.19958992739351644, w1=0.2003514536598794\n",
      "Gradient Descent(82/99): loss =184503971.8219265, w0=-0.19958629330958563, w1=0.20035103466012372\n",
      "Gradient Descent(83/99): loss =184268508.51819378, w0=-0.19958268030093326, w1=0.2003505470594637\n",
      "Gradient Descent(84/99): loss =184037819.9572949, w0=-0.19957908815314498, w1=0.20034999176935273\n",
      "Gradient Descent(85/99): loss =183811763.2612713, w0=-0.19957551665486717, w1=0.20034936968386816\n",
      "Gradient Descent(86/99): loss =183590200.6641681, w0=-0.19957196559774093, w1=0.2003486816802173\n",
      "Gradient Descent(87/99): loss =183372999.25886598, w0=-0.19956843477633895, w1=0.20034792861922304\n",
      "Gradient Descent(88/99): loss =183160030.76171166, w0=-0.19956492398810483, w1=0.20034711134579\n",
      "Gradient Descent(89/99): loss =182951171.29340446, w0=-0.19956143303329482, w1=0.20034623068935228\n",
      "Gradient Descent(90/99): loss =182746301.17474964, w0=-0.1995579617149218, w1=0.2003452874643034\n",
      "Gradient Descent(91/99): loss =182545304.736012, w0=-0.19955450983870107, w1=0.20034428247040956\n",
      "Gradient Descent(92/99): loss =182348070.13872695, w0=-0.19955107721299833, w1=0.20034321649320652\n",
      "Gradient Descent(93/99): loss =182154489.20893157, w0=-0.19954766364877924, w1=0.20034209030438144\n",
      "Gradient Descent(94/99): loss =181964457.28086373, w0=-0.19954426895956068, w1=0.20034090466213975\n",
      "Gradient Descent(95/99): loss =181777873.05028412, w0=-0.19954089296136363, w1=0.2003396603115583\n",
      "Gradient Descent(96/99): loss =181594638.43663377, w0=-0.1995375354726675, w1=0.20033835798492494\n",
      "Gradient Descent(97/99): loss =181414658.45332977, w0=-0.19953419631436578, w1=0.2003369984020654\n",
      "Gradient Descent(98/99): loss =181237841.0855427, w0=-0.1995308753097231, w1=0.20033558227065795\n",
      "Gradient Descent(99/99): loss =181064097.17488772, w0=-0.19952757228433346, w1=0.20033411028653658\n",
      "Optimizing degree 7/15, model: least_squares_GD, arguments: {'max_iters': 100}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =1.019857322971128e+22, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =93360102562.0948, w0=-0.19999885222233812, w1=0.20000185323813338\n",
      "Gradient Descent(3/99): loss =91355791987.88911, w0=-0.1999977984031826, w1=0.20000361270248757\n",
      "Gradient Descent(4/99): loss =89804992155.23299, w0=-0.19999676645979805, w1=0.20000530203331357\n",
      "Gradient Descent(5/99): loss =88361349988.175, w0=-0.19999575141411458, w1=0.20000693637089517\n",
      "Gradient Descent(6/99): loss =87010802563.65512, w0=-0.1999947510228117, w1=0.2000085256296162\n",
      "Gradient Descent(7/99): loss =85744976525.81752, w0=-0.1999937639881973, w1=0.20001007666312262\n",
      "Gradient Descent(8/99): loss =84556744110.50728, w0=-0.19999278933250278, w1=0.20001159434199592\n",
      "Gradient Descent(9/99): loss =83439841255.03786, w0=-0.19999182624244818, w1=0.20001308220435246\n",
      "Gradient Descent(10/99): loss =82388708917.08768, w0=-0.19999087401622168, w1=0.20001454287779616\n",
      "Gradient Descent(11/99): loss =81398376819.87625, w0=-0.19998993203756565, w1=0.20001597836067322\n",
      "Gradient Descent(12/99): loss =80464372657.30217, w0=-0.19998899975933168, w1=0.20001739021174822\n",
      "Gradient Descent(13/99): loss =79582649963.63255, w0=-0.19998807669166402, w1=0.20001877967903894\n",
      "Gradient Descent(14/99): loss =78749530092.95581, w0=-0.1999871623930284, w1=0.2000201477878888\n",
      "Gradient Descent(15/99): loss =77961655022.90543, w0=-0.19998625646318988, w1=0.20002149540159567\n",
      "Gradient Descent(16/99): loss =77215948554.96796, w0=-0.19998535853759222, w1=0.2000228232634858\n",
      "Gradient Descent(17/99): loss =76509584090.09787, w0=-0.1999844682827752, w1=0.20002413202638478\n",
      "Gradient Descent(18/99): loss =75839957598.22499, w0=-0.19998358539257716, w1=0.20002542227348238\n",
      "Gradient Descent(19/99): loss =75204664724.89029, w0=-0.19998270958494535, w1=0.2000266945332841\n",
      "Gradient Descent(20/99): loss =74601481220.79652, w0=-0.19998184059922536, w1=0.2000279492904687\n",
      "Gradient Descent(21/99): loss =74028346062.89381, w0=-0.19998097819383578, w1=0.20002918699388797\n",
      "Gradient Descent(22/99): loss =73483346774.37227, w0=-0.1999801221442577, w1=0.2000304080625513\n",
      "Gradient Descent(23/99): loss =72964706556.83333, w0=-0.1999792722412854, w1=0.20003161289017415\n",
      "Gradient Descent(24/99): loss =72470772929.13745, w0=-0.1999784282894965, w1=0.2000328018486909\n",
      "Gradient Descent(25/99): loss =72000007630.0192, w0=-0.19997759010590851, w1=0.20003397529101052\n",
      "Gradient Descent(26/99): loss =71550977590.01414, w0=-0.19997675751879498, w1=0.2000351335532121\n",
      "Gradient Descent(27/99): loss =71122346815.92357, w0=-0.1999759303666393, w1=0.20003627695631943\n",
      "Gradient Descent(28/99): loss =70712869060.48871, w0=-0.19997510849720795, w1=0.20003740580775542\n",
      "Gradient Descent(29/99): loss =70321381173.06274, w0=-0.19997429176672796, w1=0.20003852040254924\n",
      "Gradient Descent(30/99): loss =69946797045.32227, w0=-0.199973480039155, w1=0.2000396210243504\n",
      "Gradient Descent(31/99): loss =69588102080.54243, w0=-0.19997267318552162, w1=0.20004070794629014\n",
      "Gradient Descent(32/99): loss =69244348126.52061, w0=-0.19997187108335537, w1=0.20004178143172038\n",
      "Gradient Descent(33/99): loss =68914648821.52318, w0=-0.19997107361615873, w1=0.200042841734854\n",
      "Gradient Descent(34/99): loss =68598175310.13416, w0=-0.19997028067294337, w1=0.20004388910132453\n",
      "Gradient Descent(35/99): loss =68294152291.9921, w0=-0.19996949214781265, w1=0.20004492376867936\n",
      "Gradient Descent(36/99): loss =68001854371.41259, w0=-0.19996870793958638, w1=0.200045945966818\n",
      "Gradient Descent(37/99): loss =67720602680.01707, w0=-0.19996792795146331, w1=0.20004695591838445\n",
      "Gradient Descent(38/99): loss =67449761747.93368, w0=-0.19996715209071692, w1=0.20004795383912075\n",
      "Gradient Descent(39/99): loss =67188736602.00444, w0=-0.19996638026842062, w1=0.20004893993818784\n",
      "Gradient Descent(40/99): loss =66936970071.86884, w0=-0.19996561239919924, w1=0.20004991441845857\n",
      "Gradient Descent(41/99): loss =66693940286.85365, w0=-0.1999648484010037, w1=0.20005087747678676\n",
      "Gradient Descent(42/99): loss =66459158348.36832, w0=-0.19996408819490644, w1=0.20005182930425572\n",
      "Gradient Descent(43/99): loss =66232166164.03078, w0=-0.1999633317049152, w1=0.2000527700864088\n",
      "Gradient Descent(44/99): loss =66012534431.06971, w0=-0.19996257885780325, w1=0.20005370000346437\n",
      "Gradient Descent(45/99): loss =65799860757.70782, w0=-0.19996182958295408, w1=0.20005461923051718\n",
      "Gradient Descent(46/99): loss =65593767912.2448, w0=-0.19996108381221941, w1=0.20005552793772746\n",
      "Gradient Descent(47/99): loss =65393902190.45888, w0=-0.19996034147978864, w1=0.2000564262904994\n",
      "Gradient Descent(48/99): loss =65199931892.74336, w0=-0.1999596025220688, w1=0.20005731444965\n",
      "Gradient Descent(49/99): loss =65011545903.10709, w0=-0.1999588668775739, w1=0.20005819257156915\n",
      "Gradient Descent(50/99): loss =64828452362.806175, w0=-0.19995813448682265, w1=0.200059060808372\n",
      "Gradient Descent(51/99): loss =64650377431.95152, w0=-0.19995740529224376, w1=0.20005991930804415\n",
      "Gradient Descent(52/99): loss =64477064132.95442, w0=-0.199956679238088, w1=0.20006076821458033\n",
      "Gradient Descent(53/99): loss =64308271270.14581, w0=-0.19995595627034635, w1=0.20006160766811695\n",
      "Gradient Descent(54/99): loss =64143772420.330734, w0=-0.19995523633667378, w1=0.2000624378050594\n",
      "Gradient Descent(55/99): loss =63983354989.43299, w0=-0.199954519386318, w1=0.20006325875820377\n",
      "Gradient Descent(56/99): loss =63826819330.74004, w0=-0.19995380537005275, w1=0.20006407065685414\n",
      "Gradient Descent(57/99): loss =63673977920.585365, w0=-0.19995309424011515, w1=0.2000648736269351\n",
      "Gradient Descent(58/99): loss =63524654587.60595, w0=-0.19995238595014694, w1=0.20006566779110008\n",
      "Gradient Descent(59/99): loss =63378683791.988, w0=-0.1999516804551391, w1=0.20006645326883582\n",
      "Gradient Descent(60/99): loss =63235909951.367485, w0=-0.1999509777113795, w1=0.20006723017656283\n",
      "Gradient Descent(61/99): loss =63096186810.285934, w0=-0.19995027767640358, w1=0.20006799862773242\n",
      "Gradient Descent(62/99): loss =62959376850.31905, w0=-0.19994958030894752, w1=0.20006875873292018\n",
      "Gradient Descent(63/99): loss =62825350738.19298, w0=-0.19994888556890394, w1=0.20006951059991632\n",
      "Gradient Descent(64/99): loss =62693986809.38987, w0=-0.19994819341727982, w1=0.20007025433381279\n",
      "Gradient Descent(65/99): loss =62565170584.91365, w0=-0.1999475038161564, w1=0.20007099003708748\n",
      "Gradient Descent(66/99): loss =62438794319.04702, w0=-0.1999468167286511, w1=0.2000717178096856\n",
      "Gradient Descent(67/99): loss =62314756576.07417, w0=-0.19994613211888124, w1=0.2000724377490982\n",
      "Gradient Descent(68/99): loss =62192961834.084915, w0=-0.19994544995192917, w1=0.20007314995043812\n",
      "Gradient Descent(69/99): loss =62073320114.098335, w0=-0.19994477019380935, w1=0.20007385450651355\n",
      "Gradient Descent(70/99): loss =61955746632.862946, w0=-0.19994409281143644, w1=0.20007455150789888\n",
      "Gradient Descent(71/99): loss =61840161477.8011, w0=-0.19994341777259508, w1=0.20007524104300342\n",
      "Gradient Descent(72/99): loss =61726489302.66487, w0=-0.19994274504591078, w1=0.2000759231981378\n",
      "Gradient Descent(73/99): loss =61614659042.56723, w0=-0.19994207460082195, w1=0.20007659805757824\n",
      "Gradient Descent(74/99): loss =61504603647.13968, w0=-0.19994140640755315, w1=0.2000772657036286\n",
      "Gradient Descent(75/99): loss =61396259830.64818, w0=-0.1999407404370894, w1=0.20007792621668058\n",
      "Gradient Descent(76/99): loss =61289567837.97907, w0=-0.19994007666115146, w1=0.20007857967527176\n",
      "Gradient Descent(77/99): loss =61184471225.47572, w0=-0.19993941505217203, w1=0.200079226156142\n",
      "Gradient Descent(78/99): loss =61080916655.671646, w0=-0.19993875558327281, w1=0.20007986573428774\n",
      "Gradient Descent(79/99): loss =60978853705.033195, w0=-0.1999380982282426, w1=0.20008049848301487\n",
      "Gradient Descent(80/99): loss =60878234683.87517, w0=-0.199937442961516, w1=0.2000811244739896\n",
      "Gradient Descent(81/99): loss =60779014467.67515, w0=-0.19993678975815293, w1=0.2000817437772878\n",
      "Gradient Descent(82/99): loss =60681150339.055336, w0=-0.19993613859381903, w1=0.20008235646144282\n",
      "Gradient Descent(83/99): loss =60584601839.75207, w0=-0.19993548944476655, w1=0.20008296259349173\n",
      "Gradient Descent(84/99): loss =60489330631.93682, w0=-0.19993484228781605, w1=0.20008356223901996\n",
      "Gradient Descent(85/99): loss =60395300368.29098, w0=-0.1999341971003387, w1=0.20008415546220476\n",
      "Gradient Descent(86/99): loss =60302476570.279, w0=-0.19993355386023917, w1=0.20008474232585702\n",
      "Gradient Descent(87/99): loss =60210826514.096535, w0=-0.19993291254593923, w1=0.20008532289146186\n",
      "Gradient Descent(88/99): loss =60120319123.805695, w0=-0.19993227313636164, w1=0.20008589721921796\n",
      "Gradient Descent(89/99): loss =60030924871.2013, w0=-0.19993163561091493, w1=0.2000864653680756\n",
      "Gradient Descent(90/99): loss =59942615681.98016, w0=-0.19993099994947844, w1=0.20008702739577341\n",
      "Gradient Descent(91/99): loss =59855364847.81269, w0=-0.19993036613238796, w1=0.2000875833588741\n",
      "Gradient Descent(92/99): loss =59769146943.943, w0=-0.1999297341404219, w1=0.20008813331279895\n",
      "Gradient Descent(93/99): loss =59683937751.96597, w0=-0.19992910395478772, w1=0.2000886773118612\n",
      "Gradient Descent(94/99): loss =59599714187.45442, w0=-0.1999284755571091, w1=0.20008921540929836\n",
      "Gradient Descent(95/99): loss =59516454232.12709, w0=-0.19992784892941332, w1=0.20008974765730364\n",
      "Gradient Descent(96/99): loss =59434136870.271484, w0=-0.19992722405411906, w1=0.20009027410705624\n",
      "Gradient Descent(97/99): loss =59352742029.151, w0=-0.19992660091402478, w1=0.20009079480875067\n",
      "Gradient Descent(98/99): loss =59272250523.14566, w0=-0.19992597949229723, w1=0.20009130981162526\n",
      "Gradient Descent(99/99): loss =59192644001.38695, w0=-0.19992535977246056, w1=0.20009181916398963\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.0198573064673355e+22, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =57151790738.386986, w0=-0.19999941632163615, w1=0.2000003061359386\n",
      "Gradient Descent(3/99): loss =54871569244.03355, w0=-0.19999896486806676, w1=0.20000028022527766\n",
      "Gradient Descent(4/99): loss =53676426025.70018, w0=-0.19999853636401516, w1=0.20000011745190147\n",
      "Gradient Descent(5/99): loss =52783226242.14217, w0=-0.19999812737918127, w1=0.19999985131005407\n",
      "Gradient Descent(6/99): loss =52114992107.29742, w0=-0.19999773473015595, w1=0.19999950209380202\n",
      "Gradient Descent(7/99): loss =51614643313.14163, w0=-0.1999973560895413, w1=0.1999990837704521\n",
      "Gradient Descent(8/99): loss =51239647368.89974, w0=-0.19999698949898415, w1=0.1999986068710969\n",
      "Gradient Descent(9/99): loss =50958280840.876205, w0=-0.19999663330571424, w1=0.1999980798214504\n",
      "Gradient Descent(10/99): loss =50746867142.71024, w0=-0.19999628609734862, w1=0.19999750960834073\n",
      "Gradient Descent(11/99): loss =50587727236.992584, w0=-0.1999959466611661, w1=0.19999690214153557\n",
      "Gradient Descent(12/99): loss =50467655889.61529, w0=-0.19999561395313034, w1=0.19999626247294963\n",
      "Gradient Descent(13/99): loss =50376787039.88323, w0=-0.19999528707328956, w1=0.19999559494507133\n",
      "Gradient Descent(14/99): loss =50307748033.987274, w0=-0.19999496524542038, w1=0.1999949033012516\n",
      "Gradient Descent(15/99): loss =50255028694.01288, w0=-0.19999464779983586, w1=0.19999419077312383\n",
      "Gradient Descent(16/99): loss =50214510380.959816, w0=-0.19999433415870382, w1=0.19999346015268726\n",
      "Gradient Descent(17/99): loss =50183114328.84996, w0=-0.19999402382343756, w1=0.1999927138530819\n",
      "Gradient Descent(18/99): loss =50158538958.5178, w0=-0.19999371636383606, w1=0.19999195396044556\n",
      "Gradient Descent(19/99): loss =50139063609.92471, w0=-0.19999341140871815, w1=0.19999118227844134\n",
      "Gradient Descent(20/99): loss =50123401873.061966, w0=-0.19999310863783984, w1=0.19999040036661683\n",
      "Gradient Descent(21/99): loss =50110591968.62489, w0=-0.19999280777491704, w1=0.19998960957350603\n",
      "Gradient Descent(22/99): loss =50099914811.08688, w0=-0.19999250858160217, w1=0.1999888110652203\n",
      "Gradient Descent(23/99): loss =50090832758.8087, w0=-0.19999221085228408, w1=0.1999880058501543\n",
      "Gradient Descent(24/99): loss =50082943825.595474, w0=-0.19999191440959885, w1=0.19998719480034083\n",
      "Gradient Descent(25/99): loss =50075947449.24942, w0=-0.19999161910055466, w1=0.19998637866991112\n",
      "Gradient Descent(26/99): loss =50069618899.29262, w0=-0.19999132479318688, w1=0.19998555811105456\n",
      "Gradient Descent(27/99): loss =50063790143.06951, w0=-0.19999103137367086, w1=0.1999847336878172\n",
      "Gradient Descent(28/99): loss =50058335540.13684, w0=-0.19999073874383028, w1=0.19998390588803175\n",
      "Gradient Descent(29/99): loss =50053161146.40359, w0=-0.19999044681898667, w1=0.19998307513363267\n",
      "Gradient Descent(30/99): loss =50048196717.07302, w0=-0.19999015552610377, w1=0.19998224178957472\n",
      "Gradient Descent(31/99): loss =50043389727.364136, w0=-0.19998986480218622, w1=0.1999814061715443\n",
      "Gradient Descent(32/99): loss =50038700901.86224, w0=-0.19998957459289793, w1=0.19998056855262697\n",
      "Gradient Descent(33/99): loss =50034100871.834785, w0=-0.19998928485137, w1=0.1999797291690724\n",
      "Gradient Descent(34/99): loss =50029567675.90844, w0=-0.1999889955371723, w1=0.1999788882252791\n",
      "Gradient Descent(35/99): loss =50025084891.31813, w0=-0.19998870661542636, w1=0.19997804589810442\n",
      "Gradient Descent(36/99): loss =50020640236.62965, w0=-0.1999884180560399, w1=0.19997720234059105\n",
      "Gradient Descent(37/99): loss =50016224526.98239, w0=-0.19998812983304687, w1=0.19997635768518932\n",
      "Gradient Descent(38/99): loss =50011830892.91144, w0=-0.19998784192403807, w1=0.1999755120465432\n",
      "Gradient Descent(39/99): loss =50007454196.24861, w0=-0.1999875543096701, w1=0.19997466552389934\n",
      "Gradient Descent(40/99): loss =50003090593.38157, w0=-0.19998726697324196, w1=0.19997381820318985\n",
      "Gradient Descent(41/99): loss =49998737208.692696, w0=-0.19998697990032988, w1=0.19997297015883345\n",
      "Gradient Descent(42/99): loss =49994391890.3821, w0=-0.19998669307847255, w1=0.19997212145529258\n",
      "Gradient Descent(43/99): loss =49990053027.88937, w0=-0.19998640649689967, w1=0.19997127214841995\n",
      "Gradient Descent(44/99): loss =49985719415.37447, w0=-0.19998612014629796, w1=0.19997042228662268\n",
      "Gradient Descent(45/99): loss =49981390149.63772, w0=-0.19998583401860942, w1=0.19996957191186895\n",
      "Gradient Descent(46/99): loss =49977064553.79044, w0=-0.1999855481068575, w1=0.1999687210605585\n",
      "Gradient Descent(47/99): loss =49972742120.181046, w0=-0.19998526240499728, w1=0.19996786976427505\n",
      "Gradient Descent(48/99): loss =49968422467.71717, w0=-0.19998497690778622, w1=0.19996701805043723\n",
      "Gradient Descent(49/99): loss =49964105309.954216, w0=-0.19998469161067298, w1=0.19996616594286112\n",
      "Gradient Descent(50/99): loss =49959790431.23321, w0=-0.1999844065097015, w1=0.19996531346224677\n",
      "Gradient Descent(51/99): loss =49955477668.83707, w0=-0.19998412160142848, w1=0.19996446062659895\n",
      "Gradient Descent(52/99): loss =49951166899.64857, w0=-0.19998383688285207, w1=0.19996360745159084\n",
      "Gradient Descent(53/99): loss =49946858030.172844, w0=-0.19998355235135065, w1=0.1999627539508785\n",
      "Gradient Descent(54/99): loss =49942550989.07637, w0=-0.1999832680046301, w1=0.199961900136373\n",
      "Gradient Descent(55/99): loss =49938245721.608406, w0=-0.1999829838406782, w1=0.19996104601847534\n",
      "Gradient Descent(56/99): loss =49933942185.4286, w0=-0.1999826998577256, w1=0.19996019160627995\n",
      "Gradient Descent(57/99): loss =49929640347.487175, w0=-0.19998241605421202, w1=0.1999593369077505\n",
      "Gradient Descent(58/99): loss =49925340181.69172, w0=-0.19998213242875731, w1=0.1999584819298719\n",
      "Gradient Descent(59/99): loss =49921041667.162796, w0=-0.1999818489801364, w1=0.19995762667878175\n",
      "Gradient Descent(60/99): loss =49916744786.92848, w0=-0.19998156570725786, w1=0.1999567711598841\n",
      "Gradient Descent(61/99): loss =49912449526.9489, w0=-0.1999812826091453, w1=0.1999559153779477\n",
      "Gradient Descent(62/99): loss =49908155875.386154, w0=-0.19998099968492158, w1=0.19995505933719104\n",
      "Gradient Descent(63/99): loss =49903863822.05857, w0=-0.19998071693379502, w1=0.1999542030413558\n",
      "Gradient Descent(64/99): loss =49899573358.03213, w0=-0.1999804343550476, w1=0.19995334649377056\n",
      "Gradient Descent(65/99): loss =49895284475.31521, w0=-0.19998015194802488, w1=0.19995248969740564\n",
      "Gradient Descent(66/99): loss =49890997166.62987, w0=-0.19997986971212725, w1=0.1999516326549208\n",
      "Gradient Descent(67/99): loss =49886711425.241066, w0=-0.1999795876468024, w1=0.1999507753687062\n",
      "Gradient Descent(68/99): loss =49882427244.82836, w0=-0.19997930575153897, w1=0.1999499178409181\n",
      "Gradient Descent(69/99): loss =49878144619.39054, w0=-0.19997902402586082, w1=0.19994906007350957\n",
      "Gradient Descent(70/99): loss =49873863543.17365, w0=-0.19997874246932246, w1=0.19994820206825714\n",
      "Gradient Descent(71/99): loss =49869584010.61722, w0=-0.19997846108150483, w1=0.19994734382678378\n",
      "Gradient Descent(72/99): loss =49865306016.31419, w0=-0.19997817986201177, w1=0.19994648535057885\n",
      "Gradient Descent(73/99): loss =49861029554.98055, w0=-0.1999778988104671, w1=0.1999456266410153\n",
      "Gradient Descent(74/99): loss =49856754621.432724, w0=-0.19997761792651186, w1=0.19994476769936467\n",
      "Gradient Descent(75/99): loss =49852481210.57069, w0=-0.1999773372098022, w1=0.19994390852680985\n",
      "Gradient Descent(76/99): loss =49848209317.364845, w0=-0.1999770566600074, w1=0.19994304912445635\n",
      "Gradient Descent(77/99): loss =49843938936.84672, w0=-0.19997677627680818, w1=0.19994218949334192\n",
      "Gradient Descent(78/99): loss =49839670064.10139, w0=-0.19997649605989537, w1=0.19994132963444494\n",
      "Gradient Descent(79/99): loss =49835402694.2621, w0=-0.1999762160089686, w1=0.19994046954869168\n",
      "Gradient Descent(80/99): loss =49831136822.505974, w0=-0.19997593612373524, w1=0.19993960923696255\n",
      "Gradient Descent(81/99): loss =49826872444.05072, w0=-0.19997565640390963, w1=0.19993874870009753\n",
      "Gradient Descent(82/99): loss =49822609554.152435, w0=-0.19997537684921213, w1=0.1999378879389009\n",
      "Gradient Descent(83/99): loss =49818348148.10329, w0=-0.19997509745936853, w1=0.19993702695414534\n",
      "Gradient Descent(84/99): loss =49814088221.23039, w0=-0.19997481823410954, w1=0.19993616574657538\n",
      "Gradient Descent(85/99): loss =49809829768.894295, w0=-0.19997453917317012, w1=0.19993530431691056\n",
      "Gradient Descent(86/99): loss =49805572786.48802, w0=-0.19997426027628926, w1=0.19993444266584806\n",
      "Gradient Descent(87/99): loss =49801317269.436584, w0=-0.19997398154320942, w1=0.19993358079406504\n",
      "Gradient Descent(88/99): loss =49797063213.19599, w0=-0.1999737029736763, w1=0.1999327187022206\n",
      "Gradient Descent(89/99): loss =49792810613.25295, w0=-0.19997342456743863, w1=0.19993185639095756\n",
      "Gradient Descent(90/99): loss =49788559465.12425, w0=-0.19997314632424779, w1=0.199930993860904\n",
      "Gradient Descent(91/99): loss =49784309764.35625, w0=-0.19997286824385765, w1=0.19993013111267444\n",
      "Gradient Descent(92/99): loss =49780061506.52462, w0=-0.19997259032602452, w1=0.19992926814687123\n",
      "Gradient Descent(93/99): loss =49775814687.23409, w0=-0.1999723125705068, w1=0.19992840496408537\n",
      "Gradient Descent(94/99): loss =49771569302.11791, w0=-0.19997203497706495, w1=0.19992754156489745\n",
      "Gradient Descent(95/99): loss =49767325346.83761, w0=-0.19997175754546137, w1=0.19992667794987837\n",
      "Gradient Descent(96/99): loss =49763082817.08289, w0=-0.19997148027546027, w1=0.1999258141195901\n",
      "Gradient Descent(97/99): loss =49758841708.57095, w0=-0.19997120316682757, w1=0.1999249500745862\n",
      "Gradient Descent(98/99): loss =49754602017.046936, w0=-0.19997092621933085, w1=0.19992408581541232\n",
      "Gradient Descent(99/99): loss =49750363738.28278, w0=-0.1999706494327393, w1=0.19992322134260676\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =170814851043214.44, w0=-4.1407155482175995e-13, w1=0.19999999999999982\n",
      "Gradient Descent(2/99): loss =32253024.816693608, w0=-0.00010172923112525903, w1=0.19996341573766038\n",
      "Gradient Descent(3/99): loss =1637616.7523399023, w0=-0.00017453615585475516, w1=0.1999576324323178\n",
      "Gradient Descent(4/99): loss =1628726.3611318467, w0=-0.0002491079190002321, w1=0.19995210134515476\n",
      "Gradient Descent(5/99): loss =1626324.9008727327, w0=-0.0003227081169557867, w1=0.1999466506542475\n",
      "Gradient Descent(6/99): loss =1624635.5174678694, w0=-0.0003966569609932074, w1=0.1999412540913058\n",
      "Gradient Descent(7/99): loss =1623156.405998511, w0=-0.00047029483076245385, w1=0.19993590143818954\n",
      "Gradient Descent(8/99): loss =1621768.9337270036, w0=-0.0005439600020442834, w1=0.19993058977512193\n",
      "Gradient Descent(9/99): loss =1620431.9068246288, w0=-0.0006174792374044598, w1=0.1999253138891234\n",
      "Gradient Descent(10/99): loss =1619127.009389426, w0=-0.0006909401858543095, w1=0.19992006938924817\n",
      "Gradient Descent(11/99): loss =1617844.4011740906, w0=-0.0007642987228651139, w1=0.19991485141342932\n",
      "Gradient Descent(12/99): loss =1616578.092425167, w0=-0.0008375786801707056, w1=0.19990965571469688\n",
      "Gradient Descent(13/99): loss =1615324.171857155, w0=-0.0009107698454815219, w1=0.19990447836698366\n",
      "Gradient Descent(14/99): loss =1614079.9836410019, w0=-0.000983879661930836, w1=0.19989931601911792\n",
      "Gradient Descent(15/99): loss =1612843.6818252702, w0=-0.0010569067446320867, w1=0.19989416577126073\n",
      "Gradient Descent(16/99): loss =1611613.9610521276, w0=-0.001129854262316344, w1=0.1998890251901486\n",
      "Gradient Descent(17/99): loss =1610389.8843131894, w0=-0.0012027230602670838, w1=0.1998838922291906\n",
      "Gradient Descent(18/99): loss =1609170.7687509018, w0=-0.0012755151232026168, w1=0.19987876518950565\n",
      "Gradient Descent(19/99): loss =1607956.1086999336, w0=-0.0013482317898067515, w1=0.19987364266185534\n",
      "Gradient Descent(20/99): loss =1606745.5231502152, w0=-0.0014208746496103344, w1=0.19986852348461479\n",
      "Gradient Descent(21/99): loss =1605538.7196364414, w0=-0.0014934450806158377, w1=0.1998634067017525\n",
      "Gradient Descent(22/99): loss =1604335.4692338568, w0=-0.0015659444829219445, w1=0.19985829152934165\n",
      "Gradient Descent(23/99): loss =1603135.589146168, w0=-0.0016383741604168748, w1=0.1998531773258433\n",
      "Gradient Descent(24/99): loss =1601938.930495431, w0=-0.0017107353830740455, w1=0.19984806356780255\n",
      "Gradient Descent(25/99): loss =1600745.3696994674, w0=-0.001783029358109166, w1=0.19984294982921352\n",
      "Gradient Descent(26/99): loss =1599554.802330586, w0=-0.0018552572479534459, w1=0.19983783576453457\n",
      "Gradient Descent(27/99): loss =1598367.1387001371, w0=-0.0019274201645679782, w1=0.19983272109452188\n",
      "Gradient Descent(28/99): loss =1597182.3006489798, w0=-0.0019995191758295245, w1=0.19982760559455653\n",
      "Gradient Descent(29/99): loss =1596000.2191860229, w0=-0.0020715553057092076, w1=0.1998224890849807\n",
      "Gradient Descent(30/99): loss =1594820.8327272877, w0=-0.0021435295374983747, w1=0.1998173714231359\n",
      "Gradient Descent(31/99): loss =1593644.0857638288, w0=-0.002215442815343007, w1=0.1998122524967923\n",
      "Gradient Descent(32/99): loss =1592469.9278387746, w0=-0.0022872960464861646, w1=0.19980713221873675\n",
      "Gradient Descent(33/99): loss =1591298.312749668, w0=-0.0023590901029931013, w1=0.19980201052231247\n",
      "Gradient Descent(34/99): loss =1590129.1979170055, w0=-0.0024308258235843706, w1=0.19979688735774587\n",
      "Gradient Descent(35/99): loss =1588962.5438769876, w0=-0.002502504015263265, w1=0.19979176268912108\n",
      "Gradient Descent(36/99): loss =1587798.3138684134, w0=-0.0025741254549036326, w1=0.199786636491889\n",
      "Gradient Descent(37/99): loss =1586636.4734920235, w0=-0.0026456908907207637, w1=0.19978150875081616\n",
      "Gradient Descent(38/99): loss =1585476.990426326, w0=-0.002717201043672223, w1=0.19977637945829635\n",
      "Gradient Descent(39/99): loss =1584319.8341882154, w0=-0.0027886566087722605, w1=0.19977124861296086\n",
      "Gradient Descent(40/99): loss =1583164.9759295152, w0=-0.002860058256335418, w1=0.19976611621853468\n",
      "Gradient Descent(41/99): loss =1582012.388262742, w0=-0.002931406633148367, w1=0.1997609822828956\n",
      "Gradient Descent(42/99): loss =1580862.0451109323, w0=-0.003002702363577076, w1=0.19975584681730005\n",
      "Gradient Descent(43/99): loss =1579713.9215774678, w0=-0.0030739460506119306, w1=0.1997507098357467\n",
      "Gradient Descent(44/99): loss =1578567.9938326888, w0=-0.003145138276855323, w1=0.19974557135445303\n",
      "Gradient Descent(45/99): loss =1577424.2390147196, w0=-0.0032162796054548857, w1=0.19974043139142547\n",
      "Gradient Descent(46/99): loss =1576282.6351424085, w0=-0.0032873705809858813, w1=0.19973528996610612\n",
      "Gradient Descent(47/99): loss =1575143.16103864, w0=-0.0033584117302857463, w1=0.19973014709908282\n",
      "Gradient Descent(48/99): loss =1574005.7962625897, w0=-0.003429403563243741, w1=0.19972500281185127\n",
      "Gradient Descent(49/99): loss =1572870.521049732, w0=-0.003500346573548377, w1=0.19971985712661988\n",
      "Gradient Descent(50/99): loss =1571737.3162585534, w0=-0.0035712412393951723, w1=0.19971471006615002\n",
      "Gradient Descent(51/99): loss =1570606.1633231295, w0=-0.003642088024157084, w1=0.19970956165362508\n",
      "Gradient Descent(52/99): loss =1569477.0442108428, w0=-0.003712887377019835, w1=0.19970441191254346\n",
      "Gradient Descent(53/99): loss =1568349.941384546, w0=-0.003783639733584201, w1=0.1996992608666311\n",
      "Gradient Descent(54/99): loss =1567224.8377686986, w0=-0.0038543455164371894, w1=0.19969410853977013\n",
      "Gradient Descent(55/99): loss =1566101.7167189564, w0=-0.003925005135693924, w1=0.19968895495594066\n",
      "Gradient Descent(56/99): loss =1564980.5619948064, w0=-0.003995618989511928, w1=0.1996838001391735\n",
      "Gradient Descent(57/99): loss =1563861.3577349205, w0=-0.004066187464579404, w1=0.1996786441135116\n",
      "Gradient Descent(58/99): loss =1562744.0884348678, w0=-0.004136710936578982, w1=0.19967348690297898\n",
      "Gradient Descent(59/99): loss =1561628.738926964, w0=-0.00420718977062835, w1=0.19966832853155542\n",
      "Gradient Descent(60/99): loss =1560515.2943619862, w0=-0.004277624321699073, w1=0.19966316902315615\n",
      "Gradient Descent(61/99): loss =1559403.7401925446, w0=-0.004348014935014812, w1=0.1996580084016155\n",
      "Gradient Descent(62/99): loss =1558294.0621579548, w0=-0.004418361946430129, w1=0.1996528466906737\n",
      "Gradient Descent(63/99): loss =1557186.2462704186, w0=-0.004488665682790924, w1=0.19964768391396656\n",
      "Gradient Descent(64/99): loss =1556080.2788023874, w0=-0.0045589264622775535, w1=0.199642520095017\n",
      "Gradient Descent(65/99): loss =1554976.146274984, w0=-0.004629144594731562, w1=0.19963735525722864\n",
      "Gradient Descent(66/99): loss =1553873.8354473547, w0=-0.004699320381966938, w1=0.19963218942388053\n",
      "Gradient Descent(67/99): loss =1552773.3333068828, w0=-0.004769454118066737, w1=0.1996270226181233\n",
      "Gradient Descent(68/99): loss =1551674.627060151, w0=-0.004839546089665857, w1=0.19962185486297604\n",
      "Gradient Descent(69/99): loss =1550577.7041245936, w0=-0.004909596576220728, w1=0.199616686181324\n",
      "Gradient Descent(70/99): loss =1549482.5521207463, w0=-0.004979605850266601, w1=0.19961151659591694\n",
      "Gradient Descent(71/99): loss =1548389.1588650765, w0=-0.00504957417766311, w1=0.1996063461293679\n",
      "Gradient Descent(72/99): loss =1547297.5123632904, w0=-0.005119501817828729, w1=0.19960117480415235\n",
      "Gradient Descent(73/99): loss =1546207.6008041054, w0=-0.005189389023964697, w1=0.19959600264260777\n",
      "Gradient Descent(74/99): loss =1545119.4125534245, w0=-0.005259236043268988, w1=0.19959082966693326\n",
      "Gradient Descent(75/99): loss =1544032.9361488905, w0=-0.005329043117140815, w1=0.19958565589918956\n",
      "Gradient Descent(76/99): loss =1542948.1602947735, w0=-0.005398810481376187, w1=0.19958048136129908\n",
      "Gradient Descent(77/99): loss =1541865.0738571715, w0=-0.005468538366354966, w1=0.1995753060750461\n",
      "Gradient Descent(78/99): loss =1540783.6658594701, w0=-0.005538226997219859, w1=0.19957013006207708\n",
      "Gradient Descent(79/99): loss =1539703.9254781015, w0=-0.005607876594047766, w1=0.19956495334390098\n",
      "Gradient Descent(80/99): loss =1538625.842038488, w0=-0.005677487372013863, w1=0.1995597759418897\n",
      "Gradient Descent(81/99): loss =1537549.40501124, w0=-0.005747059541548786, w1=0.1995545978772785\n",
      "Gradient Descent(82/99): loss =1536474.604008523, w0=-0.005816593308489277, w1=0.1995494191711665\n",
      "Gradient Descent(83/99): loss =1535401.4287806295, w0=-0.005886088874222588, w1=0.19954423984451716\n",
      "Gradient Descent(84/99): loss =1534329.8692126947, w0=-0.005955546435824988, w1=0.1995390599181588\n",
      "Gradient Descent(85/99): loss =1533259.9153215892, w0=-0.006024966186194633, w1=0.19953387941278505\n",
      "Gradient Descent(86/99): loss =1532191.55725293, w0=-0.006094348314179092, w1=0.19952869834895556\n",
      "Gradient Descent(87/99): loss =1531124.785278249, w0=-0.006163693004697782, w1=0.19952351674709634\n",
      "Gradient Descent(88/99): loss =1530059.5897922604, w0=-0.006233000438859561, w1=0.19951833462750038\n",
      "Gradient Descent(89/99): loss =1528995.9613102558, w0=-0.006302270794075714, w1=0.19951315201032818\n",
      "Gradient Descent(90/99): loss =1527933.8904655976, w0=-0.006371504244168536, w1=0.19950796891560832\n",
      "Gradient Descent(91/99): loss =1526873.3680073232, w0=-0.0064407009594757494, w1=0.19950278536323787\n",
      "Gradient Descent(92/99): loss =1525814.3847978157, w0=-0.006509861106950921, w1=0.19949760137298306\n",
      "Gradient Descent(93/99): loss =1524756.931810593, w0=-0.006578984850260089, w1=0.19949241696447972\n",
      "Gradient Descent(94/99): loss =1523701.0001281512, w0=-0.006648072349874767, w1=0.1994872321572339\n",
      "Gradient Descent(95/99): loss =1522646.5809398943, w0=-0.006717123763161482, w1=0.19948204697062227\n",
      "Gradient Descent(96/99): loss =1521593.665540138, w0=-0.0067861392444680336, w1=0.19947686142389276\n",
      "Gradient Descent(97/99): loss =1520542.24532617, w0=-0.0068551189452065895, w1=0.19947167553616504\n",
      "Gradient Descent(98/99): loss =1519492.3117963874, w0=-0.00692406301393379, w1=0.19946648932643105\n",
      "Gradient Descent(99/99): loss =1518443.856548477, w0=-0.006992971596427972, w1=0.1994613028135555\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.0198573229953786e+22, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =91468850203.81013, w0=-0.19999890413959018, w1=0.20000198995674348\n",
      "Gradient Descent(3/99): loss =88964072918.77414, w0=-0.1999979612103311, w1=0.2000037995623203\n",
      "Gradient Descent(4/99): loss =87324490543.84505, w0=-0.1999970352367913, w1=0.20000550916862642\n",
      "Gradient Descent(5/99): loss =85850000394.20337, w0=-0.19999614058525925, w1=0.20000715179951684\n",
      "Gradient Descent(6/99): loss =84488222658.09839, w0=-0.19999526480904922, w1=0.2000087416135085\n",
      "Gradient Descent(7/99): loss =83223627533.16039, w0=-0.19999440791373269, w1=0.20001028807155105\n",
      "Gradient Descent(8/99): loss =82045205537.41422, w0=-0.19999356716934893, w1=0.2000117972303407\n",
      "Gradient Descent(9/99): loss =80943949292.73349, w0=-0.19999274123151797, w1=0.20001327341677716\n",
      "Gradient Descent(10/99): loss =79912276709.07896, w0=-0.19999192865708226, w1=0.20001471971592716\n",
      "Gradient Descent(11/99): loss =78943729073.99887, w0=-0.19999112828803708, w1=0.2000161384141808\n",
      "Gradient Descent(12/99): loss =78032747484.35217, w0=-0.1999903390841765, w1=0.20001753124169522\n",
      "Gradient Descent(13/99): loss =77174502688.6756, w0=-0.19998956014151054, w1=0.20001889954815402\n",
      "Gradient Descent(14/99): loss =76364762756.46696, w0=-0.19998879066044342, w1=0.20002024441620703\n",
      "Gradient Descent(15/99): loss =75599789354.9211, w0=-0.19998802993479672, w1=0.20002156673962618\n",
      "Gradient Descent(16/99): loss =74876255774.88216, w0=-0.19998727733811594, w1=0.20002286727608712\n",
      "Gradient Descent(17/99): loss =74191181689.92546, w0=-0.19998653231368685, w1=0.20002414668354254\n",
      "Gradient Descent(18/99): loss =73541880873.35664, w0=-0.19998579436562725, w1=0.20002540554537535\n",
      "Gradient Descent(19/99): loss =72925919011.62648, w0=-0.19998506305142025, w1=0.20002664438802117\n",
      "Gradient Descent(20/99): loss =72341079425.0165, w0=-0.19998433797544035, w1=0.20002786369344908\n",
      "Gradient Descent(21/99): loss =71785335010.9191, w0=-0.19998361878336038, w1=0.20002906390812397\n",
      "Gradient Descent(22/99): loss =71256825106.62704, w0=-0.19998290515728015, w1=0.2000302454495362\n",
      "Gradient Descent(23/99): loss =70753836259.5639, w0=-0.19998219681147353, w1=0.20003140871103403\n",
      "Gradient Descent(24/99): loss =70274786116.09245, w0=-0.19998149348866393, w1=0.20003255406545917\n",
      "Gradient Descent(25/99): loss =69818209812.00455, w0=-0.1999807949567558, w1=0.20003368186792816\n",
      "Gradient Descent(26/99): loss =69382748380.78471, w0=-0.19998010100596184, w1=0.20003479245799619\n",
      "Gradient Descent(27/99): loss =68967138798.88307, w0=-0.19997941144627454, w1=0.20003588616136836\n",
      "Gradient Descent(28/99): loss =68570205367.43859, w0=-0.19997872610523898, w1=0.20003696329127488\n",
      "Gradient Descent(29/99): loss =68190852192.41137, w0=-0.1999780448259891, w1=0.20003802414959265\n",
      "Gradient Descent(30/99): loss =67828056573.89569, w0=-0.19997736746551534, w1=0.2000390690277735\n",
      "Gradient Descent(31/99): loss =67480863153.6127, w0=-0.1999766938931355, w1=0.20004009820762222\n",
      "Gradient Descent(32/99): loss =67148378699.545715, w0=-0.1999760239891444, w1=0.2000411119619574\n",
      "Gradient Descent(33/99): loss =66829767430.242615, w0=-0.1999753576436209, w1=0.20004211055517848\n",
      "Gradient Descent(34/99): loss =66524246799.87401, w0=-0.19997469475537333, w1=0.2000430942437584\n",
      "Gradient Descent(35/99): loss =66231083679.80221, w0=-0.19997403523100726, w1=0.20004406327667534\n",
      "Gradient Descent(36/99): loss =65949590884.02691, w0=-0.19997337898410078, w1=0.20004501789579493\n",
      "Gradient Descent(37/99): loss =65679123995.11475, w0=-0.1999727259344747, w1=0.20004595833621142\n",
      "Gradient Descent(38/99): loss =65419078454.577194, w0=-0.19997207600754657, w1=0.20004688482655503\n",
      "Gradient Descent(39/99): loss =65168886887.55812, w0=-0.19997142913375843, w1=0.20004779758927052\n",
      "Gradient Descent(40/99): loss =64928016636.43807, w0=-0.19997078524806974, w1=0.20004869684087187\n",
      "Gradient Descent(41/99): loss =64695967481.785965, w0=-0.19997014428950807, w1=0.20004958279217636\n",
      "Gradient Descent(42/99): loss =64472269532.206665, w0=-0.19996950620077014, w1=0.2000504556485211\n",
      "Gradient Descent(43/99): loss =64256481267.17266, w0=-0.1999688709278681, w1=0.20005131560996461\n",
      "Gradient Descent(44/99): loss =64048187719.01665, w0=-0.1999682384198152, w1=0.20005216287147504\n",
      "Gradient Descent(45/99): loss =63846998781.991905, w0=-0.1999676086283464, w1=0.20005299762310705\n",
      "Gradient Descent(46/99): loss =63652547637.74449, w0=-0.19996698150767, w1=0.20005382005016853\n",
      "Gradient Descent(47/99): loss =63464489287.753746, w0=-0.19996635701424648, w1=0.2000546303333783\n",
      "Gradient Descent(48/99): loss =63282499184.31118, w0=-0.19996573510659155, w1=0.2000554286490159\n",
      "Gradient Descent(49/99): loss =63106271952.48388, w0=-0.19996511574510042, w1=0.20005621516906405\n",
      "Gradient Descent(50/99): loss =62935520196.24827, w0=-0.1999644988918911, w1=0.20005699006134464\n",
      "Gradient Descent(51/99): loss =62769973382.626465, w0=-0.19996388451066427, w1=0.20005775348964874\n",
      "Gradient Descent(52/99): loss =62609376798.21786, w0=-0.19996327256657795, w1=0.2000585056138612\n",
      "Gradient Descent(53/99): loss =62453490573.00766, w0=-0.19996266302613536, w1=0.20005924659008015\n",
      "Gradient Descent(54/99): loss =62302088766.7664, w0=-0.19996205585708415, w1=0.20005997657073188\n",
      "Gradient Descent(55/99): loss =62154958513.74225, w0=-0.19996145102832613, w1=0.20006069570468138\n",
      "Gradient Descent(56/99): loss =62011899221.684494, w0=-0.19996084850983598, w1=0.2000614041373387\n",
      "Gradient Descent(57/99): loss =61872721821.54758, w0=-0.199960248272588, w1=0.20006210201076163\n",
      "Gradient Descent(58/99): loss =61737248064.50242, w0=-0.19995965028849014, w1=0.2000627894637547\n",
      "Gradient Descent(59/99): loss =61605309863.127975, w0=-0.1999590545303243, w1=0.20006346663196467\n",
      "Gradient Descent(60/99): loss =61476748673.88763, w0=-0.19995846097169234, w1=0.20006413364797296\n",
      "Gradient Descent(61/99): loss =61351414918.20135, w0=-0.199957869586967, w1=0.2000647906413848\n",
      "Gradient Descent(62/99): loss =61229167439.613014, w0=-0.19995728035124755, w1=0.2000654377389155\n",
      "Gradient Descent(63/99): loss =61109872994.72798, w0=-0.19995669324031903, w1=0.20006607506447394\n",
      "Gradient Descent(64/99): loss =60993405775.753586, w0=-0.1999561082306155, w1=0.2000667027392433\n",
      "Gradient Descent(65/99): loss =60879646962.62826, w0=-0.19995552529918606, w1=0.20006732088175916\n",
      "Gradient Descent(66/99): loss =60768484302.85026, w0=-0.199954944423664, w1=0.20006792960798522\n",
      "Gradient Descent(67/99): loss =60659811717.257225, w0=-0.19995436558223814, w1=0.20006852903138644\n",
      "Gradient Descent(68/99): loss =60553528930.11066, w0=-0.1999537887536269, w1=0.20006911926300008\n",
      "Gradient Descent(69/99): loss =60449541121.95613, w0=-0.19995321391705398, w1=0.20006970041150435\n",
      "Gradient Descent(70/99): loss =60347758603.82784, w0=-0.19995264105222602, w1=0.20007027258328497\n",
      "Gradient Descent(71/99): loss =60248096511.45662, w0=-0.199952070139312, w1=0.2000708358824997\n",
      "Gradient Descent(72/99): loss =60150474518.23264, w0=-0.19995150115892388, w1=0.2000713904111407\n",
      "Gradient Descent(73/99): loss =60054816565.749725, w0=-0.19995093409209883, w1=0.20007193626909525\n",
      "Gradient Descent(74/99): loss =59961050610.83623, w0=-0.19995036892028245, w1=0.20007247355420432\n",
      "Gradient Descent(75/99): loss =59869108388.0464, w0=-0.1999498056253132, w1=0.20007300236231945\n",
      "Gradient Descent(76/99): loss =59778925186.65241, w0=-0.1999492441894077, w1=0.20007352278735793\n",
      "Gradient Descent(77/99): loss =59690439641.2356, w0=-0.19994868459514714, w1=0.20007403492135617\n",
      "Gradient Descent(78/99): loss =59603593535.03868, w0=-0.19994812682546417, w1=0.2000745388545216\n",
      "Gradient Descent(79/99): loss =59518331615.28414, w0=-0.19994757086363082, w1=0.2000750346752827\n",
      "Gradient Descent(80/99): loss =59434601419.72568, w0=-0.199947016693247, w1=0.20007552247033797\n",
      "Gradient Descent(81/99): loss =59352353113.73248, w0=-0.19994646429822954, w1=0.2000760023247029\n",
      "Gradient Descent(82/99): loss =59271539337.26443, w0=-0.1999459136628019, w1=0.20007647432175588\n",
      "Gradient Descent(83/99): loss =59192115061.12318, w0=-0.19994536477148436, w1=0.20007693854328248\n",
      "Gradient Descent(84/99): loss =59114037451.91198, w0=-0.19994481760908475, w1=0.2000773950695186\n",
      "Gradient Descent(85/99): loss =59037265745.1676, w0=-0.19994427216068944, w1=0.20007784397919212\n",
      "Gradient Descent(86/99): loss =58961761126.16222, w0=-0.199943728411655, w1=0.20007828534956346\n",
      "Gradient Descent(87/99): loss =58887486617.906944, w0=-0.19994318634760005, w1=0.20007871925646473\n",
      "Gradient Descent(88/99): loss =58814406975.911415, w0=-0.19994264595439756, w1=0.2000791457743379\n",
      "Gradient Descent(89/99): loss =58742488589.29002, w0=-0.19994210721816738, w1=0.20007956497627175\n",
      "Gradient Descent(90/99): loss =58671699387.82283, w0=-0.1999415701252692, w1=0.2000799769340375\n",
      "Gradient Descent(91/99): loss =58602008754.6087, w0=-0.19994103466229574, w1=0.2000803817181238\n",
      "Gradient Descent(92/99): loss =58533387443.96848, w0=-0.19994050081606615, w1=0.20008077939777028\n",
      "Gradient Descent(93/99): loss =58465807504.27712, w0=-0.19993996857361984, w1=0.2000811700410002\n",
      "Gradient Descent(94/99): loss =58399242205.42368, w0=-0.19993943792221033, w1=0.2000815537146524\n",
      "Gradient Descent(95/99): loss =58333665970.617966, w0=-0.19993890884929946, w1=0.20008193048441172\n",
      "Gradient Descent(96/99): loss =58269054312.2767, w0=-0.19993838134255182, w1=0.2000823004148392\n",
      "Gradient Descent(97/99): loss =58205383771.74313, w0=-0.19993785538982925, w1=0.2000826635694008\n",
      "Gradient Descent(98/99): loss =58142631862.60457, w0=-0.19993733097918578, w1=0.20008302001049555\n",
      "Gradient Descent(99/99): loss =58080777017.3894, w0=-0.19993680809886247, w1=0.2000833697994829\n",
      "Optimizing degree 8/15, model: least_squares_GD, arguments: {'max_iters': 100}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =1.618844536878688e+26, w0=-0.2, w1=0.20000000000000004\n",
      "Gradient Descent(2/99): loss =27233707702688.242, w0=-0.19999987513231865, w1=0.20000022378324256\n",
      "Gradient Descent(3/99): loss =26920690650366.984, w0=-0.19999975650537707, w1=0.20000044258378064\n",
      "Gradient Descent(4/99): loss =26655043807829.617, w0=-0.19999963913472657, w1=0.2000006569184361\n",
      "Gradient Descent(5/99): loss =26400644646536.66, w0=-0.1999995227629203, w1=0.20000086757873206\n",
      "Gradient Descent(6/99): loss =26156490662037.324, w0=-0.19999940727921928, w1=0.20000107510243498\n",
      "Gradient Descent(7/99): loss =25921978866406.516, w0=-0.19999929262338728, w1=0.20000127988247687\n",
      "Gradient Descent(8/99): loss =25696579704490.74, w0=-0.19999917874928533, w1=0.20000148221123462\n",
      "Gradient Descent(9/99): loss =25479816038234.23, w0=-0.19999906561782346, w1=0.2000016823090335\n",
      "Gradient Descent(10/99): loss =25271253864428.58, w0=-0.19999895319480762, w1=0.20000188034398178\n",
      "Gradient Descent(11/99): loss =25070495357329.13, w0=-0.19999884144988786, w1=0.20000207644614587\n",
      "Gradient Descent(12/99): loss =24877173421674.953, w0=-0.19999873035585874, w1=0.2000022707177598\n",
      "Gradient Descent(13/99): loss =24690947370588.184, w0=-0.1999986198881376, w1=0.20000246324060297\n",
      "Gradient Descent(14/99): loss =24511499459439.645, w0=-0.19999851002436012, w1=0.20000265408134485\n",
      "Gradient Descent(15/99): loss =24338532079452.0, w0=-0.19999840074406042, w1=0.2000028432954264\n",
      "Gradient Descent(16/99): loss =24171765464886.55, w0=-0.19999829202841432, w1=0.2000030309298866\n",
      "Gradient Descent(17/99): loss =24010935803295.152, w0=-0.19999818386003024, w1=0.20000321702542723\n",
      "Gradient Descent(18/99): loss =23855793664295.473, w0=-0.19999807622277657, w1=0.20000340161792635\n",
      "Gradient Descent(19/99): loss =23706102681616.938, w0=-0.19999796910163758, w1=0.20000358473955224\n",
      "Gradient Descent(20/99): loss =23561638437695.98, w0=-0.19999786248259155, w1=0.2000037664195869\n",
      "Gradient Descent(21/99): loss =23422187511168.35, w0=-0.1999977563525066, w1=0.20000394668503768\n",
      "Gradient Descent(22/99): loss =23287546656108.023, w0=-0.19999765069905104, w1=0.20000412556109423\n",
      "Gradient Descent(23/99): loss =23157522088436.562, w0=-0.19999754551061524, w1=0.20000430307147188\n",
      "Gradient Descent(24/99): loss =23031928860038.3, w0=-0.19999744077624354, w1=0.20000447923867132\n",
      "Gradient Descent(25/99): loss =22910590305107.35, w0=-0.19999733648557405, w1=0.20000465408417645\n",
      "Gradient Descent(26/99): loss =22793337546380.29, w0=-0.1999972326287855, w1=0.20000482762860633\n",
      "Gradient Descent(27/99): loss =22680009051368.21, w0=-0.19999712919655013, w1=0.20000499989183268\n",
      "Gradient Descent(28/99): loss =22570450230641.64, w0=-0.19999702617999163, w1=0.20000517089307168\n",
      "Gradient Descent(29/99): loss =22464513071755.742, w0=-0.19999692357064744, w1=0.20000534065095638\n",
      "Gradient Descent(30/99): loss =22362055803620.25, w0=-0.1999968213604352, w1=0.20000550918359408\n",
      "Gradient Descent(31/99): loss =22262942587085.34, w0=-0.19999671954162243, w1=0.2000056765086128\n",
      "Gradient Descent(32/99): loss =22167043228285.785, w0=-0.1999966181067994, w1=0.20000584264319865\n",
      "Gradient Descent(33/99): loss =22074232911900.895, w0=-0.19999651704885457, w1=0.20000600760412673\n",
      "Gradient Descent(34/99): loss =21984391951982.5, w0=-0.1999964163609526, w1=0.20000617140778673\n",
      "Gradient Descent(35/99): loss =21897405558398.363, w0=-0.19999631603651433, w1=0.20000633407020432\n",
      "Gradient Descent(36/99): loss =21813163617258.633, w0=-0.19999621606919873, w1=0.20000649560705946\n",
      "Gradient Descent(37/99): loss =21731560483950.484, w0=-0.19999611645288665, w1=0.20000665603370194\n",
      "Gradient Descent(38/99): loss =21652494787616.414, w0=-0.19999601718166599, w1=0.20000681536516499\n",
      "Gradient Descent(39/99): loss =21575869246080.66, w0=-0.19999591824981824, w1=0.2000069736161772\n",
      "Gradient Descent(40/99): loss =21501590490368.35, w0=-0.1999958196518064, w1=0.200007130801173\n",
      "Gradient Descent(41/99): loss =21429568898076.023, w0=-0.1999957213822638, w1=0.2000072869343022\n",
      "Gradient Descent(42/99): loss =21359718434945.83, w0=-0.19999562343598404, w1=0.20000744202943851\n",
      "Gradient Descent(43/99): loss =21291956504074.215, w0=-0.19999552580791186, w1=0.20000759610018734\n",
      "Gradient Descent(44/99): loss =21226203802250.293, w0=-0.19999542849313467, w1=0.20000774915989303\n",
      "Gradient Descent(45/99): loss =21162384182974.04, w0=-0.19999533148687498, w1=0.2000079012216455\n",
      "Gradient Descent(46/99): loss =21100424525749.555, w0=-0.19999523478448333, w1=0.20000805229828655\n",
      "Gradient Descent(47/99): loss =21040254611287.85, w0=-0.19999513838143188, w1=0.20000820240241574\n",
      "Gradient Descent(48/99): loss =20981807002286.992, w0=-0.1999950422733085, w1=0.20000835154639585\n",
      "Gradient Descent(49/99): loss =20925016929485.492, w0=-0.1999949464558114, w1=0.20000849974235838\n",
      "Gradient Descent(50/99): loss =20869822182709.277, w0=-0.19999485092474414, w1=0.20000864700220847\n",
      "Gradient Descent(51/99): loss =20816163006655.03, w0=-0.19999475567601105, w1=0.20000879333762983\n",
      "Gradient Descent(52/99): loss =20763982001170.562, w0=-0.1999946607056129, w1=0.2000089387600894\n",
      "Gradient Descent(53/99): loss =20713224025809.848, w0=-0.19999456600964305, w1=0.20000908328084194\n",
      "Gradient Descent(54/99): loss =20663836108455.426, w0=-0.1999944715842838, w1=0.20000922691093428\n",
      "Gradient Descent(55/99): loss =20615767357813.77, w0=-0.199994377425803, w1=0.20000936966120972\n",
      "Gradient Descent(56/99): loss =20568968879601.527, w0=-0.19999428353055093, w1=0.2000095115423121\n",
      "Gradient Descent(57/99): loss =20523393696251.254, w0=-0.19999418989495732, w1=0.20000965256468986\n",
      "Gradient Descent(58/99): loss =20478996669974.566, w0=-0.19999409651552863, w1=0.20000979273859987\n",
      "Gradient Descent(59/99): loss =20435734429030.89, w0=-0.1999940033888455, w1=0.20000993207411136\n",
      "Gradient Descent(60/99): loss =20393565297057.176, w0=-0.19999391051156032, w1=0.20001007058110964\n",
      "Gradient Descent(61/99): loss =20352449225321.98, w0=-0.19999381788039505, w1=0.20001020826929974\n",
      "Gradient Descent(62/99): loss =20312347727774.645, w0=-0.19999372549213898, w1=0.20001034514820998\n",
      "Gradient Descent(63/99): loss =20273223818766.75, w0=-0.19999363334364675, w1=0.20001048122719545\n",
      "Gradient Descent(64/99): loss =20235041953328.746, w0=-0.19999354143183656, w1=0.20001061651544147\n",
      "Gradient Descent(65/99): loss =20197767969891.44, w0=-0.19999344975368824, w1=0.20001075102196694\n",
      "Gradient Descent(66/99): loss =20161369035346.26, w0=-0.1999933583062416, w1=0.20001088475562762\n",
      "Gradient Descent(67/99): loss =20125813592344.203, w0=-0.19999326708659482, w1=0.20001101772511928\n",
      "Gradient Descent(68/99): loss =20091071308737.316, w0=-0.1999931760919029, w1=0.20001114993898095\n",
      "Gradient Descent(69/99): loss =20057113029072.367, w0=-0.19999308531937618, w1=0.20001128140559793\n",
      "Gradient Descent(70/99): loss =20023910728048.336, w0=-0.19999299476627896, w1=0.2000114121332048\n",
      "Gradient Descent(71/99): loss =19991437465856.406, w0=-0.19999290442992812, w1=0.2000115421298885\n",
      "Gradient Descent(72/99): loss =19959667345321.777, w0=-0.19999281430769186, w1=0.20001167140359108\n",
      "Gradient Descent(73/99): loss =19928575470773.062, w0=-0.19999272439698845, w1=0.20001179996211263\n",
      "Gradient Descent(74/99): loss =19898137908565.562, w0=-0.19999263469528505, w1=0.20001192781311405\n",
      "Gradient Descent(75/99): loss =19868331649191.23, w0=-0.19999254520009657, w1=0.2000120549641198\n",
      "Gradient Descent(76/99): loss =19839134570907.844, w0=-0.19999245590898454, w1=0.20001218142252056\n",
      "Gradient Descent(77/99): loss =19810525404825.496, w0=-0.1999923668195561, w1=0.20001230719557578\n",
      "Gradient Descent(78/99): loss =19782483701389.76, w0=-0.1999922779294629, w1=0.2000124322904164\n",
      "Gradient Descent(79/99): loss =19754989798203.633, w0=-0.1999921892364002, w1=0.20001255671404727\n",
      "Gradient Descent(80/99): loss =19728024789134.094, w0=-0.19999210073810583, w1=0.20001268047334964\n",
      "Gradient Descent(81/99): loss =19701570494649.613, w0=-0.1999920124323593, w1=0.20001280357508353\n",
      "Gradient Descent(82/99): loss =19675609433339.04, w0=-0.1999919243169809, w1=0.20001292602589021\n",
      "Gradient Descent(83/99): loss =19650124794562.69, w0=-0.19999183638983078, w1=0.20001304783229445\n",
      "Gradient Descent(84/99): loss =19625100412190.65, w0=-0.1999917486488082, w1=0.20001316900070681\n",
      "Gradient Descent(85/99): loss =19600520739382.895, w0=-0.19999166109185057, w1=0.20001328953742586\n",
      "Gradient Descent(86/99): loss =19576370824369.74, w0=-0.19999157371693274, w1=0.20001340944864038\n",
      "Gradient Descent(87/99): loss =19552636287191.633, w0=-0.19999148652206622, w1=0.20001352874043152\n",
      "Gradient Descent(88/99): loss =19529303297359.664, w0=-0.19999139950529837, w1=0.20001364741877486\n",
      "Gradient Descent(89/99): loss =19506358552399.523, w0=-0.19999131266471168, w1=0.20001376548954253\n",
      "Gradient Descent(90/99): loss =19483789257243.266, w0=-0.1999912259984231, w1=0.2000138829585051\n",
      "Gradient Descent(91/99): loss =19461583104434.688, w0=-0.19999113950458322, w1=0.20001399983133378\n",
      "Gradient Descent(92/99): loss =19439728255115.89, w0=-0.19999105318137575, w1=0.2000141161136021\n",
      "Gradient Descent(93/99): loss =19418213320763.062, w0=-0.19999096702701666, w1=0.20001423181078795\n",
      "Gradient Descent(94/99): loss =19397027345642.31, w0=-0.19999088103975365, w1=0.2000143469282755\n",
      "Gradient Descent(95/99): loss =19376159789955.6, w0=-0.19999079521786553, w1=0.20001446147135685\n",
      "Gradient Descent(96/99): loss =19355600513650.25, w0=-0.1999907095596615, w1=0.20001457544523396\n",
      "Gradient Descent(97/99): loss =19335339760864.824, w0=-0.19999062406348062, w1=0.20001468885502036\n",
      "Gradient Descent(98/99): loss =19315368144986.32, w0=-0.19999053872769115, w1=0.2000148017057428\n",
      "Gradient Descent(99/99): loss =19295676634294.0, w0=-0.19999045355069006, w1=0.200014914002343\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.6188445352767704e+26, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =17707658998691.574, w0=-0.1999999355749133, w1=0.2000000203266585\n",
      "Gradient Descent(3/99): loss =17113459775470.531, w0=-0.19999988388205467, w1=0.2000000104431786\n",
      "Gradient Descent(4/99): loss =16771402932007.445, w0=-0.19999983485542033, w1=0.19999998564747637\n",
      "Gradient Descent(5/99): loss =16520449776638.746, w0=-0.19999978803287288, w1=0.19999994917473152\n",
      "Gradient Descent(6/99): loss =16336222590020.797, w0=-0.19999974307009155, w1=0.19999990316824898\n",
      "Gradient Descent(7/99): loss =16200907530628.127, w0=-0.19999969969006637, w1=0.19999984920861016\n",
      "Gradient Descent(8/99): loss =16101462768347.03, w0=-0.199999657661106, w1=0.1999997885326431\n",
      "Gradient Descent(9/99): loss =16028332448686.64, w0=-0.1999996167875119, w1=0.19999972214603282\n",
      "Gradient Descent(10/99): loss =15974510951812.266, w0=-0.19999957690291806, w1=0.1999996508855586\n",
      "Gradient Descent(11/99): loss =15934860194951.936, w0=-0.1999995378654157, w1=0.19999957545669467\n",
      "Gradient Descent(12/99): loss =15905610666986.777, w0=-0.1999994995537386, w1=0.19999949645859352\n",
      "Gradient Descent(13/99): loss =15883996294313.877, w0=-0.19999946186416262, w1=0.1999994144021626\n",
      "Gradient Descent(14/99): loss =15867987002345.666, w0=-0.19999942470792553, w1=0.1999993297240183\n",
      "Gradient Descent(15/99): loss =15856092704916.299, w0=-0.19999938800905212, w1=0.1999992427977342\n",
      "Gradient Descent(16/99): loss =15847219575635.65, w0=-0.19999935170250654, w1=0.19999915394315126\n",
      "Gradient Descent(17/99): loss =15840564618931.23, w0=-0.19999931573261678, w1=0.19999906343420334\n",
      "Gradient Descent(18/99): loss =15835538314990.213, w0=-0.19999928005172826, w1=0.19999897150555176\n",
      "Gradient Descent(19/99): loss =15831707852233.05, w0=-0.1999992446190518, w1=0.19999887835823696\n",
      "Gradient Descent(20/99): loss =15828755462271.566, w0=-0.19999920939967736, w1=0.19999878416450567\n",
      "Gradient Descent(21/99): loss =15826447836334.396, w0=-0.19999917436372946, w1=0.19999868907193957\n",
      "Gradient Descent(22/99): loss =15824613674177.04, w0=-0.19999913948564393, w1=0.19999859320698948\n",
      "Gradient Descent(23/99): loss =15823127202048.508, w0=-0.19999910474354812, w1=0.19999849667800193\n",
      "Gradient Descent(24/99): loss =15821896072230.516, w0=-0.1999990701187303, w1=0.19999839957781157\n",
      "Gradient Descent(25/99): loss =15820852479084.867, w0=-0.1999990355951851, w1=0.1999983019859622\n",
      "Gradient Descent(26/99): loss =15819946636460.576, w0=-0.19999900115922398, w1=0.19999820397060952\n",
      "Gradient Descent(27/99): loss =15819141988729.541, w0=-0.19999896679914192, w1=0.19999810559015155\n",
      "Gradient Descent(28/99): loss =15818411694628.414, w0=-0.19999893250493164, w1=0.19999800689462544\n",
      "Gradient Descent(29/99): loss =15817736045597.336, w0=-0.19999889826803907, w1=0.19999790792690447\n",
      "Gradient Descent(30/99): loss =15817100570237.818, w0=-0.19999886408115375, w1=0.1999978087237236\n",
      "Gradient Descent(31/99): loss =15816494642534.613, w0=-0.19999882993802948, w1=0.19999770931655822\n",
      "Gradient Descent(32/99): loss =15815910459955.062, w0=-0.1999987958333307, w1=0.1999976097323771\n",
      "Gradient Descent(33/99): loss =15815342293124.865, w0=-0.19999876176250087, w1=0.1999975099942876\n",
      "Gradient Descent(34/99): loss =15814785934904.686, w0=-0.19999872772164995, w1=0.19999741012208846\n",
      "Gradient Descent(35/99): loss =15814238295875.771, w0=-0.19999869370745785, w1=0.19999731013274358\n",
      "Gradient Descent(36/99): loss =15813697107324.336, w0=-0.19999865971709194, w1=0.19999721004078777\n",
      "Gradient Descent(37/99): loss =15813160703156.979, w0=-0.19999862574813634, w1=0.19999710985867464\n",
      "Gradient Descent(38/99): loss =15812627859770.564, w0=-0.19999859179853147, w1=0.19999700959707448\n",
      "Gradient Descent(39/99): loss =15812097678475.318, w0=-0.19999855786652224, w1=0.19999690926512972\n",
      "Gradient Descent(40/99): loss =15811569499162.67, w0=-0.1999985239506138, w1=0.19999680887067367\n",
      "Gradient Descent(41/99): loss =15811042836914.084, w0=-0.19999849004953352, w1=0.1999967084204179\n",
      "Gradient Descent(42/99): loss =15810517335455.36, w0=-0.19999845616219866, w1=0.19999660792011295\n",
      "Gradient Descent(43/99): loss =15809992732978.89, w0=-0.19999842228768858, w1=0.199996507374686\n",
      "Gradient Descent(44/99): loss =15809468837047.889, w0=-0.19999838842522094, w1=0.19999640678835867\n",
      "Gradient Descent(45/99): loss =15808945506168.744, w0=-0.1999983545741315, w1=0.19999630616474817\n",
      "Gradient Descent(46/99): loss =15808422636259.686, w0=-0.19999832073385673, w1=0.19999620550695382\n",
      "Gradient Descent(47/99): loss =15807900150714.844, w0=-0.1999982869039189, w1=0.19999610481763125\n",
      "Gradient Descent(48/99): loss =15807377993107.914, w0=-0.19999825308391347, w1=0.19999600409905596\n",
      "Gradient Descent(49/99): loss =15806856121834.516, w0=-0.19999821927349823, w1=0.19999590335317777\n",
      "Gradient Descent(50/99): loss =15806334506177.61, w0=-0.199998185472384, w1=0.19999580258166744\n",
      "Gradient Descent(51/99): loss =15805813123418.27, w0=-0.19999815168032675, w1=0.19999570178595674\n",
      "Gradient Descent(52/99): loss =15805291956713.85, w0=-0.19999811789712085, w1=0.19999560096727265\n",
      "Gradient Descent(53/99): loss =15804770993539.602, w0=-0.1999980841225933, w1=0.1999955001266667\n",
      "Gradient Descent(54/99): loss =15804250224544.299, w0=-0.19999805035659873, w1=0.19999539926504012\n",
      "Gradient Descent(55/99): loss =15803729642709.643, w0=-0.19999801659901534, w1=0.19999529838316538\n",
      "Gradient Descent(56/99): loss =15803209242732.822, w0=-0.19999798284974116, w1=0.19999519748170466\n",
      "Gradient Descent(57/99): loss =15802689020572.914, w0=-0.19999794910869115, w1=0.1999950965612257\n",
      "Gradient Descent(58/99): loss =15802168973117.865, w0=-0.19999791537579448, w1=0.19999499562221523\n",
      "Gradient Descent(59/99): loss =15801649097939.592, w0=-0.19999788165099236, w1=0.19999489466509074\n",
      "Gradient Descent(60/99): loss =15801129393114.09, w0=-0.1999978479342362, w1=0.19999479369021028\n",
      "Gradient Descent(61/99): loss =15800609857089.354, w0=-0.19999781422548588, w1=0.19999469269788103\n",
      "Gradient Descent(62/99): loss =15800090488588.168, w0=-0.1999977805247085, w1=0.19999459168836664\n",
      "Gradient Descent(63/99): loss =15799571286536.56, w0=-0.19999774683187724, w1=0.19999449066189345\n",
      "Gradient Descent(64/99): loss =15799052250011.498, w0=-0.19999771314697024, w1=0.19999438961865582\n",
      "Gradient Descent(65/99): loss =15798533378202.057, w0=-0.19999767946996985, w1=0.19999428855882073\n",
      "Gradient Descent(66/99): loss =15798014670381.05, w0=-0.199997645800862, w1=0.19999418748253175\n",
      "Gradient Descent(67/99): loss =15797496125884.16, w0=-0.19999761213963546, w1=0.19999408638991237\n",
      "Gradient Descent(68/99): loss =15796977744094.326, w0=-0.1999975784862814, w1=0.19999398528106888\n",
      "Gradient Descent(69/99): loss =15796459524430.54, w0=-0.19999754484079305, w1=0.19999388415609284\n",
      "Gradient Descent(70/99): loss =15795941466339.424, w0=-0.1999975112031651, w1=0.1999937830150633\n",
      "Gradient Descent(71/99): loss =15795423569288.926, w0=-0.19999747757339373, w1=0.1999936818580484\n",
      "Gradient Descent(72/99): loss =15794905832763.848, w0=-0.199997443951476, w1=0.1999935806851071\n",
      "Gradient Descent(73/99): loss =15794388256262.406, w0=-0.19999741033740998, w1=0.19999347949629043\n",
      "Gradient Descent(74/99): loss =15793870839293.605, w0=-0.19999737673119428, w1=0.1999933782916427\n",
      "Gradient Descent(75/99): loss =15793353581375.607, w0=-0.1999973431328281, w1=0.1999932770712024\n",
      "Gradient Descent(76/99): loss =15792836482033.998, w0=-0.199997309542311, w1=0.19999317583500303\n",
      "Gradient Descent(77/99): loss =15792319540800.998, w0=-0.19999727595964292, w1=0.19999307458307386\n",
      "Gradient Descent(78/99): loss =15791802757214.49, w0=-0.1999972423848239, w1=0.19999297331544055\n",
      "Gradient Descent(79/99): loss =15791286130817.496, w0=-0.19999720881785413, w1=0.19999287203212565\n",
      "Gradient Descent(80/99): loss =15790769661157.682, w0=-0.19999717525873398, w1=0.199992770733149\n",
      "Gradient Descent(81/99): loss =15790253347787.074, w0=-0.1999971417074638, w1=0.19999266941852822\n",
      "Gradient Descent(82/99): loss =15789737190261.648, w0=-0.19999710816404392, w1=0.19999256808827895\n",
      "Gradient Descent(83/99): loss =15789221188141.21, w0=-0.1999970746284747, w1=0.1999924667424152\n",
      "Gradient Descent(84/99): loss =15788705340989.2, w0=-0.19999704110075645, w1=0.1999923653809495\n",
      "Gradient Descent(85/99): loss =15788189648372.463, w0=-0.19999700758088934, w1=0.19999226400389322\n",
      "Gradient Descent(86/99): loss =15787674109861.285, w0=-0.19999697406887357, w1=0.19999216261125663\n",
      "Gradient Descent(87/99): loss =15787158725029.088, w0=-0.19999694056470915, w1=0.19999206120304916\n",
      "Gradient Descent(88/99): loss =15786643493452.55, w0=-0.19999690706839607, w1=0.19999195977927944\n",
      "Gradient Descent(89/99): loss =15786128414711.414, w0=-0.19999687357993418, w1=0.19999185833995548\n",
      "Gradient Descent(90/99): loss =15785613488388.396, w0=-0.19999684009932323, w1=0.1999917568850847\n",
      "Gradient Descent(91/99): loss =15785098714069.238, w0=-0.19999680662656288, w1=0.19999165541467404\n",
      "Gradient Descent(92/99): loss =15784584091342.475, w0=-0.19999677316165274, w1=0.19999155392873005\n",
      "Gradient Descent(93/99): loss =15784069619799.615, w0=-0.19999673970459222, w1=0.19999145242725896\n",
      "Gradient Descent(94/99): loss =15783555299034.873, w0=-0.1999967062553807, w1=0.19999135091026665\n",
      "Gradient Descent(95/99): loss =15783041128645.252, w0=-0.19999667281401745, w1=0.19999124937775878\n",
      "Gradient Descent(96/99): loss =15782527108230.496, w0=-0.19999663938050163, w1=0.19999114782974076\n",
      "Gradient Descent(97/99): loss =15782013237392.969, w0=-0.19999660595483235, w1=0.19999104626621786\n",
      "Gradient Descent(98/99): loss =15781499515737.686, w0=-0.1999965725370086, w1=0.19999094468719514\n",
      "Gradient Descent(99/99): loss =15780985942872.229, w0=-0.1999965391270293, w1=0.19999084309267753\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.6381091116708026e+17, w0=-6.522560269672795e-16, w1=0.2\n",
      "Gradient Descent(2/99): loss =32919455.301304698, w0=-4.983289550257939e-06, w1=0.19999785694305328\n",
      "Gradient Descent(3/99): loss =2117142.5758422622, w0=-8.268196246018333e-06, w1=0.19999745768483795\n",
      "Gradient Descent(4/99): loss =1993600.7654368987, w0=-1.195342912273183e-05, w1=0.19999707529292038\n",
      "Gradient Descent(5/99): loss =1961858.4829613022, w0=-1.541908344823592e-05, w1=0.1999966984564438\n",
      "Gradient Descent(6/99): loss =1952632.4632327259, w0=-1.8976850452313555e-05, w1=0.19999632620626973\n",
      "Gradient Descent(7/99): loss =1949049.8047997865, w0=-2.24729551542749e-05, w1=0.19999595701683873\n",
      "Gradient Descent(8/99): loss =1946929.30539441, w0=-2.5984546961698373e-05, w1=0.19999559055163735\n",
      "Gradient Descent(9/99): loss =1945225.9387357773, w0=-2.9473695215426342e-05, w1=0.19999522618158497\n",
      "Gradient Descent(10/99): loss =1943673.5693906927, w0=-3.29597085505228e-05, w1=0.19999486358255097\n",
      "Gradient Descent(11/99): loss =1942201.0521540677, w0=-3.643342140314784e-05, w1=0.19999450239457997\n",
      "Gradient Descent(12/99): loss =1940786.4602069675, w0=-3.989983445399603e-05, w1=0.1999941423691921\n",
      "Gradient Descent(13/99): loss =1939420.981301636, w0=-4.335689728745231e-05, w1=0.19999378327973888\n",
      "Gradient Descent(14/99): loss =1938099.5923395, w0=-4.680605832415021e-05, w1=0.19999342495240116\n",
      "Gradient Descent(15/99): loss =1936818.6761342813, w0=-5.0247014767943855e-05, w1=0.19999306723931165\n",
      "Gradient Descent(16/99): loss =1935575.2884269352, w0=-5.36803222002418e-05, w1=0.19999271002234684\n",
      "Gradient Descent(17/99): loss =1934366.9186481084, w0=-5.710609599065627e-05, w1=0.1999923532033299\n",
      "Gradient Descent(18/99): loss =1933191.368029687, w0=-6.052465675664475e-05, w1=0.19999199670242285\n",
      "Gradient Descent(19/99): loss =1932046.6836319652, w0=-6.393620940479296e-05, w1=0.19999164045365136\n",
      "Gradient Descent(20/99): loss =1930931.110605736, w0=-6.734100290641273e-05, w1=0.19999128440285227\n",
      "Gradient Descent(21/99): loss =1929843.0590116943, w0=-7.073925116383963e-05, w1=0.19999092850523215\n",
      "Gradient Descent(22/99): loss =1928781.0781221036, w0=-7.413117272602837e-05, w1=0.19999057272378137\n",
      "Gradient Descent(23/99): loss =1927743.8369613881, w0=-7.751697145589676e-05, w1=0.19999021702779785\n",
      "Gradient Descent(24/99): loss =1926730.108832683, w0=-8.089684656968238e-05, w1=0.19998986139178695\n",
      "Gradient Descent(25/99): loss =1925738.7589953395, w0=-8.427098808103368e-05, w1=0.19998950579452085\n",
      "Gradient Descent(26/99): loss =1924768.7345638815, w0=-8.763957949734732e-05, w1=0.19998915021829738\n",
      "Gradient Descent(27/99): loss =1923819.056132123, w0=-9.100279690399252e-05, w1=0.19998879464832403\n",
      "Gradient Descent(28/99): loss =1922888.810687616, w0=-9.43608098333573e-05, w1=0.19998843907222041\n",
      "Gradient Descent(29/99): loss =1921977.145539994, w0=-9.77137812346762e-05, w1=0.19998808347960798\n",
      "Gradient Descent(30/99): loss =1921083.2630434888, w0=-0.00010106186787761115, w1=0.19998772786177454\n",
      "Gradient Descent(31/99): loss =1920206.4159605997, w0=-0.00010440522052672835, w1=0.1999873722113975\n",
      "Gradient Descent(32/99): loss =1919345.9033487828, w0=-0.00010774398421616191, w1=0.1999870165223159\n",
      "Gradient Descent(33/99): loss =1918501.066882991, w0=-0.0001110782984609417, w1=0.19998666078934188\n",
      "Gradient Descent(34/99): loss =1917671.2875464063, w0=-0.00011440829748647283, w1=0.19998630500810455\n",
      "Gradient Descent(35/99): loss =1916855.9826371106, w0=-0.00011773411043599536, w1=0.19998594917492052\n",
      "Gradient Descent(36/99): loss =1916054.603049145, w0=-0.00012105586157634904, w1=0.19998559328668616\n",
      "Gradient Descent(37/99): loss =1915266.630794589, w0=-0.000124373670492339, w1=0.19998523734078796\n",
      "Gradient Descent(38/99): loss =1914491.5767391764, w0=-0.0001276876522749317, w1=0.19998488133502768\n",
      "Gradient Descent(39/99): loss =1913728.9785286868, w0=-0.00013099791770109664, w1=0.19998452526755991\n",
      "Gradient Descent(40/99): loss =1912978.398686662, w0=-0.0001343045734067916, w1=0.1999841691368398\n",
      "Gradient Descent(41/99): loss =1912239.422866906, w0=-0.00013760772205275584, w1=0.19998381294157921\n",
      "Gradient Descent(42/99): loss =1911511.6582462564, w0=-0.00014090746248367816, w1=0.19998345668071\n",
      "Gradient Descent(43/99): loss =1910794.7320449753, w0=-0.00014420388988084997, w1=0.19998310035335304\n",
      "Gradient Descent(44/99): loss =1910088.2901634716, w0=-0.00014749709590862993, w1=0.1999827439587923\n",
      "Gradient Descent(45/99): loss =1909391.9959253194, w0=-0.00015078716885492792, w1=0.19998238749645278\n",
      "Gradient Descent(46/99): loss =1908705.5289175527, w0=-0.00015407419376596427, w1=0.19998203096588207\n",
      "Gradient Descent(47/99): loss =1908028.5839200702, w0=-0.00015735825257552383, w1=0.19998167436673459\n",
      "Gradient Descent(48/99): loss =1907360.8699168027, w0=-0.00016063942422893106, w1=0.19998131769875832\n",
      "Gradient Descent(49/99): loss =1906702.1091819084, w0=-0.00016391778480195711, w1=0.19998096096178344\n",
      "Gradient Descent(50/99): loss =1906052.0364348998, w0=-0.00016719340761486577, w1=0.19998060415571273\n",
      "Gradient Descent(51/99): loss =1905410.3980591353, w0=-0.00017046636334179573, w1=0.19998024728051328\n",
      "Gradient Descent(52/99): loss =1904776.9513785716, w0=-0.00017373672011566972, w1=0.19997989033620947\n",
      "Gradient Descent(53/99): loss =1904151.4639881141, w0=-0.00017700454362881373, w1=0.19997953332287688\n",
      "Gradient Descent(54/99): loss =1903533.7131333093, w0=-0.0001802698972294623, w1=0.19997917624063702\n",
      "Gradient Descent(55/99): loss =1902923.4851354463, w0=-0.00018353284201431873, w1=0.19997881908965284\n",
      "Gradient Descent(56/99): loss =1902320.5748584827, w0=-0.00018679343691733303, w1=0.1999784618701248\n",
      "Gradient Descent(57/99): loss =1901724.7852145056, w0=-0.00019005173879485288, w1=0.1999781045822874\n",
      "Gradient Descent(58/99): loss =1901135.9267046964, w0=-0.00019330780250729763, w1=0.19997774722640616\n",
      "Gradient Descent(59/99): loss =1900553.8169930251, w0=-0.00019656168099749862, w1=0.19997738980277502\n",
      "Gradient Descent(60/99): loss =1899978.280510101, w0=-0.00019981342536584334, w1=0.19997703231171393\n",
      "Gradient Descent(61/99): loss =1899409.148084865, w0=-0.00020306308494235593, w1=0.1999766747535668\n",
      "Gradient Descent(62/99): loss =1898846.256601945, w0=-0.00020631070735584, w1=0.19997631712869968\n",
      "Gradient Descent(63/99): loss =1898289.448682694, w0=-0.0002095563386002055, w1=0.19997595943749896\n",
      "Gradient Descent(64/99): loss =1897738.5723880797, w0=-0.00021280002309809646, w1=0.19997560168037007\n",
      "Gradient Descent(65/99): loss =1897193.4809417836, w0=-0.00021604180376192997, w1=0.1999752438577359\n",
      "Gradient Descent(66/99): loss =1896654.032471905, w0=-0.00021928172205245474, w1=0.19997488597003574\n",
      "Gradient Descent(67/99): loss =1896120.0897699008, w0=-0.0002225198180349308, w1=0.19997452801772395\n",
      "Gradient Descent(68/99): loss =1895591.5200654492, w0=-0.00022575613043302904, w1=0.1999741700012691\n",
      "Gradient Descent(69/99): loss =1895068.1948159975, w0=-0.0002289906966805446, w1=0.19997381192115296\n",
      "Gradient Descent(70/99): loss =1894549.9895099225, w0=-0.00023222355297101453, w1=0.1999734537778695\n",
      "Gradient Descent(71/99): loss =1894036.7834822908, w0=-0.00023545473430532622, w1=0.19997309557192416\n",
      "Gradient Descent(72/99): loss =1893528.459742222, w0=-0.0002386842745373992, w1=0.19997273730383314\n",
      "Gradient Descent(73/99): loss =1893024.904811076, w0=-0.00024191220641802037, w1=0.1999723789741225\n",
      "Gradient Descent(74/99): loss =1892526.008570587, w0=-0.0002451385616369081, w1=0.19997202058332766\n",
      "Gradient Descent(75/99): loss =1892031.6641202597, w0=-0.00024836337086307905, w1=0.19997166213199263\n",
      "Gradient Descent(76/99): loss =1891541.7676433523, w0=-0.00025158666378358673, w1=0.19997130362066948\n",
      "Gradient Descent(77/99): loss =1891056.2182807708, w0=-0.0002548084691406997, w1=0.19997094504991775\n",
      "Gradient Descent(78/99): loss =1890574.9180123757, w0=-0.00025802881476758276, w1=0.19997058642030394\n",
      "Gradient Descent(79/99): loss =1890097.7715451133, w0=-0.0002612477276225439, w1=0.19997022773240095\n",
      "Gradient Descent(80/99): loss =1889624.6862075033, w0=-0.0002644652338219047, w1=0.19996986898678765\n",
      "Gradient Descent(81/99): loss =1889155.5718500437, w0=-0.0002676813586715517, w1=0.19996951018404843\n",
      "Gradient Descent(82/99): loss =1888690.340751089, w0=-0.0002708961266972227, w1=0.19996915132477272\n",
      "Gradient Descent(83/99): loss =1888228.9075278502, w0=-0.00027410956167357974, w1=0.19996879240955456\n",
      "Gradient Descent(84/99): loss =1887771.189052137, w0=-0.0002773216866521186, w1=0.19996843343899232\n",
      "Gradient Descent(85/99): loss =1887317.1043705267, w0=-0.00028053252398796305, w1=0.19996807441368816\n",
      "Gradient Descent(86/99): loss =1886866.574628666, w0=-0.0002837420953655889, w1=0.19996771533424781\n",
      "Gradient Descent(87/99): loss =1886419.5229994017, w0=-0.00028695042182352214, w1=0.19996735620128014\n",
      "Gradient Descent(88/99): loss =1885975.8746145137, w0=-0.0002901575237780531, w1=0.19996699701539694\n",
      "Gradient Descent(89/99): loss =1885535.5564997871, w0=-0.00029336342104600664, w1=0.19996663777721246\n",
      "Gradient Descent(90/99): loss =1885098.4975132213, w0=-0.00029656813286660794, w1=0.19996627848734325\n",
      "Gradient Descent(91/99): loss =1884664.6282861563, w0=-0.00029977167792247944, w1=0.1999659191464078\n",
      "Gradient Descent(92/99): loss =1884233.8811671438, w0=-0.00030297407435980555, w1=0.19996555975502628\n",
      "Gradient Descent(93/99): loss =1883806.1901683803, w0=-0.0003061753398076991, w1=0.19996520031382034\n",
      "Gradient Descent(94/99): loss =1883381.490914532, w0=-0.00030937549139680137, w1=0.1999648408234128\n",
      "Gradient Descent(95/99): loss =1882959.7205938324, w0=-0.0003125745457771476, w1=0.1999644812844274\n",
      "Gradient Descent(96/99): loss =1882540.8179112696, w0=-0.0003157725191353279, w1=0.19996412169748864\n",
      "Gradient Descent(97/99): loss =1882124.7230437861, w0=-0.0003189694272109722, w1=0.19996376206322156\n",
      "Gradient Descent(98/99): loss =1881711.3775973178, w0=-0.00032216528531258656, w1=0.1999634023822515\n",
      "Gradient Descent(99/99): loss =1881300.7245656124, w0=-0.0003253601083327675, w1=0.19996304265520387\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.6188445368852137e+26, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =26894560726514.97, w0=-0.19999988135252827, w1=0.200000234244269\n",
      "Gradient Descent(3/99): loss =26515549039964.28, w0=-0.19999977375134473, w1=0.20000045806691716\n",
      "Gradient Descent(4/99): loss =26232552295797.098, w0=-0.19999966635006114, w1=0.2000006757765101\n",
      "Gradient Descent(5/99): loss =25969187926402.61, w0=-0.1999995615126334, w1=0.20000088928416332\n",
      "Gradient Descent(6/99): loss =25719587923474.37, w0=-0.19999945769750904, w1=0.20000109919082154\n",
      "Gradient Descent(7/99): loss =25481959776298.195, w0=-0.1999993553372672, w1=0.2000013060640828\n",
      "Gradient Descent(8/99): loss =25255113691618.438, w0=-0.19999925407397212, w1=0.20000151023144125\n",
      "Gradient Descent(9/99): loss =25038155752453.363, w0=-0.19999915391645998, w1=0.20000171196904873\n",
      "Gradient Descent(10/99): loss =24830334868096.406, w0=-0.19999905473628596, w1=0.20000191146861784\n",
      "Gradient Descent(11/99): loss =24631011184612.7, w0=-0.1999989564783086, w1=0.20000210888188294\n",
      "Gradient Descent(12/99): loss =24439628615602.426, w0=-0.1999988590689952, w1=0.20000230432291113\n",
      "Gradient Descent(13/99): loss =24255699091155.254, w0=-0.19999876245321294, w1=0.20000249788216334\n",
      "Gradient Descent(14/99): loss =24078789769464.242, w0=-0.1999986665772367, w1=0.2000026896313402\n",
      "Gradient Descent(15/99): loss =23908513433848.027, w0=-0.199998571394665, w1=0.20000287962907465\n",
      "Gradient Descent(16/99): loss =23744520854363.223, w0=-0.1999984768629348, w1=0.20000306792410585\n",
      "Gradient Descent(17/99): loss =23586494738385.14, w0=-0.19999838294394284, w1=0.20000325455796153\n",
      "Gradient Descent(18/99): loss =23434144870909.047, w0=-0.19999828960307772, w1=0.20000343956673722\n",
      "Gradient Descent(19/99): loss =23287204190074.035, w0=-0.19999819680901967, w1=0.20000362298245736\n",
      "Gradient Descent(20/99): loss =23145425594977.824, w0=-0.1999981045333028, w1=0.2000038048340388\n",
      "Gradient Descent(21/99): loss =23008579334260.17, w0=-0.19999801275004472, w1=0.2000039851480079\n",
      "Gradient Descent(22/99): loss =22876450857340.832, w0=-0.19999792143566278, w1=0.20000416394902268\n",
      "Gradient Descent(23/99): loss =22748839036434.78, w0=-0.19999783056864345, w1=0.20000434126026037\n",
      "Gradient Descent(24/99): loss =22625554686996.043, w0=-0.19999774012933005, w1=0.20000451710370457\n",
      "Gradient Descent(25/99): loss =22506419329347.633, w0=-0.19999765009973736, w1=0.2000046915003599\n",
      "Gradient Descent(26/99): loss =22391264145947.324, w0=-0.19999756046338518, w1=0.20000486447041343\n",
      "Gradient Descent(27/99): loss =22279929097889.477, w0=-0.1999974712051502, w1=0.20000503603335681\n",
      "Gradient Descent(28/99): loss =22172262171454.0, w0=-0.19999738231113326, w1=0.2000052062080797\n",
      "Gradient Descent(29/99): loss =22068118731220.598, w0=-0.1999972937685404, w1=0.20000537501294147\n",
      "Gradient Descent(30/99): loss =21967360960810.984, w0=-0.1999972055655763, w1=0.20000554246582702\n",
      "Gradient Descent(31/99): loss =21869857375947.9, w0=-0.19999711769134842, w1=0.20000570858419067\n",
      "Gradient Descent(32/99): loss =21775482397427.633, w0=-0.199997030135781, w1=0.20000587338509074\n",
      "Gradient Descent(33/99): loss =21684115973937.496, w0=-0.1999969428895376, w1=0.20000603688521748\n",
      "Gradient Descent(34/99): loss =21595643246526.01, w0=-0.1999968559439515, w1=0.20000619910091563\n",
      "Gradient Descent(35/99): loss =21509954248052.418, w0=-0.19999676929096305, w1=0.20000636004820307\n",
      "Gradient Descent(36/99): loss =21426943632162.566, w0=-0.1999966829230632, w1=0.20000651974278624\n",
      "Gradient Descent(37/99): loss =21346510427329.062, w0=-0.19999659683324272, w1=0.20000667820007323\n",
      "Gradient Descent(38/99): loss =21268557812295.445, w0=-0.1999965110149464, w1=0.20000683543518497\n",
      "Gradient Descent(39/99): loss =21192992909912.434, w0=-0.19999642546203175, w1=0.20000699146296494\n",
      "Gradient Descent(40/99): loss =21119726596882.742, w0=-0.19999634016873188, w1=0.2000071462979877\n",
      "Gradient Descent(41/99): loss =21048673327359.168, w0=-0.19999625512962194, w1=0.2000072999545665\n",
      "Gradient Descent(42/99): loss =20979750968689.285, w0=-0.19999617033958877, w1=0.20000745244676021\n",
      "Gradient Descent(43/99): loss =20912880647884.195, w0=-0.19999608579380382, w1=0.20000760378837945\n",
      "Gradient Descent(44/99): loss =20847986607620.047, w0=-0.19999600148769836, w1=0.20000775399299256\n",
      "Gradient Descent(45/99): loss =20784996070772.207, w0=-0.1999959174169413, w1=0.20000790307393088\n",
      "Gradient Descent(46/99): loss =20723839112634.17, w0=-0.19999583357741924, w1=0.20000805104429384\n",
      "Gradient Descent(47/99): loss =20664448540103.29, w0=-0.19999574996521816, w1=0.2000081979169538\n",
      "Gradient Descent(48/99): loss =20606759777217.867, w0=-0.1999956665766072, w1=0.2000083437045606\n",
      "Gradient Descent(49/99): loss =20550710756517.66, w0=-0.1999955834080238, w1=0.200008488419546\n",
      "Gradient Descent(50/99): loss =20496241815770.01, w0=-0.1999955004560604, w1=0.20000863207412783\n",
      "Gradient Descent(51/99): loss =20443295599663.477, w0=-0.1999954177174522, w1=0.20000877468031422\n",
      "Gradient Descent(52/99): loss =20391816966119.406, w0=-0.1999953351890663, w1=0.2000089162499074\n",
      "Gradient Descent(53/99): loss =20341752896913.54, w0=-0.19999525286789183, w1=0.20000905679450778\n",
      "Gradient Descent(54/99): loss =20293052412333.457, w0=-0.19999517075103088, w1=0.2000091963255175\n",
      "Gradient Descent(55/99): loss =20245666489625.973, w0=-0.19999508883569042, w1=0.20000933485414427\n",
      "Gradient Descent(56/99): loss =20199547985015.957, w0=-0.19999500711917487, w1=0.2000094723914049\n",
      "Gradient Descent(57/99): loss =20154651559094.117, w0=-0.19999492559887944, w1=0.20000960894812894\n",
      "Gradient Descent(58/99): loss =20110933605394.246, w0=-0.19999484427228398, w1=0.200009744534962\n",
      "Gradient Descent(59/99): loss =20068352181991.727, w0=-0.19999476313694753, w1=0.20000987916236926\n",
      "Gradient Descent(60/99): loss =20026866945970.78, w0=-0.19999468219050326, w1=0.20001001284063874\n",
      "Gradient Descent(61/99): loss =19986439090618.375, w0=-0.19999460143065384, w1=0.20001014557988459\n",
      "Gradient Descent(62/99): loss =19947031285214.016, w0=-0.19999452085516733, w1=0.20001027739005034\n",
      "Gradient Descent(63/99): loss =19908607617291.625, w0=-0.19999444046187334, w1=0.20001040828091202\n",
      "Gradient Descent(64/99): loss =19871133537260.18, w0=-0.19999436024865958, w1=0.2000105382620813\n",
      "Gradient Descent(65/99): loss =19834575805274.875, w0=-0.19999428021346857, w1=0.20001066734300849\n",
      "Gradient Descent(66/99): loss =19798902440258.234, w0=-0.1999942003542949, w1=0.2000107955329856\n",
      "Gradient Descent(67/99): loss =19764082670975.797, w0=-0.19999412066918248, w1=0.20001092284114924\n",
      "Gradient Descent(68/99): loss =19730086889076.68, w0=-0.19999404115622205, w1=0.2000110492764835\n",
      "Gradient Descent(69/99): loss =19696886604013.59, w0=-0.19999396181354898, w1=0.20001117484782285\n",
      "Gradient Descent(70/99): loss =19664454399762.344, w0=-0.1999938826393412, w1=0.20001129956385488\n",
      "Gradient Descent(71/99): loss =19632763893263.156, w0=-0.19999380363181732, w1=0.20001142343312303\n",
      "Gradient Descent(72/99): loss =19601789694512.93, w0=-0.1999937247892348, w1=0.20001154646402933\n",
      "Gradient Descent(73/99): loss =19571507368237.34, w0=-0.1999936461098884, w1=0.200011668664837\n",
      "Gradient Descent(74/99): loss =19541893397078.15, w0=-0.19999356759210862, w1=0.2000117900436731\n",
      "Gradient Descent(75/99): loss =19512925146232.87, w0=-0.19999348923426033, w1=0.20001191060853096\n",
      "Gradient Descent(76/99): loss =19484580829486.016, w0=-0.19999341103474147, w1=0.20001203036727286\n",
      "Gradient Descent(77/99): loss =19456839476576.457, w0=-0.19999333299198185, w1=0.2000121493276323\n",
      "Gradient Descent(78/99): loss =19429680901844.043, w0=-0.19999325510444205, w1=0.20001226749721657\n",
      "Gradient Descent(79/99): loss =19403085674106.105, w0=-0.19999317737061226, w1=0.20001238488350892\n",
      "Gradient Descent(80/99): loss =19377035087711.32, w0=-0.19999309978901142, w1=0.20001250149387112\n",
      "Gradient Descent(81/99): loss =19351511134725.387, w0=-0.19999302235818625, w1=0.2000126173355455\n",
      "Gradient Descent(82/99): loss =19326496478201.273, w0=-0.1999929450767104, w1=0.20001273241565742\n",
      "Gradient Descent(83/99): loss =19301974426491.973, w0=-0.1999928679431836, w1=0.2000128467412172\n",
      "Gradient Descent(84/99): loss =19277928908562.777, w0=-0.19999279095623093, w1=0.2000129603191225\n",
      "Gradient Descent(85/99): loss =19254344450263.41, w0=-0.1999927141145021, w1=0.2000130731561603\n",
      "Gradient Descent(86/99): loss =19231206151522.152, w0=-0.19999263741667067, w1=0.200013185259009\n",
      "Gradient Descent(87/99): loss =19208499664424.656, w0=-0.19999256086143355, w1=0.20001329663424045\n",
      "Gradient Descent(88/99): loss =19186211172142.848, w0=-0.1999924844475103, w1=0.20001340728832193\n",
      "Gradient Descent(89/99): loss =19164327368679.28, w0=-0.1999924081736425, w1=0.20001351722761815\n",
      "Gradient Descent(90/99): loss =19142835439396.324, w0=-0.1999923320385933, w1=0.20001362645839313\n",
      "Gradient Descent(91/99): loss =19121723042297.125, w0=-0.19999225604114676, w1=0.20001373498681205\n",
      "Gradient Descent(92/99): loss =19100978290030.242, w0=-0.19999218018010748, w1=0.2000138428189431\n",
      "Gradient Descent(93/99): loss =19080589732588.836, w0=-0.19999210445429996, w1=0.20001394996075936\n",
      "Gradient Descent(94/99): loss =19060546340677.133, w0=-0.19999202886256828, w1=0.20001405641814046\n",
      "Gradient Descent(95/99): loss =19040837489717.684, w0=-0.19999195340377554, w1=0.2000141621968744\n",
      "Gradient Descent(96/99): loss =19021452944475.41, w0=-0.19999187807680344, w1=0.20001426730265925\n",
      "Gradient Descent(97/99): loss =19002382844272.66, w0=-0.19999180288055193, w1=0.20001437174110479\n",
      "Gradient Descent(98/99): loss =18983617688773.008, w0=-0.19999172781393876, w1=0.20001447551773413\n",
      "Gradient Descent(99/99): loss =18965148324312.24, w0=-0.1999916528758991, w1=0.20001457863798536\n",
      "Optimizing degree 9/15, model: least_squares_GD, arguments: {'max_iters': 100}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =2.602390810088692e+30, w0=-0.20000000000000004, w1=0.2\n",
      "Gradient Descent(2/99): loss =8169336498665007.0, w0=-0.19999998541248845, w1=0.200000027539451\n",
      "Gradient Descent(3/99): loss =8118022825365964.0, w0=-0.19999997127701585, w1=0.2000000548145353\n",
      "Gradient Descent(4/99): loss =8071856199732022.0, w0=-0.19999995722122402, w1=0.2000000818046369\n",
      "Gradient Descent(5/99): loss =8026887540888740.0, w0=-0.1999999432309589, w1=0.20000010854901318\n",
      "Gradient Descent(6/99): loss =7983043305726699.0, w0=-0.1999999293004684, w1=0.20000013507541414\n",
      "Gradient Descent(7/99): loss =7940278946205562.0, w0=-0.19999991542676526, w1=0.2000001614051365\n",
      "Gradient Descent(8/99): loss =7898554654295894.0, w0=-0.1999999016074907, w1=0.20000018755470006\n",
      "Gradient Descent(9/99): loss =7857834141846611.0, w0=-0.1999998878405953, w1=0.20000021353701253\n",
      "Gradient Descent(10/99): loss =7818084038288368.0, w0=-0.19999987412425313, w1=0.2000002393622302\n",
      "Gradient Descent(11/99): loss =7779273423815052.0, w0=-0.19999986045682, w1=0.20000026503840893\n",
      "Gradient Descent(12/99): loss =7741373457825498.0, w0=-0.1999998468368041, w1=0.20000029057199867\n",
      "Gradient Descent(13/99): loss =7704357080345013.0, w0=-0.19999983326284324, w1=0.20000031596821963\n",
      "Gradient Descent(14/99): loss =7668198770200990.0, w0=-0.19999981973368644, w1=0.20000034123134877\n",
      "Gradient Descent(15/99): loss =7632874347810174.0, w0=-0.19999980624817926, w1=0.20000036636493823\n",
      "Gradient Descent(16/99): loss =7598360813344397.0, w0=-0.1999997928052515, w1=0.2000003913719819\n",
      "Gradient Descent(17/99): loss =7564636213159091.0, w0=-0.19999977940390723, w1=0.2000004162550426\n",
      "Gradient Descent(18/99): loss =7531679528939421.0, w0=-0.1999997660432164, w1=0.2000004410163495\n",
      "Gradient Descent(19/99): loss =7499470585202479.0, w0=-0.19999975272230758, w1=0.2000004656578724\n",
      "Gradient Descent(20/99): loss =7467989971698247.0, w0=-0.19999973944036206, w1=0.20000049018137891\n",
      "Gradient Descent(21/99): loss =7437218977951340.0, w0=-0.1999997261966086, w1=0.2000005145884781\n",
      "Gradient Descent(22/99): loss =7407139537731440.0, w0=-0.19999971299031888, w1=0.2000005388806542\n",
      "Gradient Descent(23/99): loss =7377734181670218.0, w0=-0.19999969982080354, w1=0.20000056305929254\n",
      "Gradient Descent(24/99): loss =7348985996582934.0, w0=-0.19999968668740875, w1=0.20000058712569938\n",
      "Gradient Descent(25/99): loss =7320878590324993.0, w0=-0.19999967358951315, w1=0.20000061108111736\n",
      "Gradient Descent(26/99): loss =7293396061230889.0, w0=-0.199999660526525, w1=0.2000006349267374\n",
      "Gradient Descent(27/99): loss =7266522971358687.0, w0=-0.19999964749787993, w1=0.20000065866370786\n",
      "Gradient Descent(28/99): loss =7240244322904066.0, w0=-0.19999963450303848, w1=0.20000068229314183\n",
      "Gradient Descent(29/99): loss =7214545537263066.0, w0=-0.19999962154148435, w1=0.2000007058161225\n",
      "Gradient Descent(30/99): loss =7189412436315424.0, w0=-0.19999960861272248, w1=0.20000072923370765\n",
      "Gradient Descent(31/99): loss =7164831225576392.0, w0=-0.19999959571627748, w1=0.20000075254693292\n",
      "Gradient Descent(32/99): loss =7140788478926752.0, w0=-0.19999958285169214, w1=0.2000007757568145\n",
      "Gradient Descent(33/99): loss =7117271124681333.0, w0=-0.19999957001852608, w1=0.20000079886435115\n",
      "Gradient Descent(34/99): loss =7094266432797661.0, w0=-0.19999955721635457, w1=0.20000082187052598\n",
      "Gradient Descent(35/99): loss =7071762003060678.0, w0=-0.1999995444447674, w1=0.2000008447763075\n",
      "Gradient Descent(36/99): loss =7049745754106978.0, w0=-0.19999953170336782, w1=0.20000086758265084\n",
      "Gradient Descent(37/99): loss =7028205913175362.0, w0=-0.19999951899177174, w1=0.20000089029049847\n",
      "Gradient Descent(38/99): loss =7007131006488937.0, w0=-0.19999950630960672, w1=0.20000091290078084\n",
      "Gradient Descent(39/99): loss =6986509850190355.0, w0=-0.19999949365651132, w1=0.2000009354144169\n",
      "Gradient Descent(40/99): loss =6966331541763618.0, w0=-0.19999948103213433, w1=0.20000095783231456\n",
      "Gradient Descent(41/99): loss =6946585451887587.0, w0=-0.19999946843613417, w1=0.200000980155371\n",
      "Gradient Descent(42/99): loss =6927261216674191.0, w0=-0.19999945586817824, w1=0.20000100238447294\n",
      "Gradient Descent(43/99): loss =6908348730252309.0, w0=-0.1999994433279424, w1=0.20000102452049692\n",
      "Gradient Descent(44/99): loss =6889838137663876.0, w0=-0.19999943081511048, w1=0.20000104656430945\n",
      "Gradient Descent(45/99): loss =6871719828044048.0, w0=-0.19999941832937382, w1=0.2000010685167672\n",
      "Gradient Descent(46/99): loss =6853984428061268.0, w0=-0.19999940587043083, w1=0.20000109037871713\n",
      "Gradient Descent(47/99): loss =6836622795596525.0, w0=-0.19999939343798662, w1=0.2000011121509967\n",
      "Gradient Descent(48/99): loss =6819626013644198.0, w0=-0.19999938103175263, w1=0.20000113383443385\n",
      "Gradient Descent(49/99): loss =6802985384418842.0, w0=-0.19999936865144635, w1=0.20000115542984723\n",
      "Gradient Descent(50/99): loss =6786692423654911.0, w0=-0.19999935629679094, w1=0.20000117693804625\n",
      "Gradient Descent(51/99): loss =6770738855087312.0, w0=-0.19999934396751498, w1=0.20000119835983118\n",
      "Gradient Descent(52/99): loss =6755116605102942.0, w0=-0.19999933166335226, w1=0.20000121969599327\n",
      "Gradient Descent(53/99): loss =6739817797553734.0, w0=-0.19999931938404153, w1=0.20000124094731475\n",
      "Gradient Descent(54/99): loss =6724834748723437.0, w0=-0.1999993071293262, w1=0.20000126211456906\n",
      "Gradient Descent(55/99): loss =6710159962440609.0, w0=-0.19999929489895427, w1=0.20000128319852079\n",
      "Gradient Descent(56/99): loss =6695786125331502.0, w0=-0.19999928269267803, w1=0.20000130419992582\n",
      "Gradient Descent(57/99): loss =6681706102206785.0, w0=-0.19999927051025396, w1=0.20000132511953148\n",
      "Gradient Descent(58/99): loss =6667912931576723.0, w0=-0.19999925835144255, w1=0.2000013459580765\n",
      "Gradient Descent(59/99): loss =6654399821289876.0, w0=-0.19999924621600812, w1=0.2000013667162912\n",
      "Gradient Descent(60/99): loss =6641160144290705.0, w0=-0.19999923410371875, w1=0.2000013873948975\n",
      "Gradient Descent(61/99): loss =6628187434491850.0, w0=-0.19999922201434608, w1=0.20000140799460905\n",
      "Gradient Descent(62/99): loss =6615475382757097.0, w0=-0.19999920994766524, w1=0.2000014285161313\n",
      "Gradient Descent(63/99): loss =6603017832991389.0, w0=-0.1999991979034547, w1=0.20000144896016162\n",
      "Gradient Descent(64/99): loss =6590808778334283.0, w0=-0.19999918588149623, w1=0.20000146932738924\n",
      "Gradient Descent(65/99): loss =6578842357453677.0, w0=-0.19999917388157473, w1=0.20000148961849554\n",
      "Gradient Descent(66/99): loss =6567112850936551.0, w0=-0.19999916190347822, w1=0.200001509834154\n",
      "Gradient Descent(67/99): loss =6555614677773946.0, w0=-0.19999914994699763, w1=0.2000015299750303\n",
      "Gradient Descent(68/99): loss =6544342391937172.0, w0=-0.1999991380119269, w1=0.2000015500417824\n",
      "Gradient Descent(69/99): loss =6533290679042635.0, w0=-0.1999991260980627, w1=0.20000157003506075\n",
      "Gradient Descent(70/99): loss =6522454353102705.0, w0=-0.19999911420520455, w1=0.20000158995550815\n",
      "Gradient Descent(71/99): loss =6511828353360176.0, w0=-0.19999910233315463, w1=0.20000160980376\n",
      "Gradient Descent(72/99): loss =6501407741203824.0, w0=-0.19999909048171774, w1=0.20000162958044435\n",
      "Gradient Descent(73/99): loss =6491187697162948.0, w0=-0.1999990786507013, w1=0.20000164928618192\n",
      "Gradient Descent(74/99): loss =6481163517978462.0, w0=-0.19999906683991522, w1=0.20000166892158627\n",
      "Gradient Descent(75/99): loss =6471330613748707.0, w0=-0.1999990550491719, w1=0.20000168848726382\n",
      "Gradient Descent(76/99): loss =6461684505147676.0, w0=-0.1999990432782861, w1=0.20000170798381398\n",
      "Gradient Descent(77/99): loss =6452220820713779.0, w0=-0.19999903152707504, w1=0.20000172741182917\n",
      "Gradient Descent(78/99): loss =6442935294207223.0, w0=-0.1999990197953582, w1=0.20000174677189494\n",
      "Gradient Descent(79/99): loss =6433823762034183.0, w0=-0.19999900808295734, w1=0.20000176606459008\n",
      "Gradient Descent(80/99): loss =6424882160735860.0, w0=-0.19999899638969648, w1=0.20000178529048662\n",
      "Gradient Descent(81/99): loss =6416106524540827.0, w0=-0.1999989847154018, w1=0.20000180445014998\n",
      "Gradient Descent(82/99): loss =6407492982978867.0, w0=-0.19999897305990172, w1=0.200001823544139\n",
      "Gradient Descent(83/99): loss =6399037758554734.0, w0=-0.1999989614230267, w1=0.20000184257300607\n",
      "Gradient Descent(84/99): loss =6390737164480218.0, w0=-0.1999989498046093, w1=0.20000186153729713\n",
      "Gradient Descent(85/99): loss =6382587602462954.0, w0=-0.1999989382044842, w1=0.20000188043755182\n",
      "Gradient Descent(86/99): loss =6374585560550569.0, w0=-0.19999892662248797, w1=0.2000018992743035\n",
      "Gradient Descent(87/99): loss =6366727611028597.0, w0=-0.1999989150584593, w1=0.20000191804807943\n",
      "Gradient Descent(88/99): loss =6359010408370857.0, w0=-0.19999890351223876, w1=0.20000193675940064\n",
      "Gradient Descent(89/99): loss =6351430687240862.0, w0=-0.19999889198366885, w1=0.20000195540878224\n",
      "Gradient Descent(90/99): loss =6343985260542956.0, w0=-0.199998880472594, w1=0.2000019739967333\n",
      "Gradient Descent(91/99): loss =6336671017521887.0, w0=-0.1999988689788605, w1=0.20000199252375706\n",
      "Gradient Descent(92/99): loss =6329484921909548.0, w0=-0.1999988575023164, w1=0.2000020109903509\n",
      "Gradient Descent(93/99): loss =6322424010117716.0, w0=-0.1999988460428117, w1=0.20000202939700648\n",
      "Gradient Descent(94/99): loss =6315485389475459.0, w0=-0.19999883460019813, w1=0.2000020477442098\n",
      "Gradient Descent(95/99): loss =6308666236510313.0, w0=-0.19999882317432913, w1=0.20000206603244117\n",
      "Gradient Descent(96/99): loss =6301963795271796.0, w0=-0.19999881176505993, w1=0.20000208426217544\n",
      "Gradient Descent(97/99): loss =6295375375696505.0, w0=-0.1999988003722475, w1=0.20000210243388195\n",
      "Gradient Descent(98/99): loss =6288898352013348.0, w0=-0.19999878899575044, w1=0.20000212054802463\n",
      "Gradient Descent(99/99): loss =6282530161188356.0, w0=-0.19999877763542906, w1=0.20000213860506208\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =2.602390809931332e+30, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =5606874368837310.0, w0=-0.19999999230774246, w1=0.2000000013971763\n",
      "Gradient Descent(3/99): loss =5448614169911689.0, w0=-0.1999999859291548, w1=0.19999999983791317\n",
      "Gradient Descent(4/99): loss =5352081402237650.0, w0=-0.19999997986454685, w1=0.19999999661376644\n",
      "Gradient Descent(5/99): loss =5282144491533076.0, w0=-0.19999997406005282, w1=0.19999999204730134\n",
      "Gradient Descent(6/99): loss =5231452182025519.0, w0=-0.19999996847518903, w1=0.1999999863722733\n",
      "Gradient Descent(7/99): loss =5194693425615662.0, w0=-0.19999996307649284, w1=0.19999997977009187\n",
      "Gradient Descent(8/99): loss =5168026817041169.0, w0=-0.19999995783591026, w1=0.19999997238658387\n",
      "Gradient Descent(9/99): loss =5148672044380254.0, w0=-0.1999999527297737, w1=0.19999996434167583\n",
      "Gradient Descent(10/99): loss =5134615817367675.0, w0=-0.19999994773804078, w1=0.19999995573539808\n",
      "Gradient Descent(11/99): loss =5124399758349073.0, w0=-0.19999994284370176, w1=0.19999994665189896\n",
      "Gradient Descent(12/99): loss =5116967189708781.0, w0=-0.19999993803230223, w1=0.19999993716232403\n",
      "Gradient Descent(13/99): loss =5111552349399055.0, w0=-0.19999993329155025, w1=0.19999992732700325\n",
      "Gradient Descent(14/99): loss =5107600225536715.0, w0=-0.19999992861098845, w1=0.1999999171971833\n",
      "Gradient Descent(15/99): loss =5104708516176487.0, w0=-0.1999999239817181, w1=0.19999990681643878\n",
      "Gradient Descent(16/99): loss =5102585591437683.0, w0=-0.19999991939616588, w1=0.19999989622184236\n",
      "Gradient Descent(17/99): loss =5101020037335961.0, w0=-0.19999991484788612, w1=0.19999988544494557\n",
      "Gradient Descent(18/99): loss =5099858585958342.0, w0=-0.19999991033139275, w1=0.19999987451260642\n",
      "Gradient Descent(19/99): loss =5098990120356714.0, w0=-0.19999990584201646, w1=0.1999998634476902\n",
      "Gradient Descent(20/99): loss =5098334080843035.0, w0=-0.19999990137578322, w1=0.1999998522696642\n",
      "Gradient Descent(21/99): loss =5097832060886788.0, w0=-0.19999989692931092, w1=0.19999984099510354\n",
      "Gradient Descent(22/99): loss =5097441714761639.0, w0=-0.1999998924997213, w1=0.199999829638121\n",
      "Gradient Descent(23/99): loss =5097132340857186.0, w0=-0.19999988808456506, w1=0.19999981821073298\n",
      "Gradient Descent(24/99): loss =5096881679678564.0, w0=-0.19999988368175803, w1=0.19999980672317108\n",
      "Gradient Descent(25/99): loss =5096673592417193.0, w0=-0.19999987928952698, w1=0.19999979518414712\n",
      "Gradient Descent(26/99): loss =5096496377903188.0, w0=-0.19999987490636326, w1=0.19999978360107892\n",
      "Gradient Descent(27/99): loss =5096341552373483.0, w0=-0.19999987053098353, w1=0.1999997719802825\n",
      "Gradient Descent(28/99): loss =5096202964780166.0, w0=-0.19999986616229626, w1=0.1999997603271357\n",
      "Gradient Descent(29/99): loss =5096076155367855.0, w0=-0.19999986179937307, w1=0.1999997486462176\n",
      "Gradient Descent(30/99): loss =5095957890624986.0, w0=-0.1999998574414246, w1=0.1999997369414271\n",
      "Gradient Descent(31/99): loss =5095845826109618.0, w0=-0.19999985308777973, w1=0.19999972521608386\n",
      "Gradient Descent(32/99): loss =5095738261987549.0, w0=-0.19999984873786802, w1=0.1999997134730145\n",
      "Gradient Descent(33/99): loss =5095633965789435.0, w0=-0.1999998443912047, w1=0.19999970171462572\n",
      "Gradient Descent(34/99): loss =5095532043903614.0, w0=-0.19999984004737784, w1=0.19999968994296666\n",
      "Gradient Descent(35/99): loss =5095431848403899.0, w0=-0.1999998357060376, w1=0.19999967815978206\n",
      "Gradient Descent(36/99): loss =5095332909496393.0, w0=-0.19999983136688695, w1=0.19999966636655747\n",
      "Gradient Descent(37/99): loss =5095234886540917.0, w0=-0.19999982702967373, w1=0.19999965456455768\n",
      "Gradient Descent(38/99): loss =5095137532539698.0, w0=-0.199999822694184, w1=0.19999964275485957\n",
      "Gradient Descent(39/99): loss =5095040668390362.0, w0=-0.19999981836023636, w1=0.19999963093838\n",
      "Gradient Descent(40/99): loss =5094944164218245.0, w0=-0.19999981402767703, w1=0.1999996191158996\n",
      "Gradient Descent(41/99): loss =5094847925841601.0, w0=-0.19999980969637568, w1=0.199999607288083\n",
      "Gradient Descent(42/99): loss =5094751884958310.0, w0=-0.19999980536622208, w1=0.199999595455496\n",
      "Gradient Descent(43/99): loss =5094655992030799.0, w0=-0.1999998010371229, w1=0.19999958361862039\n",
      "Gradient Descent(44/99): loss =5094560211127283.0, w0=-0.1999997967089993, w1=0.19999957177786623\n",
      "Gradient Descent(45/99): loss =5094464516181412.0, w0=-0.19999979238178472, w1=0.1999995599335827\n",
      "Gradient Descent(46/99): loss =5094368888280323.0, w0=-0.19999978805542298, w1=0.19999954808606707\n",
      "Gradient Descent(47/99): loss =5094273313698308.0, w0=-0.19999978372986682, w1=0.19999953623557234\n",
      "Gradient Descent(48/99): loss =5094177782471145.0, w0=-0.19999977940507643, w1=0.19999952438231397\n",
      "Gradient Descent(49/99): loss =5094082287362313.0, w0=-0.19999977508101843, w1=0.19999951252647527\n",
      "Gradient Descent(50/99): loss =5093986823113508.0, w0=-0.1999997707576648, w1=0.19999950066821237\n",
      "Gradient Descent(51/99): loss =5093891385901130.0, w0=-0.1999997664349922, w1=0.1999994888076581\n",
      "Gradient Descent(52/99): loss =5093795972942232.0, w0=-0.19999976211298107, w1=0.1999994769449256\n",
      "Gradient Descent(53/99): loss =5093700582208715.0, w0=-0.19999975779161522, w1=0.19999946508011107\n",
      "Gradient Descent(54/99): loss =5093605212220209.0, w0=-0.19999975347088123, w1=0.19999945321329646\n",
      "Gradient Descent(55/99): loss =5093509861893702.0, w0=-0.199999749150768, w1=0.1999994413445515\n",
      "Gradient Descent(56/99): loss =5093414530434650.0, w0=-0.19999974483126648, w1=0.1999994294739355\n",
      "Gradient Descent(57/99): loss =5093319217257908.0, w0=-0.19999974051236924, w1=0.199999417601499\n",
      "Gradient Descent(58/99): loss =5093223921930420.0, w0=-0.19999973619407033, w1=0.19999940572728492\n",
      "Gradient Descent(59/99): loss =5093128644129623.0, w0=-0.1999997318763649, w1=0.19999939385132984\n",
      "Gradient Descent(60/99): loss =5093033383613314.0, w0=-0.19999972755924922, w1=0.19999938197366485\n",
      "Gradient Descent(61/99): loss =5092938140197731.0, w0=-0.1999997232427203, w1=0.19999937009431643\n",
      "Gradient Descent(62/99): loss =5092842913741692.0, w0=-0.19999971892677593, w1=0.19999935821330708\n",
      "Gradient Descent(63/99): loss =5092747704135049.0, w0=-0.19999971461141441, w1=0.19999934633065597\n",
      "Gradient Descent(64/99): loss =5092652511290311.0, w0=-0.1999997102966346, w1=0.19999933444637938\n",
      "Gradient Descent(65/99): loss =5092557335136592.0, w0=-0.19999970598243572, w1=0.19999932256049116\n",
      "Gradient Descent(66/99): loss =5092462175615160.0, w0=-0.1999997016688173, w1=0.1999993106730031\n",
      "Gradient Descent(67/99): loss =5092367032676243.0, w0=-0.19999969735577924, w1=0.19999929878392522\n",
      "Gradient Descent(68/99): loss =5092271906276658.0, w0=-0.19999969304332157, w1=0.19999928689326604\n",
      "Gradient Descent(69/99): loss =5092176796378150.0, w0=-0.19999968873144455, w1=0.1999992750010328\n",
      "Gradient Descent(70/99): loss =5092081702946105.0, w0=-0.19999968442014857, w1=0.19999926310723165\n",
      "Gradient Descent(71/99): loss =5091986625948674.0, w0=-0.19999968010943414, w1=0.19999925121186785\n",
      "Gradient Descent(72/99): loss =5091891565356096.0, w0=-0.19999967579930186, w1=0.19999923931494584\n",
      "Gradient Descent(73/99): loss =5091796521140194.0, w0=-0.19999967148975242, w1=0.19999922741646942\n",
      "Gradient Descent(74/99): loss =5091701493274048.0, w0=-0.19999966718078654, w1=0.1999992155164418\n",
      "Gradient Descent(75/99): loss =5091606481731697.0, w0=-0.19999966287240498, w1=0.1999992036148657\n",
      "Gradient Descent(76/99): loss =5091511486487962.0, w0=-0.1999996585646085, w1=0.19999919171174346\n",
      "Gradient Descent(77/99): loss =5091416507518284.0, w0=-0.19999965425739794, w1=0.1999991798070771\n",
      "Gradient Descent(78/99): loss =5091321544798632.0, w0=-0.19999964995077407, w1=0.19999916790086827\n",
      "Gradient Descent(79/99): loss =5091226598305405.0, w0=-0.19999964564473774, w1=0.1999991559931184\n",
      "Gradient Descent(80/99): loss =5091131668015356.0, w0=-0.1999996413392897, w1=0.19999914408382874\n",
      "Gradient Descent(81/99): loss =5091036753905578.0, w0=-0.1999996370344308, w1=0.19999913217300033\n",
      "Gradient Descent(82/99): loss =5090941855953429.0, w0=-0.19999963273016177, w1=0.19999912026063407\n",
      "Gradient Descent(83/99): loss =5090846974136534.0, w0=-0.1999996284264834, w1=0.1999991083467307\n",
      "Gradient Descent(84/99): loss =5090752108432721.0, w0=-0.1999996241233964, w1=0.1999990964312909\n",
      "Gradient Descent(85/99): loss =5090657258820036.0, w0=-0.1999996198209015, w1=0.19999908451431525\n",
      "Gradient Descent(86/99): loss =5090562425276717.0, w0=-0.19999961551899942, w1=0.1999990725958042\n",
      "Gradient Descent(87/99): loss =5090467607781166.0, w0=-0.1999996112176908, w1=0.1999990606757582\n",
      "Gradient Descent(88/99): loss =5090372806311954.0, w0=-0.1999996069169763, w1=0.1999990487541776\n",
      "Gradient Descent(89/99): loss =5090278020847801.0, w0=-0.19999960261685654, w1=0.19999903683106268\n",
      "Gradient Descent(90/99): loss =5090183251367561.0, w0=-0.19999959831733213, w1=0.19999902490641377\n",
      "Gradient Descent(91/99): loss =5090088497850247.0, w0=-0.19999959401840364, w1=0.19999901298023112\n",
      "Gradient Descent(92/99): loss =5089993760274976.0, w0=-0.19999958972007162, w1=0.19999900105251492\n",
      "Gradient Descent(93/99): loss =5089899038621016.0, w0=-0.1999995854223366, w1=0.19999898912326539\n",
      "Gradient Descent(94/99): loss =5089804332867718.0, w0=-0.19999958112519906, w1=0.1999989771924827\n",
      "Gradient Descent(95/99): loss =5089709642994570.0, w0=-0.19999957682865951, w1=0.199998965260167\n",
      "Gradient Descent(96/99): loss =5089614968981162.0, w0=-0.1999995725327184, w1=0.19999895332631845\n",
      "Gradient Descent(97/99): loss =5089520310807173.0, w0=-0.19999956823737616, w1=0.19999894139093718\n",
      "Gradient Descent(98/99): loss =5089425668452402.0, w0=-0.1999995639426332, w1=0.19999892945402334\n",
      "Gradient Descent(99/99): loss =5089331041896738.0, w0=-0.19999955964848995, w1=0.19999891751557705\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.5961767803446487e+20, w0=0.0, w1=0.2\n",
      "Gradient Descent(2/99): loss =32962565.91138427, w0=-1.8957552917113863e-07, w1=0.19999988624558107\n",
      "Gradient Descent(3/99): loss =2278887.384817944, w0=-3.315756915778672e-07, w1=0.19999986373550155\n",
      "Gradient Descent(4/99): loss =2227483.2295051315, w0=-4.787925457717699e-07, w1=0.19999984176691052\n",
      "Gradient Descent(5/99): loss =2221516.789064623, w0=-6.241503741747885e-07, w1=0.19999982005364475\n",
      "Gradient Descent(6/99): loss =2219538.902232683, w0=-7.698389141708036e-07, w1=0.1999997985174092\n",
      "Gradient Descent(7/99): loss =2217962.5842159754, w0=-9.15227356587524e-07, w1=0.199999777122714\n",
      "Gradient Descent(8/99): loss =2216464.637990186, w0=-1.060505488278555e-06, w1=0.1999997558435744\n",
      "Gradient Descent(9/99): loss =2215010.18212699, w0=-1.2056169587202838e-06, w1=0.19999973465957144\n",
      "Gradient Descent(10/99): loss =2213591.552659673, w0=-1.350578657889121e-06, w1=0.1999997135538751\n",
      "Gradient Descent(11/99): loss =2212204.727526398, w0=-1.4953862002852394e-06, w1=0.19999969251268626\n",
      "Gradient Descent(12/99): loss =2210846.851333394, w0=-1.640041894715892e-06, w1=0.19999967152468323\n",
      "Gradient Descent(13/99): loss =2209515.7586422893, w0=-1.7845463226690444e-06, w1=0.19999965058060676\n",
      "Gradient Descent(14/99): loss =2208209.759290119, w0=-1.928900749353505e-06, w1=0.19999962967289228\n",
      "Gradient Descent(15/99): loss =2206927.5014186553, w0=-2.0731063464106852e-06, w1=0.19999960879536416\n",
      "Gradient Descent(16/99): loss =2205667.877294105, w0=-2.2171643813467973e-06, w1=0.19999958794297976\n",
      "Gradient Descent(17/99): loss =2204429.9572098027, w0=-2.361076133626289e-06, w1=0.19999956711161865\n",
      "Gradient Descent(18/99): loss =2203212.9426324554, w0=-2.5048429010365535e-06, w1=0.19999954629790978\n",
      "Gradient Descent(19/99): loss =2202016.132769207, w0=-2.648465985310809e-06, w1=0.19999952549909059\n",
      "Gradient Descent(20/99): loss =2200838.9005917357, w0=-2.791946687936224e-06, w1=0.1999995047128927\n",
      "Gradient Descent(21/99): loss =2199680.675569831, w0=-2.9352863056226013e-06, w1=0.19999948393744924\n",
      "Gradient Descent(22/99): loss =2198540.9311869973, w0=-3.078486127669882e-06, w1=0.1999994631712197\n",
      "Gradient Descent(23/99): loss =2197419.1758754416, w0=-3.221547433984758e-06, w1=0.19999944241292952\n",
      "Gradient Descent(24/99): loss =2196314.9464018354, w0=-3.3644714937305024e-06, w1=0.19999942166152088\n",
      "Gradient Descent(25/99): loss =2195227.803013066, w0=-3.5072595643583052e-06, w1=0.19999940091611304\n",
      "Gradient Descent(26/99): loss =2194157.325848027, w0=-3.6499128909240934e-06, w1=0.19999938017597024\n",
      "Gradient Descent(27/99): loss =2193103.1122617284, w0=-3.792432705604867e-06, w1=0.19999935944047556\n",
      "Gradient Descent(28/99): loss =2192064.774808068, w0=-3.934820227362612e-06, w1=0.19999933870910974\n",
      "Gradient Descent(29/99): loss =2191041.939699228, w0=-4.0770766617180834e-06, w1=0.199999317981434\n",
      "Gradient Descent(30/99): loss =2190034.245610906, w0=-4.219203200608617e-06, w1=0.1999992972570759\n",
      "Gradient Descent(31/99): loss =2189041.3427393422, w0=-4.3612010223112714e-06, w1=0.19999927653571792\n",
      "Gradient Descent(32/99): loss =2188062.8920424995, w0=-4.503071291417676e-06, w1=0.19999925581708794\n",
      "Gradient Descent(33/99): loss =2187098.564616637, w0=-4.6448151588502586e-06, w1=0.19999923510095166\n",
      "Gradient Descent(34/99): loss =2186148.0411731135, w0=-4.786433761911968e-06, w1=0.19999921438710616\n",
      "Gradient Descent(35/99): loss =2185211.0115900435, w0=-4.927928224363183e-06, w1=0.19999919367537472\n",
      "Gradient Descent(36/99): loss =2184287.174520419, w0=-5.069299656520778e-06, w1=0.19999917296560246\n",
      "Gradient Descent(37/99): loss =2183376.2370433505, w0=-5.210549155375202e-06, w1=0.19999915225765283\n",
      "Gradient Descent(38/99): loss =2182477.914348746, w0=-5.351677804722124e-06, w1=0.19999913155140459\n",
      "Gradient Descent(39/99): loss =2181591.9294483205, w0=-5.49268667530577e-06, w1=0.1999991108467494\n",
      "Gradient Descent(40/99): loss =2180718.0129077462, w0=-5.633576824971527e-06, w1=0.1999990901435898\n",
      "Gradient Descent(41/99): loss =2179855.902596095, w0=-5.77434929882574e-06, w1=0.19999906944183746\n",
      "Gradient Descent(42/99): loss =2179005.343449746, w0=-5.915005129400995e-06, w1=0.1999990487414118\n",
      "Gradient Descent(43/99): loss =2178166.0872485475, w0=-6.055545336825375e-06, w1=0.19999902804223882\n",
      "Gradient Descent(44/99): loss =2177337.892402702, w0=-6.19597092899448e-06, w1=0.19999900734425005\n",
      "Gradient Descent(45/99): loss =2176520.523749049, w0=-6.336282901745136e-06, w1=0.1999989866473818\n",
      "Gradient Descent(46/99): loss =2175713.7523558126, w0=-6.476482239029923e-06, w1=0.1999989659515745\n",
      "Gradient Descent(47/99): loss =2174917.3553350344, w0=-6.616569913091788e-06, w1=0.19999894525677195\n",
      "Gradient Descent(48/99): loss =2174131.1156620826, w0=-6.756546884638111e-06, w1=0.1999989245629211\n",
      "Gradient Descent(49/99): loss =2173354.8220017157, w0=-6.896414103013757e-06, w1=0.1999989038699714\n",
      "Gradient Descent(50/99): loss =2172588.2685402906, w0=-7.036172506372651e-06, w1=0.19999888317787462\n",
      "Gradient Descent(51/99): loss =2171831.2548237466, w0=-7.175823021847582e-06, w1=0.19999886248658447\n",
      "Gradient Descent(52/99): loss =2171083.5856010546, w0=-7.315366565717939e-06, w1=0.1999988417960564\n",
      "Gradient Descent(53/99): loss =2170345.0706728683, w0=-7.454804043575189e-06, w1=0.19999882110624742\n",
      "Gradient Descent(54/99): loss =2169615.5247451165, w0=-7.594136350485913e-06, w1=0.1999988004171159\n",
      "Gradient Descent(55/99): loss =2168894.7672873386, w0=-7.733364371152278e-06, w1=0.19999877972862143\n",
      "Gradient Descent(56/99): loss =2168182.622395522, w0=-7.872488980069872e-06, w1=0.19999875904072467\n",
      "Gradient Descent(57/99): loss =2167478.9186593215, w0=-8.011511041682818e-06, w1=0.19999873835338736\n",
      "Gradient Descent(58/99): loss =2166783.4890334243, w0=-8.15043141053615e-06, w1=0.1999987176665721\n",
      "Gradient Descent(59/99): loss =2166096.1707129334, w0=-8.28925093142541e-06, w1=0.19999869698024236\n",
      "Gradient Descent(60/99): loss =2165416.8050126275, w0=-8.427970439543478e-06, w1=0.19999867629436238\n",
      "Gradient Descent(61/99): loss =2164745.237249922, w0=-8.566590760624654e-06, w1=0.19999865560889718\n",
      "Gradient Descent(62/99): loss =2164081.3166314363, w0=-8.70511271108599e-06, w1=0.19999863492381242\n",
      "Gradient Descent(63/99): loss =2163424.8961430117, w0=-8.843537098165923e-06, w1=0.19999861423907447\n",
      "Gradient Descent(64/99): loss =2162775.832443081, w0=-8.98186472006024e-06, w1=0.19999859355465027\n",
      "Gradient Descent(65/99): loss =2162133.985759244, w0=-9.12009636605541e-06, w1=0.1999985728705074\n",
      "Gradient Descent(66/99): loss =2161499.2197879995, w0=-9.258232816659353e-06, w1=0.19999855218661403\n",
      "Gradient Descent(67/99): loss =2160871.401597441, w0=-9.39627484372966e-06, w1=0.19999853150293884\n",
      "Gradient Descent(68/99): loss =2160250.4015329108, w0=-9.534223210599349e-06, w1=0.19999851081945108\n",
      "Gradient Descent(69/99): loss =2159636.0931254304, w0=-9.672078672200199e-06, w1=0.19999849013612056\n",
      "Gradient Descent(70/99): loss =2159028.353002891, w0=-9.809841975183712e-06, w1=0.19999846945291755\n",
      "Gradient Descent(71/99): loss =2158427.0608038683, w0=-9.947513858039759e-06, w1=0.19999844876981285\n",
      "Gradient Descent(72/99): loss =2157832.0990939774, w0=-1.0085095051212975e-05, w1=0.19999842808677773\n",
      "Gradient Descent(73/99): loss =2157243.353284718, w0=-1.0222586277216944e-05, w1=0.199998407403784\n",
      "Gradient Descent(74/99): loss =2156660.711554697, w0=-1.0359988250746241e-05, w1=0.1999983867208039\n",
      "Gradient Descent(75/99): loss =2156084.064773186, w0=-1.0497301678786372e-05, w1=0.19999836603781013\n",
      "Gradient Descent(76/99): loss =2155513.306425897, w0=-1.0634527260721674e-05, w1=0.19999834535477592\n",
      "Gradient Descent(77/99): loss =2154948.33254297, w0=-1.0771665688441225e-05, w1=0.19999832467167492\n",
      "Gradient Descent(78/99): loss =2154389.041629023, w0=-1.0908717646442802e-05, w1=0.19999830398848117\n",
      "Gradient Descent(79/99): loss =2153835.334595282, w0=-1.1045683811934955e-05, w1=0.19999828330516928\n",
      "Gradient Descent(80/99): loss =2153287.1146936775, w0=-1.1182564854937234e-05, w1=0.1999982626217142\n",
      "Gradient Descent(81/99): loss =2152744.287452849, w0=-1.1319361438378607e-05, w1=0.19999824193809135\n",
      "Gradient Descent(82/99): loss =2152206.760616017, w0=-1.1456074218194141e-05, w1=0.1999982212542766\n",
      "Gradient Descent(83/99): loss =2151674.4440806694, w0=-1.159270384341995e-05, w1=0.19999820057024625\n",
      "Gradient Descent(84/99): loss =2151147.2498399825, w0=-1.1729250956286489e-05, w1=0.19999817988597698\n",
      "Gradient Descent(85/99): loss =2150625.0919259368, w0=-1.1865716192310217e-05, w1=0.19999815920144595\n",
      "Gradient Descent(86/99): loss =2150107.886354089, w0=-1.2002100180383669e-05, w1=0.19999813851663067\n",
      "Gradient Descent(87/99): loss =2149595.5510699246, w0=-1.2138403542863979e-05, w1=0.1999981178315091\n",
      "Gradient Descent(88/99): loss =2149088.0058967704, w0=-1.2274626895659886e-05, w1=0.19999809714605954\n",
      "Gradient Descent(89/99): loss =2148585.172485197, w0=-1.2410770848317272e-05, w1=0.19999807646026083\n",
      "Gradient Descent(90/99): loss =2148086.9742638986, w0=-1.2546836004103251e-05, w1=0.19999805577409205\n",
      "Gradient Descent(91/99): loss =2147593.3363919435, w0=-1.268282296008885e-05, w1=0.19999803508753278\n",
      "Gradient Descent(92/99): loss =2147104.185712435, w0=-1.2818732307230316e-05, w1=0.19999801440056292\n",
      "Gradient Descent(93/99): loss =2146619.450707491, w0=-1.2954564630449075e-05, w1=0.19999799371316282\n",
      "Gradient Descent(94/99): loss =2146139.061454499, w0=-1.3090320508710384e-05, w1=0.19999797302531314\n",
      "Gradient Descent(95/99): loss =2145662.9495836417, w0=-1.3226000515100688e-05, w1=0.19999795233699494\n",
      "Gradient Descent(96/99): loss =2145191.0482366197, w0=-1.336160521690373e-05, w1=0.1999979316481897\n",
      "Gradient Descent(97/99): loss =2144723.292026579, w0=-1.3497135175675428e-05, w1=0.19999791095887914\n",
      "Gradient Descent(98/99): loss =2144259.616999146, w0=-1.3632590947317557e-05, w1=0.1999978902690455\n",
      "Gradient Descent(99/99): loss =2143799.960594612, w0=-1.3767973082150241e-05, w1=0.19999786957867124\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =2.602390810089267e+30, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =8121358804842121.0, w0=-0.19999998580208572, w1=0.20000002830509006\n",
      "Gradient Descent(3/99): loss =8055412549087775.0, w0=-0.1999999725988469, w1=0.20000005601434862\n",
      "Gradient Descent(4/99): loss =8005290069941447.0, w0=-0.19999995933536505, w1=0.2000000833455108\n",
      "Gradient Descent(5/99): loss =7957990457278428.0, w0=-0.1999999462702332, w1=0.20000011040866214\n",
      "Gradient Descent(6/99): loss =7912381858136464.0, w0=-0.19999993327757942, w1=0.20000013722794793\n",
      "Gradient Descent(7/99): loss =7868218592580738.0, w0=-0.19999992039558875, w1=0.2000001638346299\n",
      "Gradient Descent(8/99): loss =7825384002250979.0, w0=-0.19999990760058525, w1=0.20000019024651455\n",
      "Gradient Descent(9/99): loss =7783794630838951.0, w0=-0.19999989489401332, w1=0.20000021647965857\n",
      "Gradient Descent(10/99): loss =7743378901563364.0, w0=-0.1999998822683099, w1=0.20000024254562648\n",
      "Gradient Descent(11/99): loss =7704073999888617.0, w0=-0.1999998697200337, w1=0.2000002684538164\n",
      "Gradient Descent(12/99): loss =7665823925064800.0, w0=-0.19999985724469968, w1=0.20000029421150137\n",
      "Gradient Descent(13/99): loss =7628578480759437.0, w0=-0.19999984483870706, w1=0.20000031982452388\n",
      "Gradient Descent(14/99): loss =7592292390880122.0, w0=-0.19999983249854095, w1=0.20000034529756608\n",
      "Gradient Descent(15/99): loss =7556924603873140.0, w0=-0.19999982022103355, w1=0.2000003706344532\n",
      "Gradient Descent(16/99): loss =7522437705720090.0, w0=-0.1999998080032249, w1=0.20000039583834575\n",
      "Gradient Descent(17/99): loss =7488797430951015.0, w0=-0.19999979584238312, w1=0.20000042091190015\n",
      "Gradient Descent(18/99): loss =7455972250054097.0, w0=-0.19999978373596802, w1=0.20000044585738508\n",
      "Gradient Descent(19/99): loss =7423933020671968.0, w0=-0.19999977168162014, w1=0.2000004706767722\n",
      "Gradient Descent(20/99): loss =7392652691297299.0, w0=-0.19999975967714295, w1=0.2000004953718043\n",
      "Gradient Descent(21/99): loss =7362106048793399.0, w0=-0.19999974772049037, w1=0.2000005199440476\n",
      "Gradient Descent(22/99): loss =7332269502583603.0, w0=-0.1999997358097542, w1=0.20000054439493153\n",
      "Gradient Descent(23/99): loss =7303120899689756.0, w0=-0.19999972394315332, w1=0.20000056872577904\n",
      "Gradient Descent(24/99): loss =7274639365811630.0, w0=-0.19999971211902398, w1=0.20000059293782985\n",
      "Gradient Descent(25/99): loss =7246805168462406.0, w0=-0.1999997003358107, w1=0.20000061703225813\n",
      "Gradient Descent(26/99): loss =7219599598835960.0, w0=-0.19999968859205813, w1=0.2000006410101862\n",
      "Gradient Descent(27/99): loss =7193004869621273.0, w0=-0.1999996768864038, w1=0.20000066487269474\n",
      "Gradient Descent(28/99): loss =7167004026421441.0, w0=-0.19999966521757104, w1=0.20000068862083098\n",
      "Gradient Descent(29/99): loss =7141580870800601.0, w0=-0.1999996535843629, w1=0.20000071225561455\n",
      "Gradient Descent(30/99): loss =7116719893285262.0, w0=-0.19999964198565628, w1=0.20000073577804242\n",
      "Gradient Descent(31/99): loss =7092406214900426.0, w0=-0.19999963042039673, w1=0.2000007591890923\n",
      "Gradient Descent(32/99): loss =7068625536032886.0, w0=-0.19999961888759338, w1=0.2000007824897255\n",
      "Gradient Descent(33/99): loss =7045364091593240.0, w0=-0.19999960738631456, w1=0.20000080568088896\n",
      "Gradient Descent(34/99): loss =7022608611598752.0, w0=-0.19999959591568356, w1=0.20000082876351694\n",
      "Gradient Descent(35/99): loss =7000346286427210.0, w0=-0.19999958447487473, w1=0.20000085173853221\n",
      "Gradient Descent(36/99): loss =6978564736100025.0, w0=-0.19999957306310992, w1=0.200000874606847\n",
      "Gradient Descent(37/99): loss =6957251983044967.0, w0=-0.19999956167965513, w1=0.20000089736936374\n",
      "Gradient Descent(38/99): loss =6936396427867651.0, w0=-0.1999995503238175, w1=0.2000009200269755\n",
      "Gradient Descent(39/99): loss =6915986827727437.0, w0=-0.19999953899494238, w1=0.20000094258056655\n",
      "Gradient Descent(40/99): loss =6896012276970809.0, w0=-0.19999952769241072, w1=0.20000096503101253\n",
      "Gradient Descent(41/99): loss =6876462189723904.0, w0=-0.19999951641563662, w1=0.20000098737918076\n",
      "Gradient Descent(42/99): loss =6857326284188056.0, w0=-0.19999950516406503, w1=0.2000010096259304\n",
      "Gradient Descent(43/99): loss =6838594568417547.0, w0=-0.19999949393716968, w1=0.20000103177211254\n",
      "Gradient Descent(44/99): loss =6820257327389841.0, w0=-0.19999948273445106, w1=0.2000010538185703\n",
      "Gradient Descent(45/99): loss =6802305111204650.0, w0=-0.19999947155543468, w1=0.20000107576613896\n",
      "Gradient Descent(46/99): loss =6784728724270958.0, w0=-0.1999994603996693, w1=0.20000109761564588\n",
      "Gradient Descent(47/99): loss =6767519215360370.0, w0=-0.19999944926672542, w1=0.2000011193679107\n",
      "Gradient Descent(48/99): loss =6750667868421736.0, w0=-0.19999943815619384, w1=0.20000114102374525\n",
      "Gradient Descent(49/99): loss =6734166194066718.0, w0=-0.19999942706768428, w1=0.20000116258395356\n",
      "Gradient Descent(50/99): loss =6718005921647754.0, w0=-0.1999994160008241, w1=0.20000118404933198\n",
      "Gradient Descent(51/99): loss =6702178991860768.0, w0=-0.19999940495525723, w1=0.20000120542066913\n",
      "Gradient Descent(52/99): loss =6686677549813962.0, w0=-0.19999939393064295, w1=0.2000012266987459\n",
      "Gradient Descent(53/99): loss =6671493938511976.0, w0=-0.19999938292665503, w1=0.20000124788433551\n",
      "Gradient Descent(54/99): loss =6656620692711197.0, w0=-0.19999937194298073, w1=0.20000126897820353\n",
      "Gradient Descent(55/99): loss =6642050533108092.0, w0=-0.19999936097931995, w1=0.20000128998110783\n",
      "Gradient Descent(56/99): loss =6627776360827265.0, w0=-0.19999935003538435, w1=0.20000131089379874\n",
      "Gradient Descent(57/99): loss =6613791252180163.0, w0=-0.19999933911089676, w1=0.20000133171701895\n",
      "Gradient Descent(58/99): loss =6600088453669423.0, w0=-0.19999932820559035, w1=0.20000135245150355\n",
      "Gradient Descent(59/99): loss =6586661377216490.0, w0=-0.19999931731920806, w1=0.2000013730979802\n",
      "Gradient Descent(60/99): loss =6573503595593551.0, w0=-0.19999930645150196, w1=0.20000139365716893\n",
      "Gradient Descent(61/99): loss =6560608838042696.0, w0=-0.19999929560223273, w1=0.20000141412978242\n",
      "Gradient Descent(62/99): loss =6547970986067322.0, w0=-0.19999928477116907, w1=0.20000143451652588\n",
      "Gradient Descent(63/99): loss =6535584069383189.0, w0=-0.19999927395808734, w1=0.20000145481809714\n",
      "Gradient Descent(64/99): loss =6523442262016995.0, w0=-0.19999926316277097, w1=0.20000147503518673\n",
      "Gradient Descent(65/99): loss =6511539878542729.0, w0=-0.1999992523850102, w1=0.20000149516847784\n",
      "Gradient Descent(66/99): loss =6499871370446564.0, w0=-0.19999924162460153, w1=0.20000151521864645\n",
      "Gradient Descent(67/99): loss =6488431322612225.0, w0=-0.1999992308813475, w1=0.20000153518636138\n",
      "Gradient Descent(68/99): loss =6477214449919704.0, w0=-0.19999922015505625, w1=0.20000155507228426\n",
      "Gradient Descent(69/99): loss =6466215593950913.0, w0=-0.19999920944554128, w1=0.2000015748770697\n",
      "Gradient Descent(70/99): loss =6455429719796412.0, w0=-0.19999919875262115, w1=0.20000159460136527\n",
      "Gradient Descent(71/99): loss =6444851912958045.0, w0=-0.19999918807611913, w1=0.20000161424581156\n",
      "Gradient Descent(72/99): loss =6434477376342834.0, w0=-0.19999917741586307, w1=0.20000163381104225\n",
      "Gradient Descent(73/99): loss =6424301427343716.0, w0=-0.19999916677168506, w1=0.2000016532976842\n",
      "Gradient Descent(74/99): loss =6414319495003373.0, w0=-0.19999915614342129, w1=0.2000016727063575\n",
      "Gradient Descent(75/99): loss =6404527117257439.0, w0=-0.19999914553091178, w1=0.2000016920376754\n",
      "Gradient Descent(76/99): loss =6394919938253939.0, w0=-0.19999913493400026, w1=0.20000171129224464\n",
      "Gradient Descent(77/99): loss =6385493705745867.0, w0=-0.19999912435253392, w1=0.20000173047066527\n",
      "Gradient Descent(78/99): loss =6376244268554054.0, w0=-0.19999911378636331, w1=0.2000017495735308\n",
      "Gradient Descent(79/99): loss =6367167574097889.0, w0=-0.19999910323534212, w1=0.20000176860142835\n",
      "Gradient Descent(80/99): loss =6358259665991376.0, w0=-0.19999909269932706, w1=0.2000017875549385\n",
      "Gradient Descent(81/99): loss =6349516681702158.0, w0=-0.19999908217817777, w1=0.20000180643463558\n",
      "Gradient Descent(82/99): loss =6340934850271641.0, w0=-0.19999907167175668, w1=0.20000182524108762\n",
      "Gradient Descent(83/99): loss =6332510490093972.0, w0=-0.1999990611799288, w1=0.20000184397485643\n",
      "Gradient Descent(84/99): loss =6324240006752223.0, w0=-0.19999905070256171, w1=0.20000186263649763\n",
      "Gradient Descent(85/99): loss =6316119890909744.0, w0=-0.19999904023952544, w1=0.2000018812265608\n",
      "Gradient Descent(86/99): loss =6308146716255185.0, w0=-0.19999902979069234, w1=0.20000189974558946\n",
      "Gradient Descent(87/99): loss =6300317137499585.0, w0=-0.19999901935593703, w1=0.2000019181941212\n",
      "Gradient Descent(88/99): loss =6292627888423802.0, w0=-0.19999900893513625, w1=0.20000193657268767\n",
      "Gradient Descent(89/99): loss =6285075779975057.0, w0=-0.19999899852816888, w1=0.20000195488181471\n",
      "Gradient Descent(90/99): loss =6277657698411114.0, w0=-0.1999989881349158, w1=0.20000197312202242\n",
      "Gradient Descent(91/99): loss =6270370603490615.0, w0=-0.19999897775525974, w1=0.20000199129382515\n",
      "Gradient Descent(92/99): loss =6263211526708501.0, w0=-0.19999896738908543, w1=0.2000020093977316\n",
      "Gradient Descent(93/99): loss =6256177569575120.0, w0=-0.19999895703627935, w1=0.20000202743424494\n",
      "Gradient Descent(94/99): loss =6249265901938000.0, w0=-0.1999989466967297, w1=0.2000020454038628\n",
      "Gradient Descent(95/99): loss =6242473760344865.0, w0=-0.19999893637032645, w1=0.2000020633070773\n",
      "Gradient Descent(96/99): loss =6235798446447098.0, w0=-0.19999892605696112, w1=0.2000020811443752\n",
      "Gradient Descent(97/99): loss =6229237325442364.0, w0=-0.1999989157565269, w1=0.20000209891623794\n",
      "Gradient Descent(98/99): loss =6222787824555467.0, w0=-0.1999989054689185, w1=0.20000211662314166\n",
      "Gradient Descent(99/99): loss =6216447431556379.0, w0=-0.19999889519403213, w1=0.2000021342655573\n",
      "Optimizing degree 10/15, model: least_squares_GD, arguments: {'max_iters': 100}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =4.219572666932663e+34, w0=-0.20000000000000004, w1=0.2\n",
      "Gradient Descent(2/99): loss =2.4994839622918963e+18, w0=-0.19999999822097617, w1=0.20000000342261853\n",
      "Gradient Descent(3/99): loss =2.490826309806517e+18, w0=-0.19999999647627104, w1=0.20000000683114774\n",
      "Gradient Descent(4/99): loss =2.482738884333112e+18, w0=-0.19999999473697191, w1=0.2000000102210129\n",
      "Gradient Descent(5/99): loss =2.4747796946624717e+18, w0=-0.19999999300226468, w1=0.2000000135941329\n",
      "Gradient Descent(6/99): loss =2.466943269485469e+18, w0=-0.19999999127184334, w1=0.20000001695191805\n",
      "Gradient Descent(7/99): loss =2.459226265841292e+18, w0=-0.1999999895455545, w1=0.20000002029549524\n",
      "Gradient Descent(8/99): loss =2.451625660856275e+18, w0=-0.19999998782327275, w1=0.200000023625769\n",
      "Gradient Descent(9/99): loss =2.4441386797938534e+18, w0=-0.19999998610488617, w1=0.20000002694346813\n",
      "Gradient Descent(10/99): loss =2.436762756523328e+18, w0=-0.19999998439029312, w1=0.2000000302491823\n",
      "Gradient Descent(11/99): loss =2.4294955018743875e+18, w0=-0.1999999826794005, w1=0.20000003354339072\n",
      "Gradient Descent(12/99): loss =2.4223346778641116e+18, w0=-0.19999998097212265, w1=0.20000003682648496\n",
      "Gradient Descent(13/99): loss =2.415278176541745e+18, w0=-0.19999997926838037, w1=0.20000004009878708\n",
      "Gradient Descent(14/99): loss =2.4083240025071247e+18, w0=-0.19999997756810012, w1=0.2000000433605639\n",
      "Gradient Descent(15/99): loss =2.401470258380033e+18, w0=-0.19999997587121338, w1=0.2000000466120384\n",
      "Gradient Descent(16/99): loss =2.3947151326600986e+18, w0=-0.19999997417765605, w1=0.2000000498533987\n",
      "Gradient Descent(17/99): loss =2.388056889537702e+18, w0=-0.19999997248736806, w1=0.20000005308480526\n",
      "Gradient Descent(18/99): loss =2.3814938603075195e+18, w0=-0.19999997080029291, w1=0.20000005630639647\n",
      "Gradient Descent(19/99): loss =2.375024436106008e+18, w0=-0.19999996911637735, w1=0.2000000595182933\n",
      "Gradient Descent(20/99): loss =2.368647061747807e+18, w0=-0.19999996743557108, w1=0.20000006272060272\n",
      "Gradient Descent(21/99): loss =2.3623602304783334e+18, w0=-0.19999996575782644, w1=0.20000006591342062\n",
      "Gradient Descent(22/99): loss =2.3561624794928466e+18, w0=-0.19999996408309828, w1=0.20000006909683407\n",
      "Gradient Descent(23/99): loss =2.3500523860989066e+18, w0=-0.19999996241134368, w1=0.20000007227092306\n",
      "Gradient Descent(24/99): loss =2.344028564420355e+18, w0=-0.1999999607425218, w1=0.200000075435762\n",
      "Gradient Descent(25/99): loss =2.33808966255824e+18, w0=-0.19999995907659365, w1=0.20000007859142083\n",
      "Gradient Descent(26/99): loss =2.332234360138053e+18, w0=-0.1999999574135221, w1=0.2000000817379658\n",
      "Gradient Descent(27/99): loss =2.326461366184364e+18, w0=-0.1999999557532716, w1=0.20000008487546034\n",
      "Gradient Descent(28/99): loss =2.3207694172732073e+18, w0=-0.19999995409580806, w1=0.20000008800396554\n",
      "Gradient Descent(29/99): loss =2.31515727592064e+18, w0=-0.1999999524410989, w1=0.2000000911235406\n",
      "Gradient Descent(30/99): loss =2.309623729172303e+18, w0=-0.19999995078911273, w1=0.20000009423424325\n",
      "Gradient Descent(31/99): loss =2.304167587364375e+18, w0=-0.19999994913981947, w1=0.20000009733612995\n",
      "Gradient Descent(32/99): loss =2.2987876830307732e+18, w0=-0.19999994749319014, w1=0.2000001004292561\n",
      "Gradient Descent(33/99): loss =2.293482869935383e+18, w0=-0.19999994584919678, w1=0.20000010351367628\n",
      "Gradient Descent(34/99): loss =2.2882520222112735e+18, w0=-0.19999994420781242, w1=0.2000001065894444\n",
      "Gradient Descent(35/99): loss =2.2830940335915963e+18, w0=-0.19999994256901102, w1=0.2000001096566137\n",
      "Gradient Descent(36/99): loss =2.2780078167191488e+18, w0=-0.19999994093276738, w1=0.2000001127152369\n",
      "Gradient Descent(37/99): loss =2.2729923025235295e+18, w0=-0.1999999392990571, w1=0.20000011576536633\n",
      "Gradient Descent(38/99): loss =2.268046439656466e+18, w0=-0.19999993766785656, w1=0.20000011880705387\n",
      "Gradient Descent(39/99): loss =2.2631691939772736e+18, w0=-0.1999999360391428, w1=0.20000012184035104\n",
      "Gradient Descent(40/99): loss =2.2583595480816172e+18, w0=-0.1999999344128935, w1=0.200000124865309\n",
      "Gradient Descent(41/99): loss =2.2536165008677023e+18, w0=-0.19999993278908698, w1=0.2000001278819786\n",
      "Gradient Descent(42/99): loss =2.248939067134931e+18, w0=-0.19999993116770215, w1=0.20000013089041044\n",
      "Gradient Descent(43/99): loss =2.2443262772107743e+18, w0=-0.19999992954871845, w1=0.20000013389065477\n",
      "Gradient Descent(44/99): loss =2.2397771766021757e+18, w0=-0.1999999279321158, w1=0.20000013688276155\n",
      "Gradient Descent(45/99): loss =2.2352908256684283e+18, w0=-0.1999999263178746, w1=0.20000013986678053\n",
      "Gradient Descent(46/99): loss =2.2308662993128172e+18, w0=-0.19999992470597577, w1=0.20000014284276113\n",
      "Gradient Descent(47/99): loss =2.226502686690753e+18, w0=-0.19999992309640058, w1=0.2000001458107525\n",
      "Gradient Descent(48/99): loss =2.2221990909324756e+18, w0=-0.1999999214891307, w1=0.20000014877080355\n",
      "Gradient Descent(49/99): loss =2.217954628878614e+18, w0=-0.19999991988414823, w1=0.2000001517229629\n",
      "Gradient Descent(50/99): loss =2.2137684308271762e+18, w0=-0.19999991828143557, w1=0.20000015466727888\n",
      "Gradient Descent(51/99): loss =2.2096396402907374e+18, w0=-0.19999991668097547, w1=0.20000015760379958\n",
      "Gradient Descent(52/99): loss =2.2055674137627658e+18, w0=-0.19999991508275103, w1=0.2000001605325728\n",
      "Gradient Descent(53/99): loss =2.2015509204921636e+18, w0=-0.1999999134867456, w1=0.20000016345364605\n",
      "Gradient Descent(54/99): loss =2.1975893422652562e+18, w0=-0.19999991189294286, w1=0.20000016636706658\n",
      "Gradient Descent(55/99): loss =2.1936818731945044e+18, w0=-0.19999991030132674, w1=0.20000016927288133\n",
      "Gradient Descent(56/99): loss =2.1898277195134205e+18, w0=-0.19999990871188145, w1=0.200000172171137\n",
      "Gradient Descent(57/99): loss =2.1860260993771164e+18, w0=-0.1999999071245914, w1=0.20000017506187998\n",
      "Gradient Descent(58/99): loss =2.1822762426680814e+18, w0=-0.19999990553944128, w1=0.20000017794515637\n",
      "Gradient Descent(59/99): loss =2.17857739080679e+18, w0=-0.199999903956416, w1=0.200000180821012\n",
      "Gradient Descent(60/99): loss =2.1749287965668224e+18, w0=-0.19999990237550067, w1=0.2000001836894924\n",
      "Gradient Descent(61/99): loss =2.1713297238941783e+18, w0=-0.1999999007966806, w1=0.2000001865506428\n",
      "Gradient Descent(62/99): loss =2.167779447730564e+18, w0=-0.1999998992199413, w1=0.20000018940450814\n",
      "Gradient Descent(63/99): loss =2.1642772538404093e+18, w0=-0.19999989764526846, w1=0.2000001922511331\n",
      "Gradient Descent(64/99): loss =2.1608224386414024e+18, w0=-0.19999989607264798, w1=0.20000019509056205\n",
      "Gradient Descent(65/99): loss =2.1574143090384243e+18, w0=-0.1999998945020659, w1=0.20000019792283905\n",
      "Gradient Descent(66/99): loss =2.1540521822606467e+18, w0=-0.19999989293350845, w1=0.20000020074800787\n",
      "Gradient Descent(67/99): loss =2.150735385701727e+18, w0=-0.19999989136696203, w1=0.20000020356611203\n",
      "Gradient Descent(68/99): loss =2.1474632567629594e+18, w0=-0.19999988980241315, w1=0.20000020637719473\n",
      "Gradient Descent(69/99): loss =2.1442351426992256e+18, w0=-0.1999998882398485, w1=0.20000020918129885\n",
      "Gradient Descent(70/99): loss =2.1410504004677233e+18, w0=-0.1999998866792549, w1=0.20000021197846704\n",
      "Gradient Descent(71/99): loss =2.1379083965793103e+18, w0=-0.19999988512061934, w1=0.2000002147687416\n",
      "Gradient Descent(72/99): loss =2.134808506952436e+18, w0=-0.19999988356392892, w1=0.20000021755216457\n",
      "Gradient Descent(73/99): loss =2.131750116769544e+18, w0=-0.19999988200917088, w1=0.2000002203287777\n",
      "Gradient Descent(74/99): loss =2.128732620335903e+18, w0=-0.19999988045633257, w1=0.20000022309862245\n",
      "Gradient Descent(75/99): loss =2.1257554209407992e+18, w0=-0.19999987890540152, w1=0.20000022586173996\n",
      "Gradient Descent(76/99): loss =2.1228179307210056e+18, w0=-0.19999987735636532, w1=0.20000022861817113\n",
      "Gradient Descent(77/99): loss =2.1199195705265167e+18, w0=-0.1999998758092117, w1=0.20000023136795655\n",
      "Gradient Descent(78/99): loss =2.117059769788448e+18, w0=-0.19999987426392848, w1=0.20000023411113654\n",
      "Gradient Descent(79/99): loss =2.114237966389094e+18, w0=-0.19999987272050365, w1=0.20000023684775112\n",
      "Gradient Descent(80/99): loss =2.111453606534061e+18, w0=-0.19999987117892526, w1=0.20000023957784002\n",
      "Gradient Descent(81/99): loss =2.1087061446264732e+18, w0=-0.1999998696391815, w1=0.20000024230144273\n",
      "Gradient Descent(82/99): loss =2.1059950431431606e+18, w0=-0.19999986810126066, w1=0.20000024501859842\n",
      "Gradient Descent(83/99): loss =2.1033197725128338e+18, w0=-0.1999998665651511, w1=0.200000247729346\n",
      "Gradient Descent(84/99): loss =2.1006798109961797e+18, w0=-0.1999998650308413, w1=0.2000002504337241\n",
      "Gradient Descent(85/99): loss =2.0980746445678492e+18, w0=-0.19999986349831986, w1=0.20000025313177106\n",
      "Gradient Descent(86/99): loss =2.0955037668003085e+18, w0=-0.19999986196757544, w1=0.20000025582352496\n",
      "Gradient Descent(87/99): loss =2.0929666787495053e+18, w0=-0.19999986043859683, w1=0.20000025850902364\n",
      "Gradient Descent(88/99): loss =2.0904628888423386e+18, w0=-0.1999998589113729, w1=0.2000002611883046\n",
      "Gradient Descent(89/99): loss =2.0879919127658678e+18, w0=-0.1999998573858926, w1=0.20000026386140513\n",
      "Gradient Descent(90/99): loss =2.085553273358273e+18, w0=-0.199999855862145, w1=0.20000026652836222\n",
      "Gradient Descent(91/99): loss =2.083146500501493e+18, w0=-0.19999985434011922, w1=0.20000026918921263\n",
      "Gradient Descent(92/99): loss =2.0807711310155397e+18, w0=-0.1999998528198045, w1=0.20000027184399277\n",
      "Gradient Descent(93/99): loss =2.0784267085544522e+18, w0=-0.19999985130119016, w1=0.2000002744927389\n",
      "Gradient Descent(94/99): loss =2.0761127835038636e+18, w0=-0.19999984978426558, w1=0.200000277135487\n",
      "Gradient Descent(95/99): loss =2.073828912880147e+18, w0=-0.19999984826902026, w1=0.20000027977227267\n",
      "Gradient Descent(96/99): loss =2.0715746602311245e+18, w0=-0.19999984675544377, w1=0.2000002824031314\n",
      "Gradient Descent(97/99): loss =2.0693495955383194e+18, w0=-0.19999984524352574, w1=0.20000028502809836\n",
      "Gradient Descent(98/99): loss =2.0671532951206738e+18, w0=-0.1999998437332559, w1=0.20000028764720848\n",
      "Gradient Descent(99/99): loss =2.0649853415398152e+18, w0=-0.1999998422246241, w1=0.2000002902604964\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =4.219572666917086e+34, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =1.7987085187672625e+18, w0=-0.199999999036899, w1=0.20000000009935573\n",
      "Gradient Descent(3/99): loss =1.7563123337731663e+18, w0=-0.19999999821479053, w1=0.19999999989271294\n",
      "Gradient Descent(4/99): loss =1.7293951651973489e+18, w0=-0.1999999974304755, w1=0.19999999949562564\n",
      "Gradient Descent(5/99): loss =1.710011688548291e+18, w0=-0.1999999966776841, w1=0.1999999989419319\n",
      "Gradient Descent(6/99): loss =1.6960478179827963e+18, w0=-0.19999999595152582, w1=0.19999999825778633\n",
      "Gradient Descent(7/99): loss =1.68598455327144e+18, w0=-0.19999999524791237, w1=0.1999999974642017\n",
      "Gradient Descent(8/99): loss =1.6787295555026806e+18, w0=-0.19999999456340495, w1=0.19999999657839052\n",
      "Gradient Descent(9/99): loss =1.6734968622229932e+18, w0=-0.1999999938951013, w1=0.1999999956146353\n",
      "Gradient Descent(10/99): loss =1.6697207273825618e+18, w0=-0.19999999324054676, w1=0.19999999458488707\n",
      "Gradient Descent(11/99): loss =1.6669938015017766e+18, w0=-0.1999999925976624, w1=0.1999999934991995\n",
      "Gradient Descent(12/99): loss =1.6650227229234504e+18, w0=-0.19999999196468612, w1=0.19999999236605875\n",
      "Gradient Descent(13/99): loss =1.663596189109879e+18, w0=-0.19999999134012356, w1=0.1999999911926431\n",
      "Gradient Descent(14/99): loss =1.66256198661571e+18, w0=-0.19999999072270697, w1=0.19999998998503257\n",
      "Gradient Descent(15/99): loss =1.6618104587494167e+18, w0=-0.19999999011136066, w1=0.19999998874838112\n",
      "Gradient Descent(16/99): loss =1.6612626028656722e+18, w0=-0.19999998950517184, w1=0.19999998748706002\n",
      "Gradient Descent(17/99): loss =1.6608614988585846e+18, w0=-0.1999999889033658, w1=0.1999999862047779\n",
      "Gradient Descent(18/99): loss =1.6605661355318034e+18, w0=-0.199999988305285, w1=0.19999998490468182\n",
      "Gradient Descent(19/99): loss =1.6603469635086769e+18, w0=-0.19999998771037147, w1=0.19999998358944257\n",
      "Gradient Descent(20/99): loss =1.6601826915560172e+18, w0=-0.19999998711815153, w1=0.19999998226132704\n",
      "Gradient Descent(21/99): loss =1.6600579785177096e+18, w0=-0.1999999865282232, w1=0.19999998092225926\n",
      "Gradient Descent(22/99): loss =1.659961770409834e+18, w0=-0.1999999859402453, w1=0.19999997957387247\n",
      "Gradient Descent(23/99): loss =1.6598861023000742e+18, w0=-0.1999999853539282, w1=0.19999997821755292\n",
      "Gradient Descent(24/99): loss =1.6598252350434294e+18, w0=-0.19999998476902622, w1=0.19999997685447737\n",
      "Gradient Descent(25/99): loss =1.659775033276591e+18, w0=-0.19999998418533071, w1=0.1999999754856447\n",
      "Gradient Descent(26/99): loss =1.6597325172403791e+18, w0=-0.19999998360266466, w1=0.19999997411190273\n",
      "Gradient Descent(27/99): loss =1.6596955398487537e+18, w0=-0.19999998302087785, w1=0.19999997273397124\n",
      "Gradient Descent(28/99): loss =1.6596625540018007e+18, w0=-0.19999998243984277, w1=0.19999997135246117\n",
      "Gradient Descent(29/99): loss =1.6596324449230272e+18, w0=-0.19999998185945123, w1=0.19999996996789113\n",
      "Gradient Descent(30/99): loss =1.6596044093494956e+18, w0=-0.1999999812796114, w1=0.19999996858070132\n",
      "Gradient Descent(31/99): loss =1.659577868481737e+18, w0=-0.19999998070024544, w1=0.1999999671912654\n",
      "Gradient Descent(32/99): loss =1.6595524052592814e+18, w0=-0.19999998012128722, w1=0.19999996579990056\n",
      "Gradient Descent(33/99): loss =1.6595277191640993e+18, w0=-0.19999997954268067, w1=0.1999999644068761\n",
      "Gradient Descent(34/99): loss =1.6595035936538972e+18, w0=-0.1999999789643782, w1=0.19999996301242054\n",
      "Gradient Descent(35/99): loss =1.6594798726958945e+18, w0=-0.19999997838633948, w1=0.1999999616167279\n",
      "Gradient Descent(36/99): loss =1.6594564438580367e+18, w0=-0.1999999778085303, w1=0.19999996021996286\n",
      "Gradient Descent(37/99): loss =1.6594332261252285e+18, w0=-0.19999997723092158, w1=0.1999999588222652\n",
      "Gradient Descent(38/99): loss =1.6594101611201892e+18, w0=-0.19999997665348876, w1=0.1999999574237536\n",
      "Gradient Descent(39/99): loss =1.6593872067775744e+18, w0=-0.19999997607621095, w1=0.19999995602452875\n",
      "Gradient Descent(40/99): loss =1.6593643327857836e+18, w0=-0.19999997549907048, w1=0.19999995462467618\n",
      "Gradient Descent(41/99): loss =1.6593415173025377e+18, w0=-0.19999997492205232, w1=0.19999995322426845\n",
      "Gradient Descent(42/99): loss =1.6593187445882194e+18, w0=-0.19999997434514383, w1=0.19999995182336716\n",
      "Gradient Descent(43/99): loss =1.6592960033005793e+18, w0=-0.1999999737683342, w1=0.19999995042202462\n",
      "Gradient Descent(44/99): loss =1.6592732852659684e+18, w0=-0.19999997319161436, w1=0.1999999490202852\n",
      "Gradient Descent(45/99): loss =1.659250584593906e+18, w0=-0.1999999726149766, w1=0.1999999476181866\n",
      "Gradient Descent(46/99): loss =1.6592278970391089e+18, w0=-0.1999999720384144, w1=0.19999994621576075\n",
      "Gradient Descent(47/99): loss =1.6592052195417795e+18, w0=-0.19999997146192225, w1=0.19999994481303485\n",
      "Gradient Descent(48/99): loss =1.6591825498963453e+18, w0=-0.19999997088549548, w1=0.1999999434100319\n",
      "Gradient Descent(49/99): loss =1.659159886512774e+18, w0=-0.19999997030913017, w1=0.1999999420067715\n",
      "Gradient Descent(50/99): loss =1.6591372282445914e+18, w0=-0.199999969732823, w1=0.1999999406032702\n",
      "Gradient Descent(51/99): loss =1.65911457426495e+18, w0=-0.19999996915657117, w1=0.19999993919954212\n",
      "Gradient Descent(52/99): loss =1.6590919239773202e+18, w0=-0.19999996858037236, w1=0.19999993779559921\n",
      "Gradient Descent(53/99): loss =1.6590692769511606e+18, w0=-0.19999996800422457, w1=0.19999993639145164\n",
      "Gradient Descent(54/99): loss =1.6590466328755361e+18, w0=-0.19999996742812612, w1=0.199999934987108\n",
      "Gradient Descent(55/99): loss =1.6590239915257236e+18, w0=-0.19999996685207563, w1=0.1999999335825756\n",
      "Gradient Descent(56/99): loss =1.659001352739123e+18, w0=-0.19999996627607197, w1=0.19999993217786063\n",
      "Gradient Descent(57/99): loss =1.6589787163979154e+18, w0=-0.19999996570011414, w1=0.1999999307729684\n",
      "Gradient Descent(58/99): loss =1.6589560824165665e+18, w0=-0.19999996512420135, w1=0.19999992936790337\n",
      "Gradient Descent(59/99): loss =1.6589334507328164e+18, w0=-0.19999996454833294, w1=0.19999992796266933\n",
      "Gradient Descent(60/99): loss =1.658910821301182e+18, w0=-0.19999996397250833, w1=0.1999999265572695\n",
      "Gradient Descent(61/99): loss =1.6588881940882883e+18, w0=-0.1999999633967271, w1=0.19999992515170661\n",
      "Gradient Descent(62/99): loss =1.6588655690694932e+18, w0=-0.1999999628209889, w1=0.19999992374598297\n",
      "Gradient Descent(63/99): loss =1.6588429462264576e+18, w0=-0.1999999622452934, w1=0.19999992234010056\n",
      "Gradient Descent(64/99): loss =1.6588203255453967e+18, w0=-0.1999999616696404, w1=0.19999992093406102\n",
      "Gradient Descent(65/99): loss =1.6587977070158154e+18, w0=-0.1999999610940297, w1=0.1999999195278658\n",
      "Gradient Descent(66/99): loss =1.6587750906295977e+18, w0=-0.19999996051846117, w1=0.1999999181215161\n",
      "Gradient Descent(67/99): loss =1.658752476380357e+18, w0=-0.19999995994293468, w1=0.19999991671501294\n",
      "Gradient Descent(68/99): loss =1.6587298642629553e+18, w0=-0.1999999593674502, w1=0.19999991530835715\n",
      "Gradient Descent(69/99): loss =1.6587072542731681e+18, w0=-0.19999995879200763, w1=0.1999999139015495\n",
      "Gradient Descent(70/99): loss =1.6586846464074317e+18, w0=-0.199999958216607, w1=0.1999999124945906\n",
      "Gradient Descent(71/99): loss =1.658662040662676e+18, w0=-0.19999995764124823, w1=0.19999991108748097\n",
      "Gradient Descent(72/99): loss =1.658639437036188e+18, w0=-0.19999995706593138, w1=0.19999990968022108\n",
      "Gradient Descent(73/99): loss =1.6586168355255124e+18, w0=-0.19999995649065644, w1=0.19999990827281128\n",
      "Gradient Descent(74/99): loss =1.6585942361284058e+18, w0=-0.19999995591542344, w1=0.1999999068652519\n",
      "Gradient Descent(75/99): loss =1.6585716388427638e+18, w0=-0.19999995534023243, w1=0.19999990545754323\n",
      "Gradient Descent(76/99): loss =1.6585490436666079e+18, w0=-0.19999995476508345, w1=0.1999999040496855\n",
      "Gradient Descent(77/99): loss =1.6585264505980424e+18, w0=-0.19999995418997651, w1=0.1999999026416789\n",
      "Gradient Descent(78/99): loss =1.6585038596352438e+18, w0=-0.1999999536149117, w1=0.1999999012335236\n",
      "Gradient Descent(79/99): loss =1.6584812707764447e+18, w0=-0.19999995303988907, w1=0.19999989982521973\n",
      "Gradient Descent(80/99): loss =1.658458684019928e+18, w0=-0.19999995246490868, w1=0.19999989841676744\n",
      "Gradient Descent(81/99): loss =1.6584360993640118e+18, w0=-0.19999995188997058, w1=0.19999989700816678\n",
      "Gradient Descent(82/99): loss =1.658413516807054e+18, w0=-0.19999995131507484, w1=0.1999998955994179\n",
      "Gradient Descent(83/99): loss =1.6583909363474335e+18, w0=-0.1999999507402215, w1=0.1999998941905208\n",
      "Gradient Descent(84/99): loss =1.658368357983565e+18, w0=-0.19999995016541067, w1=0.1999998927814756\n",
      "Gradient Descent(85/99): loss =1.6583457817138793e+18, w0=-0.19999994959064238, w1=0.1999998913722823\n",
      "Gradient Descent(86/99): loss =1.6583232075368364e+18, w0=-0.1999999490159167, w1=0.199999889962941\n",
      "Gradient Descent(87/99): loss =1.6583006354509123e+18, w0=-0.19999994844123367, w1=0.1999998885534517\n",
      "Gradient Descent(88/99): loss =1.6582780654545997e+18, w0=-0.1999999478665934, w1=0.19999988714381445\n",
      "Gradient Descent(89/99): loss =1.658255497546418e+18, w0=-0.1999999472919959, w1=0.19999988573402927\n",
      "Gradient Descent(90/99): loss =1.6582329317248927e+18, w0=-0.1999999467174413, w1=0.1999998843240962\n",
      "Gradient Descent(91/99): loss =1.658210367988575e+18, w0=-0.19999994614292962, w1=0.19999988291401527\n",
      "Gradient Descent(92/99): loss =1.6581878063360276e+18, w0=-0.19999994556846093, w1=0.19999988150378645\n",
      "Gradient Descent(93/99): loss =1.658165246765827e+18, w0=-0.19999994499403528, w1=0.19999988009340977\n",
      "Gradient Descent(94/99): loss =1.6581426892765668e+18, w0=-0.19999994441965274, w1=0.19999987868288527\n",
      "Gradient Descent(95/99): loss =1.6581201338668518e+18, w0=-0.19999994384531336, w1=0.19999987727221294\n",
      "Gradient Descent(96/99): loss =1.6580975805353021e+18, w0=-0.19999994327101722, w1=0.19999987586139278\n",
      "Gradient Descent(97/99): loss =1.658075029280551e+18, w0=-0.19999994269676435, w1=0.19999987445042483\n",
      "Gradient Descent(98/99): loss =1.65805248010124e+18, w0=-0.1999999421225548, w1=0.19999987303930908\n",
      "Gradient Descent(99/99): loss =1.6580299329960325e+18, w0=-0.19999994154838868, w1=0.19999987162804553\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.5717357063146913e+23, w0=0.0, w1=0.2\n",
      "Gradient Descent(2/99): loss =2.5276730399347603e+17, w0=0.00019644859457829182, w1=0.20018451541202686\n",
      "Gradient Descent(3/99): loss =4.339896329442741e+16, w0=0.000130708681759595, w1=0.20015509455251593\n",
      "Gradient Descent(4/99): loss =7799147884487355.0, w0=0.00017392766986954288, w1=0.2002166112344828\n",
      "Gradient Descent(5/99): loss =1819479592182000.8, w0=0.00017284075674932938, w1=0.2002422708474711\n",
      "Gradient Descent(6/99): loss =816896614667919.8, w0=0.0001900525058963864, w1=0.20028331745276784\n",
      "Gradient Descent(7/99): loss =648610946072739.9, w0=0.0001998530709117775, w1=0.2003184203091431\n",
      "Gradient Descent(8/99): loss =620124400787108.6, w0=0.00021272456859324233, w1=0.2003561296608587\n",
      "Gradient Descent(9/99): loss =615063517215482.8, w0=0.00022435542648971342, w1=0.2003928530636094\n",
      "Gradient Descent(10/99): loss =613928938965333.8, w0=0.000236499805221266, w1=0.20043001335016805\n",
      "Gradient Descent(11/99): loss =613453122223744.2, w0=0.0002484343789369776, w1=0.20046700353302832\n",
      "Gradient Descent(12/99): loss =613088370092815.4, w0=0.0002604525957376224, w1=0.2005040592053796\n",
      "Gradient Descent(13/99): loss =612742789084968.5, w0=0.00027243308740192943, w1=0.2005410770742299\n",
      "Gradient Descent(14/99): loss =612400900359705.5, w0=0.00028442496100470314, w1=0.2005780956244958\n",
      "Gradient Descent(15/99): loss =612060053497041.9, w0=0.0002964078838771245, w1=0.20061509688618978\n",
      "Gradient Descent(16/99): loss =611719763585189.9, w0=0.0003083901093713678, w1=0.20065208681805285\n",
      "Gradient Descent(17/99): loss =611379919676056.4, w0=0.00032036826535050683, w1=0.20068906202212183\n",
      "Gradient Descent(18/99): loss =611040481435028.9, w0=0.00033234376968916296, w1=0.20072602317393934\n",
      "Gradient Descent(19/99): loss =610701426123858.8, w0=0.00034431608993198417, w1=0.20076296942685748\n",
      "Gradient Descent(20/99): loss =610362738151240.0, w0=0.00035628549231316416, w1=0.20079990065194692\n",
      "Gradient Descent(21/99): loss =610024406187743.9, w0=0.00036825191226746464, w1=0.20083681649464197\n",
      "Gradient Descent(22/99): loss =609686421856367.6, w0=0.0003802154152279198, w1=0.20087371674622853\n",
      "Gradient Descent(23/99): loss =609348778910484.8, w0=0.00039217600779439766, w1=0.20091060118300136\n",
      "Gradient Descent(24/99): loss =609011472656163.6, w0=0.00040413371537981207, w1=0.20094746962619509\n",
      "Gradient Descent(25/99): loss =608674499534021.2, w0=0.00041608855100102845, w1=0.2009843219129911\n",
      "Gradient Descent(26/99): loss =608337856814411.8, w0=0.00042804052867428656, w1=0.2010211579046822\n",
      "Gradient Descent(27/99): loss =608001542374895.2, w0=0.00043998965852518805, w1=0.20105797748007542\n",
      "Gradient Descent(28/99): loss =607665554537603.0, w0=0.00045193594932460595, w1=0.20109478053524715\n",
      "Gradient Descent(29/99): loss =607329891950442.0, w0=0.00046387940791806884, w1=0.2011315669809514\n",
      "Gradient Descent(30/99): loss =606994553500216.8, w0=0.0004758200398614176, w1=0.2011683367412181\n",
      "Gradient Descent(31/99): loss =606659538249158.5, w0=0.0004877578495033155, w1=0.2012050897516828\n",
      "Gradient Descent(32/99): loss =606324845388493.0, w0=0.0004996928402419481, w1=0.20124182595823642\n",
      "Gradient Descent(33/99): loss =605990474204446.8, w0=0.000511625014665366, w1=0.2012785453157451\n",
      "Gradient Descent(34/99): loss =605656424053412.8, w0=0.0005235543747009788, w1=0.20131524778693177\n",
      "Gradient Descent(35/99): loss =605322694343740.6, w0=0.0005354809217286428, w1=0.2013519333413698\n",
      "Gradient Descent(36/99): loss =604989284522381.2, w0=0.0005474046566810108, w1=0.2013886019545955\n",
      "Gradient Descent(37/99): loss =604656194065124.8, w0=0.000559325580125749, w1=0.20142525360732297\n",
      "Gradient Descent(38/99): loss =604323422469409.5, w0=0.0005712436923354707, w1=0.20146188828475417\n",
      "Gradient Descent(39/99): loss =603990969249044.0, w0=0.0005831589933460694, w1=0.20149850597597346\n",
      "Gradient Descent(40/99): loss =603658833930312.9, w0=0.0005950714830057725, w1=0.20153510667341834\n",
      "Gradient Descent(41/99): loss =603327016049111.5, w0=0.0006069811610161553, w1=0.20157169037241743\n",
      "Gradient Descent(42/99): loss =602995515148834.5, w0=0.0006188880269664576, w1=0.20160825707078842\n",
      "Gradient Descent(43/99): loss =602664330778780.2, w0=0.0006307920803622113, w1=0.2016448067684886\n",
      "Gradient Descent(44/99): loss =602333462492997.9, w0=0.0006426933206490822, w1=0.20168133946731176\n",
      "Gradient Descent(45/99): loss =602002909849405.8, w0=0.0006545917472326665, w1=0.20171785517062574\n",
      "Gradient Descent(46/99): loss =601672672409131.1, w0=0.0006664873594948767, w1=0.2017543538831454\n",
      "Gradient Descent(47/99): loss =601342749735997.6, w0=0.0006783801568074496, w1=0.20179083561073663\n",
      "Gradient Descent(48/99): loss =601013141396175.6, w0=0.0006902701385430273, w1=0.20182730036024732\n",
      "Gradient Descent(49/99): loss =600683846957846.6, w0=0.0007021573040841953, w1=0.20186374813936162\n",
      "Gradient Descent(50/99): loss =600354865991017.0, w0=0.0007140416528307967, w1=0.2019001789564746\n",
      "Gradient Descent(51/99): loss =600026198067317.1, w0=0.0007259231842057992, w1=0.2019365928205846\n",
      "Gradient Descent(52/99): loss =599697842759858.0, w0=0.0007378018976599458, w1=0.20197298974120056\n",
      "Gradient Descent(53/99): loss =599369799643123.0, w0=0.0007496777926753821, w1=0.20200936972826275\n",
      "Gradient Descent(54/99): loss =599042068292874.1, w0=0.0007615508687684295, w1=0.20204573279207463\n",
      "Gradient Descent(55/99): loss =598714648286074.9, w0=0.0007734211254916437, w1=0.20208207894324462\n",
      "Gradient Descent(56/99): loss =598387539200825.2, w0=0.0007852885624352748, w1=0.20211840819263627\n",
      "Gradient Descent(57/99): loss =598060740616308.2, w0=0.0007971531792282328, w1=0.20215472055132566\n",
      "Gradient Descent(58/99): loss =597734252112721.5, w0=0.0008090149755386426, w1=0.20219101603056502\n",
      "Gradient Descent(59/99): loss =597408073271282.9, w0=0.0008208739510740576, w1=0.2022272946417517\n",
      "Gradient Descent(60/99): loss =597082203674139.2, w0=0.0008327301055813975, w1=0.2022635563964018\n",
      "Gradient Descent(61/99): loss =596756642904385.0, w0=0.0008445834388466571, w1=0.20229980130612768\n",
      "Gradient Descent(62/99): loss =596431390546001.8, w0=0.000856433950694432, w1=0.20233602938261883\n",
      "Gradient Descent(63/99): loss =596106446183829.1, w0=0.0008682816409872966, w1=0.2023722406376257\n",
      "Gradient Descent(64/99): loss =595781809403576.5, w0=0.0008801265096250649, w1=0.20240843508294604\n",
      "Gradient Descent(65/99): loss =595457479791758.6, w0=0.0008919685565439612, w1=0.2024446127304131\n",
      "Gradient Descent(66/99): loss =595133456935723.5, w0=0.0009038077817157205, w1=0.20248077359188604\n",
      "Gradient Descent(67/99): loss =594809740423584.5, w0=0.0009156441851466376, w1=0.2025169176792416\n",
      "Gradient Descent(68/99): loss =594486329844245.2, w0=0.0009274777668765802, w1=0.2025530450043672\n",
      "Gradient Descent(69/99): loss =594163224787369.6, w0=0.0009393085269779786, w1=0.20258915557915508\n",
      "Gradient Descent(70/99): loss =593840424843355.1, w0=0.0009511364655548015, w1=0.2026252494154976\n",
      "Gradient Descent(71/99): loss =593517929603354.4, w0=0.0009629615827415269, w1=0.20266132652528301\n",
      "Gradient Descent(72/99): loss =593195738659237.6, w0=0.0009747838787021168, w1=0.20269738692039235\n",
      "Gradient Descent(73/99): loss =592873851603573.5, w0=0.000986603353628998, w1=0.20273343061269655\n",
      "Gradient Descent(74/99): loss =592552268029649.2, w0=0.0009984200077420577, w1=0.2027694576140543\n",
      "Gradient Descent(75/99): loss =592230987531458.1, w0=0.0010102338412876538, w1=0.20280546793631016\n",
      "Gradient Descent(76/99): loss =591910009703649.6, w0=0.0010220448545376468, w1=0.20284146159129315\n",
      "Gradient Descent(77/99): loss =591589334141579.4, w0=0.0010338530477884502, w1=0.20287743859081567\n",
      "Gradient Descent(78/99): loss =591268960441267.1, w0=0.0010456584213601094, w1=0.20291339894667246\n",
      "Gradient Descent(79/99): loss =590948888199394.9, w0=0.0010574609755954, w1=0.20294934267064002\n",
      "Gradient Descent(80/99): loss =590629117013314.6, w0=0.0010692607108589555, w1=0.20298526977447615\n",
      "Gradient Descent(81/99): loss =590309646481011.6, w0=0.0010810576275364211, w1=0.20302118026991953\n",
      "Gradient Descent(82/99): loss =589990476201142.1, w0=0.0010928517260336314, w1=0.20305707416868954\n",
      "Gradient Descent(83/99): loss =589671605772991.8, w0=0.0011046430067758182, w1=0.20309295148248613\n",
      "Gradient Descent(84/99): loss =589353034796494.0, w0=0.001116431470206843, w1=0.20312881222298984\n",
      "Gradient Descent(85/99): loss =589034762872214.8, w0=0.001128217116788457, w1=0.20316465640186177\n",
      "Gradient Descent(86/99): loss =588716789601353.2, w0=0.0011399999469995864, w1=0.20320048403074373\n",
      "Gradient Descent(87/99): loss =588399114585726.5, w0=0.0011517799613356432, w1=0.2032362951212584\n",
      "Gradient Descent(88/99): loss =588081737427784.0, w0=0.0011635571603078622, w1=0.2032720896850095\n",
      "Gradient Descent(89/99): loss =587764657730598.1, w0=0.0011753315444426621, w1=0.203307867733582\n",
      "Gradient Descent(90/99): loss =587447875097848.4, w0=0.0011871031142810303, w1=0.20334362927854238\n",
      "Gradient Descent(91/99): loss =587131389133835.5, w0=0.001198871870377931, w1=0.20337937433143888\n",
      "Gradient Descent(92/99): loss =586815199443470.0, w0=0.001210637813301736, w1=0.20341510290380174\n",
      "Gradient Descent(93/99): loss =586499305632273.2, w0=0.0012224009436336784, w1=0.20345081500714357\n",
      "Gradient Descent(94/99): loss =586183707306371.9, w0=0.001234161261967326, w1=0.20348651065295953\n",
      "Gradient Descent(95/99): loss =585868404072493.1, w0=0.0012459187689080763, w1=0.2035221898527277\n",
      "Gradient Descent(96/99): loss =585553395537974.9, w0=0.0012576734650726706, w1=0.20355785261790926\n",
      "Gradient Descent(97/99): loss =585238681310740.2, w0=0.0012694253510887286, w1=0.20359349895994897\n",
      "Gradient Descent(98/99): loss =584924260999324.9, w0=0.0012811744275942998, w1=0.20362912889027526\n",
      "Gradient Descent(99/99): loss =584610134212851.6, w0=0.001292920695237433, w1=0.20366474242030064\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =4.219572666932707e+34, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =2.491123858731304e+18, w0=-0.19999999826708717, w1=0.20000000348034988\n",
      "Gradient Descent(3/99): loss =2.480286429688446e+18, w0=-0.19999999661111037, w1=0.2000000069248798\n",
      "Gradient Descent(4/99): loss =2.471660626672159e+18, w0=-0.19999999495032356, w1=0.20000001034597098\n",
      "Gradient Descent(5/99): loss =2.4633535280525066e+18, w0=-0.19999999330255155, w1=0.20000001374931195\n",
      "Gradient Descent(6/99): loss =2.455225144474654e+18, w0=-0.199999991660565, w1=0.20000001713605559\n",
      "Gradient Descent(7/99): loss =2.4472534046285353e+18, w0=-0.19999999002635713, w1=0.20000002050777776\n",
      "Gradient Descent(8/99): loss =2.4394299542848087e+18, w0=-0.1999999883988361, w1=0.2000000238654686\n",
      "Gradient Descent(9/99): loss =2.431748666393823e+18, w0=-0.1999999867780008, w1=0.2000000272100086\n",
      "Gradient Descent(10/99): loss =2.4242040835187917e+18, w0=-0.19999998516350292, w1=0.20000003054207346\n",
      "Gradient Descent(11/99): loss =2.416791222590587e+18, w0=-0.1999999835551358, w1=0.2000000338622177\n",
      "Gradient Descent(12/99): loss =2.409505496370599e+18, w0=-0.19999998195266477, w1=0.200000037170886\n",
      "Gradient Descent(13/99): loss =2.4023426697459666e+18, w0=-0.19999998035588318, w1=0.20000004046844083\n",
      "Gradient Descent(14/99): loss =2.3952988209064653e+18, w0=-0.19999997876459144, w1=0.2000000437551781\n",
      "Gradient Descent(15/99): loss =2.3883703085442657e+18, w0=-0.19999997717860277, w1=0.20000004703134155\n",
      "Gradient Descent(16/99): loss =2.381553742948551e+18, w0=-0.19999997559773994, w1=0.20000005029713336\n",
      "Gradient Descent(17/99): loss =2.3748459606648325e+18, w0=-0.19999997402183534, w1=0.20000005355272296\n",
      "Gradient Descent(18/99): loss =2.3682440021149706e+18, w0=-0.19999997245073, w1=0.20000005679825383\n",
      "Gradient Descent(19/99): loss =2.3617450917897503e+18, w0=-0.1999999708842731, w1=0.20000006003384888\n",
      "Gradient Descent(20/99): loss =2.3553466206634573e+18, w0=-0.19999996932232134, w1=0.20000006325961472\n",
      "Gradient Descent(21/99): loss =2.3490461305463025e+18, w0=-0.19999996776473852, w1=0.20000006647564514\n",
      "Gradient Descent(22/99): loss =2.3428413001324713e+18, w0=-0.1999999662113951, w1=0.20000006968202372\n",
      "Gradient Descent(23/99): loss =2.3367299325383506e+18, w0=-0.1999999646621678, w1=0.200000072878826\n",
      "Gradient Descent(24/99): loss =2.330709944154706e+18, w0=-0.19999996311693918, w1=0.2000000760661212\n",
      "Gradient Descent(25/99): loss =2.3247793546608297e+18, w0=-0.19999996157559746, w1=0.20000007924397342\n",
      "Gradient Descent(26/99): loss =2.3189362780687375e+18, w0=-0.199999960038036, w1=0.2000000824124429\n",
      "Gradient Descent(27/99): loss =2.313178914682426e+18, w0=-0.19999995850415317, w1=0.20000008557158672\n",
      "Gradient Descent(28/99): loss =2.307505543871615e+18, w0=-0.19999995697385206, w1=0.20000008872145944\n",
      "Gradient Descent(29/99): loss =2.3019145175713677e+18, w0=-0.19999995544704013, w1=0.20000009186211376\n",
      "Gradient Descent(30/99): loss =2.29640425442986e+18, w0=-0.19999995392362913, w1=0.20000009499360083\n",
      "Gradient Descent(31/99): loss =2.2909732345351624e+18, w0=-0.19999995240353474, w1=0.2000000981159706\n",
      "Gradient Descent(32/99): loss =2.2856199946601567e+18, w0=-0.19999995088667646, w1=0.20000010122927214\n",
      "Gradient Descent(33/99): loss =2.280343123971348e+18, w0=-0.19999994937297735, w1=0.20000010433355367\n",
      "Gradient Descent(34/99): loss =2.2751412601533686e+18, w0=-0.19999994786236389, w1=0.20000010742886293\n",
      "Gradient Descent(35/99): loss =2.270013085906292e+18, w0=-0.19999994635476578, w1=0.20000011051524713\n",
      "Gradient Descent(36/99): loss =2.264957325777442e+18, w0=-0.19999994485011582, w1=0.2000001135927532\n",
      "Gradient Descent(37/99): loss =2.2599727432936005e+18, w0=-0.1999999433483497, w1=0.20000011666142767\n",
      "Gradient Descent(38/99): loss =2.2550581383629594e+18, w0=-0.19999994184940584, w1=0.20000011972131695\n",
      "Gradient Descent(39/99): loss =2.2502123449196099e+18, w0=-0.19999994035322533, w1=0.2000001227724672\n",
      "Gradient Descent(40/99): loss =2.2454342287860398e+18, w0=-0.19999993885975176, w1=0.20000012581492443\n",
      "Gradient Descent(41/99): loss =2.2407226857317348e+18, w0=-0.19999993736893107, w1=0.2000001288487345\n",
      "Gradient Descent(42/99): loss =2.236076639708284e+18, w0=-0.19999993588071144, w1=0.20000013187394314\n",
      "Gradient Descent(43/99): loss =2.2314950412433308e+18, w0=-0.1999999343950432, w1=0.20000013489059598\n",
      "Gradient Descent(44/99): loss =2.226976865977598e+18, w0=-0.19999993291187876, w1=0.20000013789873855\n",
      "Gradient Descent(45/99): loss =2.2225211133307584e+18, w0=-0.1999999314311724, w1=0.20000014089841625\n",
      "Gradient Descent(46/99): loss =2.218126805283479e+18, w0=-0.19999992995288024, w1=0.2000001438896744\n",
      "Gradient Descent(47/99): loss =2.2137929852641172e+18, w0=-0.1999999284769602, w1=0.2000001468725582\n",
      "Gradient Descent(48/99): loss =2.2095187171298452e+18, w0=-0.19999992700337177, w1=0.20000014984711273\n",
      "Gradient Descent(49/99): loss =2.2053030842329684e+18, w0=-0.19999992553207613, w1=0.20000015281338296\n",
      "Gradient Descent(50/99): loss =2.2011451885640381e+18, w0=-0.19999992406303588, w1=0.20000015577141375\n",
      "Gradient Descent(51/99): loss =2.1970441499644262e+18, w0=-0.19999992259621507, w1=0.20000015872124982\n",
      "Gradient Descent(52/99): loss =2.1929991054015227e+18, w0=-0.19999992113157913, w1=0.20000016166293574\n",
      "Gradient Descent(53/99): loss =2.1890092083005975e+18, w0=-0.19999991966909475, w1=0.20000016459651598\n",
      "Gradient Descent(54/99): loss =2.185073627927819e+18, w0=-0.19999991820872987, w1=0.20000016752203487\n",
      "Gradient Descent(55/99): loss =2.1811915488195763e+18, w0=-0.19999991675045356, w1=0.20000017043953655\n",
      "Gradient Descent(56/99): loss =2.177362170253726e+18, w0=-0.19999991529423608, w1=0.200000173349065\n",
      "Gradient Descent(57/99): loss =2.1735847057587318e+18, w0=-0.19999991384004867, w1=0.2000001762506641\n",
      "Gradient Descent(58/99): loss =2.1698583826571523e+18, w0=-0.1999999123878636, w1=0.2000001791443775\n",
      "Gradient Descent(59/99): loss =2.166182441640309e+18, w0=-0.19999991093765415, w1=0.20000018203024872\n",
      "Gradient Descent(60/99): loss =2.1625561363711708e+18, w0=-0.19999990948939445, w1=0.20000018490832103\n",
      "Gradient Descent(61/99): loss =2.1589787331128817e+18, w0=-0.1999999080430595, w1=0.2000001877786376\n",
      "Gradient Descent(62/99): loss =2.1554495103805868e+18, w0=-0.19999990659862518, w1=0.2000001906412414\n",
      "Gradient Descent(63/99): loss =2.1519677586144187e+18, w0=-0.1999999051560681, w1=0.20000019349617512\n",
      "Gradient Descent(64/99): loss =2.148532779871738e+18, w0=-0.1999999037153657, w1=0.20000019634348137\n",
      "Gradient Descent(65/99): loss =2.145143887536945e+18, w0=-0.19999990227649606, w1=0.2000001991832025\n",
      "Gradient Descent(66/99): loss =2.1418004060472325e+18, w0=-0.19999990083943797, w1=0.20000020201538066\n",
      "Gradient Descent(67/99): loss =2.1385016706329738e+18, w0=-0.1999998994041709, w1=0.20000020484005776\n",
      "Gradient Descent(68/99): loss =2.1352470270714214e+18, w0=-0.19999989797067488, w1=0.20000020765727558\n",
      "Gradient Descent(69/99): loss =2.13203583145257e+18, w0=-0.1999998965389306, w1=0.2000002104670756\n",
      "Gradient Descent(70/99): loss =2.1288674499562245e+18, w0=-0.19999989510891927, w1=0.20000021326949916\n",
      "Gradient Descent(71/99): loss =2.125741258639269e+18, w0=-0.19999989368062263, w1=0.20000021606458732\n",
      "Gradient Descent(72/99): loss =2.1226566432323302e+18, w0=-0.19999989225402295, w1=0.20000021885238092\n",
      "Gradient Descent(73/99): loss =2.1196129989450993e+18, w0=-0.19999989082910305, w1=0.20000022163292064\n",
      "Gradient Descent(74/99): loss =2.1166097302795735e+18, w0=-0.19999988940584612, w1=0.20000022440624685\n",
      "Gradient Descent(75/99): loss =2.1136462508506627e+18, w0=-0.19999988798423585, w1=0.20000022717239976\n",
      "Gradient Descent(76/99): loss =2.1107219832135508e+18, w0=-0.19999988656425635, w1=0.20000022993141933\n",
      "Gradient Descent(77/99): loss =2.1078363586973222e+18, w0=-0.19999988514589215, w1=0.20000023268334527\n",
      "Gradient Descent(78/99): loss =2.1049888172444183e+18, w0=-0.19999988372912814, w1=0.2000002354282171\n",
      "Gradient Descent(79/99): loss =2.1021788072554555e+18, w0=-0.1999998823139496, w1=0.20000023816607404\n",
      "Gradient Descent(80/99): loss =2.0994057854390912e+18, w0=-0.19999988090034218, w1=0.20000024089695514\n",
      "Gradient Descent(81/99): loss =2.0966692166665452e+18, w0=-0.19999987948829184, w1=0.20000024362089922\n",
      "Gradient Descent(82/99): loss =2.0939685738304893e+18, w0=-0.1999998780777849, w1=0.2000002463379448\n",
      "Gradient Descent(83/99): loss =2.0913033377080381e+18, w0=-0.199999876668808, w1=0.20000024904813024\n",
      "Gradient Descent(84/99): loss =2.0886729968275407e+18, w0=-0.19999987526134805, w1=0.20000025175149364\n",
      "Gradient Descent(85/99): loss =2.086077047338999e+18, w0=-0.19999987385539225, w1=0.20000025444807282\n",
      "Gradient Descent(86/99): loss =2.0835149928878444e+18, w0=-0.19999987245092807, w1=0.20000025713790542\n",
      "Gradient Descent(87/99): loss =2.0809863444919314e+18, w0=-0.19999987104794326, w1=0.20000025982102884\n",
      "Gradient Descent(88/99): loss =2.0784906204215222e+18, w0=-0.19999986964642583, w1=0.2000002624974802\n",
      "Gradient Descent(89/99): loss =2.0760273460821504e+18, w0=-0.199999868246364, w1=0.20000026516729644\n",
      "Gradient Descent(90/99): loss =2.073596053900168e+18, w0=-0.1999998668477463, w1=0.2000002678305142\n",
      "Gradient Descent(91/99): loss =2.0711962832109074e+18, w0=-0.19999986545056136, w1=0.20000027048716995\n",
      "Gradient Descent(92/99): loss =2.0688275801492431e+18, w0=-0.1999998640547981, w1=0.2000002731372999\n",
      "Gradient Descent(93/99): loss =2.0664894975425446e+18, w0=-0.19999986266044567, w1=0.20000027578094\n",
      "Gradient Descent(94/99): loss =2.0641815948058225e+18, w0=-0.19999986126749336, w1=0.20000027841812604\n",
      "Gradient Descent(95/99): loss =2.0619034378390354e+18, w0=-0.1999998598759307, w1=0.20000028104889347\n",
      "Gradient Descent(96/99): loss =2.0596545989264645e+18, w0=-0.19999985848574733, w1=0.2000002836732776\n",
      "Gradient Descent(97/99): loss =2.057434656638007e+18, w0=-0.1999998570969332, w1=0.20000028629131347\n",
      "Gradient Descent(98/99): loss =2.0552431957324436e+18, w0=-0.19999985570947826, w1=0.2000002889030359\n",
      "Gradient Descent(99/99): loss =2.0530798070624535e+18, w0=-0.19999985432337275, w1=0.20000029150847945\n",
      "Optimizing degree 11/15, model: least_squares_GD, arguments: {'max_iters': 100}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =6.881612850879227e+38, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =7.758516349919839e+20, w0=-0.19999999977681482, w1=0.20000000042739222\n",
      "Gradient Descent(3/99): loss =7.743638906942915e+20, w0=-0.199999999556312, w1=0.20000000085403527\n",
      "Gradient Descent(4/99): loss =7.729402637599137e+20, w0=-0.19999999933619378, w1=0.20000000127941217\n",
      "Gradient Descent(5/99): loss =7.715303797857362e+20, w0=-0.19999999911641134, w1=0.20000000170361623\n",
      "Gradient Descent(6/99): loss =7.701338311658498e+20, w0=-0.1999999988969483, w1=0.20000000212671887\n",
      "Gradient Descent(7/99): loss =7.68750366153092e+20, w0=-0.19999999867779666, w1=0.20000000254877914\n",
      "Gradient Descent(8/99): loss =7.673797547011822e+20, w0=-0.19999999845894967, w1=0.20000000296984574\n",
      "Gradient Descent(9/99): loss =7.660217842507974e+20, w0=-0.1999999982404011, w1=0.20000000338995896\n",
      "Gradient Descent(10/99): loss =7.64676257211148e+20, w0=-0.19999999802214521, w1=0.20000000380915217\n",
      "Gradient Descent(11/99): loss =7.633429888924363e+20, w0=-0.19999999780417668, w1=0.20000000422745312\n",
      "Gradient Descent(12/99): loss =7.62021805786526e+20, w0=-0.19999999758649048, w1=0.20000000464488493\n",
      "Gradient Descent(13/99): loss =7.607125441264098e+20, w0=-0.19999999736908194, w1=0.20000000506146692\n",
      "Gradient Descent(14/99): loss =7.594150486711628e+20, w0=-0.19999999715194666, w1=0.20000000547721533\n",
      "Gradient Descent(15/99): loss =7.581291716749631e+20, w0=-0.1999999969350805, w1=0.2000000058921439\n",
      "Gradient Descent(16/99): loss =7.568547720076805e+20, w0=-0.19999999671847954, w1=0.20000000630626422\n",
      "Gradient Descent(17/99): loss =7.555917144012612e+20, w0=-0.19999999650214006, w1=0.20000000671958626\n",
      "Gradient Descent(18/99): loss =7.543398688012858e+20, w0=-0.19999999628605855, w1=0.2000000071321186\n",
      "Gradient Descent(19/99): loss =7.530991098070421e+20, w0=-0.19999999607023167, w1=0.20000000754386868\n",
      "Gradient Descent(20/99): loss =7.518693161865149e+20, w0=-0.19999999585465625, w1=0.20000000795484302\n",
      "Gradient Descent(21/99): loss =7.506503704551261e+20, w0=-0.19999999563932927, w1=0.2000000083650474\n",
      "Gradient Descent(22/99): loss =7.49442158508971e+20, w0=-0.19999999542424787, w1=0.20000000877448695\n",
      "Gradient Descent(23/99): loss =7.482445693048109e+20, w0=-0.19999999520940928, w1=0.20000000918316638\n",
      "Gradient Descent(24/99): loss =7.470574945803369e+20, w0=-0.1999999949948109, w1=0.20000000959108993\n",
      "Gradient Descent(25/99): loss =7.458808286092193e+20, w0=-0.19999999478045016, w1=0.20000000999826154\n",
      "Gradient Descent(26/99): loss =7.447144679862632e+20, w0=-0.1999999945663247, w1=0.20000001040468487\n",
      "Gradient Descent(27/99): loss =7.435583114387237e+20, w0=-0.19999999435243218, w1=0.20000001081036342\n",
      "Gradient Descent(28/99): loss =7.424122596603307e+20, w0=-0.1999999941387704, w1=0.20000001121530045\n",
      "Gradient Descent(29/99): loss =7.412762151651247e+20, w0=-0.19999999392533724, w1=0.2000000116194991\n",
      "Gradient Descent(30/99): loss =7.40150082158554e+20, w0=-0.19999999371213062, w1=0.2000000120229624\n",
      "Gradient Descent(31/99): loss =7.390337664236538e+20, w0=-0.1999999934991486, w1=0.2000000124256933\n",
      "Gradient Descent(32/99): loss =7.37927175220402e+20, w0=-0.1999999932863893, w1=0.20000001282769467\n",
      "Gradient Descent(33/99): loss =7.368302171966202e+20, w0=-0.19999999307385083, w1=0.20000001322896935\n",
      "Gradient Descent(34/99): loss =7.357428023089639e+20, w0=-0.19999999286153147, w1=0.2000000136295201\n",
      "Gradient Descent(35/99): loss =7.346648417527718e+20, w0=-0.19999999264942953, w1=0.20000001402934967\n",
      "Gradient Descent(36/99): loss =7.335962478996728e+20, w0=-0.19999999243754332, w1=0.20000001442846074\n",
      "Gradient Descent(37/99): loss =7.325369342420052e+20, w0=-0.19999999222587128, w1=0.20000001482685603\n",
      "Gradient Descent(38/99): loss =7.314868153432135e+20, w0=-0.19999999201441185, w1=0.20000001522453822\n",
      "Gradient Descent(39/99): loss =7.304458067934928e+20, w0=-0.19999999180316352, w1=0.20000001562150996\n",
      "Gradient Descent(40/99): loss =7.294138251700468e+20, w0=-0.19999999159212484, w1=0.2000000160177739\n",
      "Gradient Descent(41/99): loss =7.283907880014025e+20, w0=-0.1999999913812944, w1=0.20000001641333268\n",
      "Gradient Descent(42/99): loss =7.273766137352884e+20, w0=-0.19999999117067085, w1=0.2000000168081889\n",
      "Gradient Descent(43/99): loss =7.263712217096488e+20, w0=-0.19999999096025284, w1=0.20000001720234525\n",
      "Gradient Descent(44/99): loss =7.253745321264218e+20, w0=-0.19999999075003908, w1=0.20000001759580435\n",
      "Gradient Descent(45/99): loss =7.243864660277466e+20, w0=-0.1999999905400283, w1=0.2000000179885688\n",
      "Gradient Descent(46/99): loss =7.234069452743119e+20, w0=-0.19999999033021923, w1=0.20000001838064124\n",
      "Gradient Descent(47/99): loss =7.224358925255912e+20, w0=-0.1999999901206107, w1=0.20000001877202428\n",
      "Gradient Descent(48/99): loss =7.214732312217502e+20, w0=-0.19999998991120155, w1=0.20000001916272053\n",
      "Gradient Descent(49/99): loss =7.205188855670111e+20, w0=-0.19999998970199062, w1=0.20000001955273264\n",
      "Gradient Descent(50/99): loss =7.195727805143256e+20, w0=-0.1999999894929768, w1=0.20000001994206318\n",
      "Gradient Descent(51/99): loss =7.186348417511846e+20, w0=-0.19999998928415894, w1=0.20000002033071476\n",
      "Gradient Descent(52/99): loss =7.177049956864498e+20, w0=-0.199999989075536, w1=0.20000002071869002\n",
      "Gradient Descent(53/99): loss =7.167831694380736e+20, w0=-0.19999998886710696, w1=0.20000002110599155\n",
      "Gradient Descent(54/99): loss =7.158692908216232e+20, w0=-0.19999998865887073, w1=0.20000002149262194\n",
      "Gradient Descent(55/99): loss =7.149632883394983e+20, w0=-0.1999999884508263, w1=0.2000000218785838\n",
      "Gradient Descent(56/99): loss =7.140650911707863e+20, w0=-0.19999998824297271, w1=0.20000002226387967\n",
      "Gradient Descent(57/99): loss =7.131746291616619e+20, w0=-0.19999998803530894, w1=0.2000000226485122\n",
      "Gradient Descent(58/99): loss =7.122918328162892e+20, w0=-0.19999998782783404, w1=0.20000002303248393\n",
      "Gradient Descent(59/99): loss =7.11416633288163e+20, w0=-0.19999998762054705, w1=0.20000002341579745\n",
      "Gradient Descent(60/99): loss =7.105489623718423e+20, w0=-0.19999998741344704, w1=0.20000002379845533\n",
      "Gradient Descent(61/99): loss =7.096887524950366e+20, w0=-0.19999998720653311, w1=0.20000002418046015\n",
      "Gradient Descent(62/99): loss =7.088359367110105e+20, w0=-0.19999998699980434, w1=0.20000002456181443\n",
      "Gradient Descent(63/99): loss =7.079904486912683e+20, w0=-0.19999998679325984, w1=0.20000002494252075\n",
      "Gradient Descent(64/99): loss =7.071522227184921e+20, w0=-0.19999998658689874, w1=0.20000002532258165\n",
      "Gradient Descent(65/99): loss =7.063211936797174e+20, w0=-0.19999998638072017, w1=0.20000002570199968\n",
      "Gradient Descent(66/99): loss =7.054972970597105e+20, w0=-0.19999998617472325, w1=0.20000002608077735\n",
      "Gradient Descent(67/99): loss =7.046804689345337e+20, w0=-0.19999998596890714, w1=0.2000000264589172\n",
      "Gradient Descent(68/99): loss =7.038706459652902e+20, w0=-0.199999985763271, w1=0.20000002683642173\n",
      "Gradient Descent(69/99): loss =7.030677653920138e+20, w0=-0.199999985557814, w1=0.20000002721329346\n",
      "Gradient Descent(70/99): loss =7.02271765027713e+20, w0=-0.19999998535253535, w1=0.2000000275895349\n",
      "Gradient Descent(71/99): loss =7.014825832525412e+20, w0=-0.19999998514743422, w1=0.20000002796514849\n",
      "Gradient Descent(72/99): loss =7.007001590080862e+20, w0=-0.19999998494250978, w1=0.20000002834013675\n",
      "Gradient Descent(73/99): loss =6.999244317917792e+20, w0=-0.1999999847377613, w1=0.20000002871450218\n",
      "Gradient Descent(74/99): loss =6.99155341651398e+20, w0=-0.19999998453318793, w1=0.2000000290882472\n",
      "Gradient Descent(75/99): loss =6.983928291796753e+20, w0=-0.19999998432878893, w1=0.20000002946137432\n",
      "Gradient Descent(76/99): loss =6.976368355089944e+20, w0=-0.19999998412456352, w1=0.20000002983388596\n",
      "Gradient Descent(77/99): loss =6.968873023061678e+20, w0=-0.19999998392051094, w1=0.20000003020578455\n",
      "Gradient Descent(78/99): loss =6.961441717672979e+20, w0=-0.19999998371663044, w1=0.20000003057707255\n",
      "Gradient Descent(79/99): loss =6.954073866127137e+20, w0=-0.19999998351292125, w1=0.20000003094775237\n",
      "Gradient Descent(80/99): loss =6.946768900819763e+20, w0=-0.19999998330938265, w1=0.2000000313178264\n",
      "Gradient Descent(81/99): loss =6.939526259289555e+20, w0=-0.19999998310601388, w1=0.20000003168729708\n",
      "Gradient Descent(82/99): loss =6.932345384169667e+20, w0=-0.19999998290281423, w1=0.2000000320561668\n",
      "Gradient Descent(83/99): loss =6.92522572313976e+20, w0=-0.19999998269978295, w1=0.20000003242443792\n",
      "Gradient Descent(84/99): loss =6.918166728878568e+20, w0=-0.19999998249691933, w1=0.20000003279211284\n",
      "Gradient Descent(85/99): loss =6.911167859017118e+20, w0=-0.19999998229422267, w1=0.20000003315919393\n",
      "Gradient Descent(86/99): loss =6.904228576092425e+20, w0=-0.19999998209169223, w1=0.20000003352568352\n",
      "Gradient Descent(87/99): loss =6.897348347501781e+20, w0=-0.19999998188932733, w1=0.200000033891584\n",
      "Gradient Descent(88/99): loss =6.890526645457532e+20, w0=-0.19999998168712724, w1=0.20000003425689766\n",
      "Gradient Descent(89/99): loss =6.883762946942337e+20, w0=-0.1999999814850913, w1=0.20000003462162688\n",
      "Gradient Descent(90/99): loss =6.87705673366494e+20, w0=-0.1999999812832188, w1=0.20000003498577396\n",
      "Gradient Descent(91/99): loss =6.870407492016421e+20, w0=-0.19999998108150907, w1=0.2000000353493412\n",
      "Gradient Descent(92/99): loss =6.863814713026888e+20, w0=-0.1999999808799614, w1=0.20000003571233088\n",
      "Gradient Descent(93/99): loss =6.857277892322634e+20, w0=-0.19999998067857513, w1=0.20000003607474534\n",
      "Gradient Descent(94/99): loss =6.850796530083721e+20, w0=-0.1999999804773496, w1=0.20000003643658684\n",
      "Gradient Descent(95/99): loss =6.844370131002037e+20, w0=-0.1999999802762841, w1=0.20000003679785766\n",
      "Gradient Descent(96/99): loss =6.83799820423972e+20, w0=-0.199999980075378, w1=0.20000003715856005\n",
      "Gradient Descent(97/99): loss =6.831680263388063e+20, w0=-0.19999997987463064, w1=0.20000003751869627\n",
      "Gradient Descent(98/99): loss =6.825415826426737e+20, w0=-0.19999997967404134, w1=0.20000003787826856\n",
      "Gradient Descent(99/99): loss =6.819204415683558e+20, w0=-0.19999997947360948, w1=0.20000003823727916\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =6.881612850877677e+38, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =5.817366596248639e+20, w0=-0.1999999998755204, w1=0.20000000000720364\n",
      "Gradient Descent(3/99): loss =5.7039775187987916e+20, w0=-0.1999999997665888, w1=0.1999999999815021\n",
      "Gradient Descent(4/99): loss =5.6296694651525844e+20, w0=-0.1999999996622842, w1=0.1999999999336737\n",
      "Gradient Descent(5/99): loss =5.576144738199735e+20, w0=-0.19999999956186656, w1=0.1999999998673957\n",
      "Gradient Descent(6/99): loss =5.5375774625974216e+20, w0=-0.1999999994647398, w1=0.19999999978563215\n",
      "Gradient Descent(7/99): loss =5.5097788908035867e+20, w0=-0.199999999370402, w1=0.19999999969082116\n",
      "Gradient Descent(8/99): loss =5.4897353802191294e+20, w0=-0.1999999992784293, w1=0.19999999958498849\n",
      "Gradient Descent(9/99): loss =5.4752777344422923e+20, w0=-0.19999999918846317, w1=0.19999999946982908\n",
      "Gradient Descent(10/99): loss =5.464844067006992e+20, w0=-0.1999999991002, w1=0.19999999934676793\n",
      "Gradient Descent(11/99): loss =5.457309505508329e+20, w0=-0.1999999990133824, w1=0.19999999921700687\n",
      "Gradient Descent(12/99): loss =5.451863776941821e+20, w0=-0.19999999892779216, w1=0.1999999990815617\n",
      "Gradient Descent(13/99): loss =5.447923149992146e+20, w0=-0.19999999884324418, w1=0.19999999894129236\n",
      "Gradient Descent(14/99): loss =5.4450670596194756e+20, w0=-0.19999999875958138, w1=0.19999999879692734\n",
      "Gradient Descent(15/99): loss =5.4429924789638326e+20, w0=-0.19999999867667045, w1=0.19999999864908433\n",
      "Gradient Descent(16/99): loss =5.4414810600814137e+20, w0=-0.1999999985943983, w1=0.19999999849828726\n",
      "Gradient Descent(17/99): loss =5.4403754657336374e+20, w0=-0.1999999985126689, w1=0.1999999983449806\n",
      "Gradient Descent(18/99): loss =5.4395623190794456e+20, w0=-0.19999999843140076, w1=0.1999999981895416\n",
      "Gradient Descent(19/99): loss =5.438959919622811e+20, w0=-0.1999999983505247, w1=0.19999999803229035\n",
      "Gradient Descent(20/99): loss =5.438509392426324e+20, w0=-0.199999998269982, w1=0.1999999978734986\n",
      "Gradient Descent(21/99): loss =5.438168310703013e+20, w0=-0.1999999981897228, w1=0.19999999771339694\n",
      "Gradient Descent(22/99): loss =5.4379061004244674e+20, w0=-0.19999999810970484, w1=0.1999999975521812\n",
      "Gradient Descent(23/99): loss =5.4377007289149646e+20, w0=-0.19999999802989216, w1=0.19999999739001753\n",
      "Gradient Descent(24/99): loss =5.437536318630874e+20, w0=-0.19999999795025428, w1=0.19999999722704703\n",
      "Gradient Descent(25/99): loss =5.4374014276107973e+20, w0=-0.1999999978707653, w1=0.19999999706338945\n",
      "Gradient Descent(26/99): loss =5.437287810327312e+20, w0=-0.1999999977914033, w1=0.1999999968991464\n",
      "Gradient Descent(27/99): loss =5.4371895247209326e+20, w0=-0.19999999771214957, w1=0.19999999673440425\n",
      "Gradient Descent(28/99): loss =5.437102288699689e+20, w0=-0.19999999763298834, w1=0.19999999656923623\n",
      "Gradient Descent(29/99): loss =5.43702301640993e+20, w0=-0.19999999755390613, w1=0.19999999640370453\n",
      "Gradient Descent(30/99): loss =5.436949484055794e+20, w0=-0.19999999747489156, w1=0.19999999623786194\n",
      "Gradient Descent(31/99): loss =5.436880089075719e+20, w0=-0.1999999973959349, w1=0.19999999607175328\n",
      "Gradient Descent(32/99): loss =5.4368136765954313e+20, w0=-0.199999997317028, w1=0.1999999959054166\n",
      "Gradient Descent(33/99): loss =5.4367494143629276e+20, w0=-0.19999999723816383, w1=0.1999999957388842\n",
      "Gradient Descent(34/99): loss =5.4366867026214486e+20, w0=-0.19999999715933647, w1=0.19999999557218348\n",
      "Gradient Descent(35/99): loss =5.436625109160218e+20, w0=-0.19999999708054092, w1=0.19999999540533772\n",
      "Gradient Descent(36/99): loss =5.436564322509225e+20, w0=-0.1999999970017729, w1=0.1999999952383667\n",
      "Gradient Descent(37/99): loss =5.4365041182094216e+20, w0=-0.19999999692302878, w1=0.19999999507128716\n",
      "Gradient Descent(38/99): loss =5.436444334505635e+20, w0=-0.19999999684430547, w1=0.19999999490411335\n",
      "Gradient Descent(39/99): loss =5.4363848548297854e+20, w0=-0.1999999967656004, w1=0.19999999473685737\n",
      "Gradient Descent(40/99): loss =5.4363255951775426e+20, w0=-0.19999999668691135, w1=0.19999999456952947\n",
      "Gradient Descent(41/99): loss =5.4362664950114786e+20, w0=-0.19999999660823642, w1=0.19999999440213836\n",
      "Gradient Descent(42/99): loss =5.436207510705378e+20, w0=-0.199999996529574, w1=0.19999999423469145\n",
      "Gradient Descent(43/99): loss =5.436148610820014e+20, w0=-0.19999999645092278, w1=0.199999994067195\n",
      "Gradient Descent(44/99): loss =5.4360897726986165e+20, w0=-0.1999999963722816, w1=0.19999999389965437\n",
      "Gradient Descent(45/99): loss =5.436030980013452e+20, w0=-0.19999999629364948, w1=0.19999999373207406\n",
      "Gradient Descent(46/99): loss =5.4359722209977506e+20, w0=-0.1999999962150256, w1=0.19999999356445788\n",
      "Gradient Descent(47/99): loss =5.4359134871715814e+20, w0=-0.19999999613640923, w1=0.19999999339680916\n",
      "Gradient Descent(48/99): loss =5.435854772423663e+20, w0=-0.1999999960577998, w1=0.19999999322913062\n",
      "Gradient Descent(49/99): loss =5.435796072349645e+20, w0=-0.1999999959791968, w1=0.1999999930614246\n",
      "Gradient Descent(50/99): loss =5.435737383775294e+20, w0=-0.19999999590059978, w1=0.19999999289369313\n",
      "Gradient Descent(51/99): loss =5.4356787044128267e+20, w0=-0.1999999958220084, w1=0.19999999272593788\n",
      "Gradient Descent(52/99): loss =5.435620032613295e+20, w0=-0.19999999574342234, w1=0.19999999255816028\n",
      "Gradient Descent(53/99): loss =5.435561367188122e+20, w0=-0.19999999566484136, w1=0.19999999239036156\n",
      "Gradient Descent(54/99): loss =5.435502707280486e+20, w0=-0.19999999558626524, w1=0.19999999222254278\n",
      "Gradient Descent(55/99): loss =5.435444052272648e+20, w0=-0.19999999550769376, w1=0.19999999205470478\n",
      "Gradient Descent(56/99): loss =5.43538540171917e+20, w0=-0.1999999954291268, w1=0.19999999188684833\n",
      "Gradient Descent(57/99): loss =5.43532675529878e+20, w0=-0.1999999953505642, w1=0.19999999171897406\n",
      "Gradient Descent(58/99): loss =5.435268112779682e+20, w0=-0.19999999527200585, w1=0.19999999155108253\n",
      "Gradient Descent(59/99): loss =5.4352094739945626e+20, w0=-0.19999999519345166, w1=0.19999999138317417\n",
      "Gradient Descent(60/99): loss =5.4351508388225804e+20, w0=-0.19999999511490155, w1=0.19999999121524936\n",
      "Gradient Descent(61/99): loss =5.435092207176382e+20, w0=-0.19999999503635543, w1=0.19999999104730845\n",
      "Gradient Descent(62/99): loss =5.435033578992762e+20, w0=-0.19999999495781326, w1=0.19999999087935172\n",
      "Gradient Descent(63/99): loss =5.4349749542258953e+20, w0=-0.199999994879275, w1=0.1999999907113794\n",
      "Gradient Descent(64/99): loss =5.434916332842504e+20, w0=-0.1999999948007406, w1=0.19999999054339168\n",
      "Gradient Descent(65/99): loss =5.434857714818337e+20, w0=-0.19999999472221003, w1=0.19999999037538874\n",
      "Gradient Descent(66/99): loss =5.434799100135656e+20, w0=-0.19999999464368326, w1=0.19999999020737072\n",
      "Gradient Descent(67/99): loss =5.434740488781426e+20, w0=-0.19999999456516024, w1=0.19999999003933777\n",
      "Gradient Descent(68/99): loss =5.4346818807459984e+20, w0=-0.199999994486641, w1=0.19999998987128997\n",
      "Gradient Descent(69/99): loss =5.434623276022149e+20, w0=-0.19999999440812546, w1=0.1999999897032274\n",
      "Gradient Descent(70/99): loss =5.434564674604421e+20, w0=-0.19999999432961366, w1=0.1999999895351502\n",
      "Gradient Descent(71/99): loss =5.4345060764886277e+20, w0=-0.19999999425110557, w1=0.19999998936705835\n",
      "Gradient Descent(72/99): loss =5.4344474816715044e+20, w0=-0.19999999417260117, w1=0.19999998919895193\n",
      "Gradient Descent(73/99): loss =5.4343888901504375e+20, w0=-0.19999999409410046, w1=0.199999989030831\n",
      "Gradient Descent(74/99): loss =5.434330301923291e+20, w0=-0.19999999401560345, w1=0.1999999888626956\n",
      "Gradient Descent(75/99): loss =5.4342717169882864e+20, w0=-0.1999999939371101, w1=0.19999998869454577\n",
      "Gradient Descent(76/99): loss =5.434213135343877e+20, w0=-0.19999999385862044, w1=0.19999998852638154\n",
      "Gradient Descent(77/99): loss =5.434154556988716e+20, w0=-0.19999999378013444, w1=0.1999999883582029\n",
      "Gradient Descent(78/99): loss =5.434095981921569e+20, w0=-0.19999999370165214, w1=0.19999998819000991\n",
      "Gradient Descent(79/99): loss =5.434037410141314e+20, w0=-0.1999999936231735, w1=0.19999998802180258\n",
      "Gradient Descent(80/99): loss =5.433978841646892e+20, w0=-0.19999999354469852, w1=0.1999999878535809\n",
      "Gradient Descent(81/99): loss =5.433920276437296e+20, w0=-0.1999999934662272, w1=0.19999998768534488\n",
      "Gradient Descent(82/99): loss =5.433861714511563e+20, w0=-0.19999999338775956, w1=0.19999998751709458\n",
      "Gradient Descent(83/99): loss =5.4338031558687444e+20, w0=-0.19999999330929558, w1=0.19999998734882996\n",
      "Gradient Descent(84/99): loss =5.433744600507925e+20, w0=-0.19999999323083528, w1=0.19999998718055104\n",
      "Gradient Descent(85/99): loss =5.433686048428203e+20, w0=-0.19999999315237865, w1=0.19999998701225785\n",
      "Gradient Descent(86/99): loss =5.433627499628694e+20, w0=-0.19999999307392569, w1=0.19999998684395037\n",
      "Gradient Descent(87/99): loss =5.4335689541085115e+20, w0=-0.19999999299547638, w1=0.19999998667562863\n",
      "Gradient Descent(88/99): loss =5.433510411866789e+20, w0=-0.19999999291703074, w1=0.19999998650729262\n",
      "Gradient Descent(89/99): loss =5.433451872902669e+20, w0=-0.1999999928385888, w1=0.19999998633894234\n",
      "Gradient Descent(90/99): loss =5.433393337215288e+20, w0=-0.1999999927601505, w1=0.1999999861705778\n",
      "Gradient Descent(91/99): loss =5.433334804803794e+20, w0=-0.1999999926817159, w1=0.19999998600219898\n",
      "Gradient Descent(92/99): loss =5.433276275667338e+20, w0=-0.19999999260328497, w1=0.1999999858338059\n",
      "Gradient Descent(93/99): loss =5.433217749805073e+20, w0=-0.1999999925248577, w1=0.19999998566539856\n",
      "Gradient Descent(94/99): loss =5.4331592272161735e+20, w0=-0.19999999244643413, w1=0.19999998549697698\n",
      "Gradient Descent(95/99): loss =5.4331007078997964e+20, w0=-0.19999999236801425, w1=0.19999998532854113\n",
      "Gradient Descent(96/99): loss =5.433042191855102e+20, w0=-0.19999999228959803, w1=0.19999998516009104\n",
      "Gradient Descent(97/99): loss =5.4329836790812744e+20, w0=-0.1999999922111855, w1=0.1999999849916267\n",
      "Gradient Descent(98/99): loss =5.4329251695774794e+20, w0=-0.19999999213277667, w1=0.1999999848231481\n",
      "Gradient Descent(99/99): loss =5.432866663342901e+20, w0=-0.19999999205437152, w1=0.19999998465465527\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.5599278640894964e+26, w0=0.0, w1=0.2\n",
      "Gradient Descent(2/99): loss =2.527673088664829e+17, w0=6.550485876996202e-06, w1=0.20000829063258507\n",
      "Gradient Descent(3/99): loss =1.3056915331459954e+16, w0=6.0704291655630365e-06, w1=0.2000102673280641\n",
      "Gradient Descent(4/99): loss =2602724864694493.0, w0=7.057268588768485e-06, w1=0.2000135110153305\n",
      "Gradient Descent(5/99): loss =2146016632566997.0, w0=7.732884957671513e-06, w1=0.200016461450969\n",
      "Gradient Descent(6/99): loss =2125392762753360.8, w0=8.470759672682462e-06, w1=0.20001945784160619\n",
      "Gradient Descent(7/99): loss =2123817001802284.8, w0=9.194099607614104e-06, w1=0.2000224364292937\n",
      "Gradient Descent(8/99): loss =2123081290978072.8, w0=9.919589000390815e-06, w1=0.20002541425726392\n",
      "Gradient Descent(9/99): loss =2122385330743195.5, w0=1.0644091922477446e-05, w1=0.20002838971213327\n",
      "Gradient Descent(10/99): loss =2121692372153549.5, w0=1.1368450934042694e-05, w1=0.2000313641297598\n",
      "Gradient Descent(11/99): loss =2121000244897170.0, w0=1.2092591621919908e-05, w1=0.20003433772995938\n",
      "Gradient Descent(12/99): loss =2120308663835699.5, w0=1.2816584789487765e-05, w1=0.20003731070354056\n",
      "Gradient Descent(13/99): loss =2119617541406615.2, w0=1.3540446017041917e-05, w1=0.20004028311188682\n",
      "Gradient Descent(14/99): loss =2118926837034698.2, w0=1.4264188982935054e-05, w1=0.20004325497493688\n",
      "Gradient Descent(15/99): loss =2118236527104706.5, w0=1.4987820481136795e-05, w1=0.2000462262880417\n",
      "Gradient Descent(16/99): loss =2117546595908783.2, w0=1.571134476778384e-05, w1=0.20004919703675939\n",
      "Gradient Descent(17/99): loss =2116857032280806.0, w0=1.643476443364888e-05, w1=0.20005216720311209\n",
      "Gradient Descent(18/99): loss =2116167828042551.0, w0=1.715808118364526e-05, w1=0.20005513676889752\n",
      "Gradient Descent(19/99): loss =2115478977121320.5, w0=1.78812961963883e-05, w1=0.20005810571714647\n",
      "Gradient Descent(20/99): loss =2114790474973445.8, w0=1.8604410335262906e-05, w1=0.20006107403269785\n",
      "Gradient Descent(21/99): loss =2114102318179153.5, w0=1.9327424263231558e-05, w1=0.20006404170232223\n",
      "Gradient Descent(22/99): loss =2113414504149011.2, w0=2.0050338508900584e-05, w1=0.2000670087146358\n",
      "Gradient Descent(23/99): loss =2112727030908096.8, w0=2.077315350529188e-05, w1=0.20006997505992632\n",
      "Gradient Descent(24/99): loss =2112039896936667.8, w0=2.1495869613711556e-05, w1=0.2000729407299523\n",
      "Gradient Descent(25/99): loss =2111353101052324.5, w0=2.221848713922482e-05, w1=0.20007590571774544\n",
      "Gradient Descent(26/99): loss =2110666642322623.2, w0=2.2941006341304664e-05, w1=0.20007887001742897\n",
      "Gradient Descent(27/99): loss =2109980520000485.2, w0=2.3663427441597203e-05, w1=0.20008183362405654\n",
      "Gradient Descent(28/99): loss =2109294733476195.5, w0=2.4385750629881156e-05, w1=0.2000847965334724\n",
      "Gradient Descent(29/99): loss =2108609282241960.2, w0=2.510797606882989e-05, w1=0.20008775874219115\n",
      "Gradient Descent(30/99): loss =2107924165865562.0, w0=2.5830103897929934e-05, w1=0.2000907202472955\n",
      "Gradient Descent(31/99): loss =2107239383970878.2, w0=2.655213423677043e-05, w1=0.20009368104634925\n",
      "Gradient Descent(32/99): loss =2106554936223420.8, w0=2.7274067187840357e-05, w1=0.2000966411373237\n",
      "Gradient Descent(33/99): loss =2105870822319623.0, w0=2.799590283892633e-05, w1=0.20009960051853523\n",
      "Gradient Descent(34/99): loss =2105187041978933.2, w0=2.871764126517776e-05, w1=0.20010255918859268\n",
      "Gradient Descent(35/99): loss =2104503594937902.0, w0=2.943928253089026e-05, w1=0.2001055171463528\n",
      "Gradient Descent(36/99): loss =2103820480945807.0, w0=3.016082669104787e-05, w1=0.20010847439088245\n",
      "Gradient Descent(37/99): loss =2103137699761488.5, w0=3.08822737926573e-05, w1=0.20011143092142686\n",
      "Gradient Descent(38/99): loss =2102455251150843.5, w0=3.16036238759024e-05, w1=0.20011438673738252\n",
      "Gradient Descent(39/99): loss =2101773134885145.8, w0=3.232487697514267e-05, w1=0.20011734183827443\n",
      "Gradient Descent(40/99): loss =2101091350739702.0, w0=3.304603311977654e-05, w1=0.20012029622373664\n",
      "Gradient Descent(41/99): loss =2100409898492816.5, w0=3.3767092334987194e-05, w1=0.20012324989349595\n",
      "Gradient Descent(42/99): loss =2099728777925128.2, w0=3.4488054642386406e-05, w1=0.20012620284735788\n",
      "Gradient Descent(43/99): loss =2099047988819049.5, w0=3.520892006057008e-05, w1=0.20012915508519497\n",
      "Gradient Descent(44/99): loss =2098367530958361.8, w0=3.5929688605596997e-05, w1=0.20013210660693664\n",
      "Gradient Descent(45/99): loss =2097687404127911.0, w0=3.6650360291401046e-05, w1=0.2001350574125608\n",
      "Gradient Descent(46/99): loss =2097007608113408.2, w0=3.737093513014591e-05, w1=0.2001380075020865\n",
      "Gradient Descent(47/99): loss =2096328142701224.5, w0=3.809141313252982e-05, w1=0.20014095687556782\n",
      "Gradient Descent(48/99): loss =2095649007678310.5, w0=3.881179430804712e-05, w1=0.20014390553308872\n",
      "Gradient Descent(49/99): loss =2094970202832096.8, w0=3.953207866521253e-05, w1=0.20014685347475847\n",
      "Gradient Descent(50/99): loss =2094291727950357.5, w0=4.0252266211753054e-05, w1=0.20014980070070787\n",
      "Gradient Descent(51/99): loss =2093613582821272.5, w0=4.097235695477206e-05, w1=0.20015274721108609\n",
      "Gradient Descent(52/99): loss =2092935767233285.0, w0=4.169235090088923e-05, w1=0.2001556930060579\n",
      "Gradient Descent(53/99): loss =2092258280975139.5, w0=4.241224805635974e-05, w1=0.2001586380858013\n",
      "Gradient Descent(54/99): loss =2091581123835800.2, w0=4.313204842717554e-05, w1=0.20016158245050558\n",
      "Gradient Descent(55/99): loss =2090904295604530.2, w0=4.385175201915113e-05, w1=0.20016452610036953\n",
      "Gradient Descent(56/99): loss =2090227796070734.0, w0=4.457135883799613e-05, w1=0.20016746903560007\n",
      "Gradient Descent(57/99): loss =2089551625024068.0, w0=4.529086888937628e-05, w1=0.20017041125641097\n",
      "Gradient Descent(58/99): loss =2088875782254395.5, w0=4.601028217896471e-05, w1=0.20017335276302184\n",
      "Gradient Descent(59/99): loss =2088200267551771.0, w0=4.672959871248468e-05, w1=0.20017629355565714\n",
      "Gradient Descent(60/99): loss =2087525080706417.0, w0=4.7448818495745095e-05, w1=0.2001792336345455\n",
      "Gradient Descent(61/99): loss =2086850221508769.8, w0=4.816794153466981e-05, w1=0.20018217299991908\n",
      "Gradient Descent(62/99): loss =2086175689749432.2, w0=4.8886967835321556e-05, w1=0.2001851116520129\n",
      "Gradient Descent(63/99): loss =2085501485219179.0, w0=4.960589740392134e-05, w1=0.2001880495910644\n",
      "Gradient Descent(64/99): loss =2084827607708981.8, w0=5.032473024686395e-05, w1=0.20019098681731315\n",
      "Gradient Descent(65/99): loss =2084154057009959.8, w0=5.104346637073014e-05, w1=0.20019392333100036\n",
      "Gradient Descent(66/99): loss =2083480832913430.8, w0=5.1762105782295944e-05, w1=0.20019685913236868\n",
      "Gradient Descent(67/99): loss =2082807935210856.5, w0=5.248064848853965e-05, w1=0.20019979422166187\n",
      "Gradient Descent(68/99): loss =2082135363693870.5, w0=5.319909449664668e-05, w1=0.20020272859912464\n",
      "Gradient Descent(69/99): loss =2081463118154291.2, w0=5.391744381401279e-05, w1=0.20020566226500244\n",
      "Gradient Descent(70/99): loss =2080791198384100.0, w0=5.4635696448245775e-05, w1=0.20020859521954132\n",
      "Gradient Descent(71/99): loss =2080119604175405.0, w0=5.5353852407166014e-05, w1=0.2002115274629878\n",
      "Gradient Descent(72/99): loss =2079448335320523.8, w0=5.607191169880591e-05, w1=0.20021445899558873\n",
      "Gradient Descent(73/99): loss =2078777391611906.5, w0=5.678987433140856e-05, w1=0.20021738981759127\n",
      "Gradient Descent(74/99): loss =2078106772842157.8, w0=5.7507740313425665e-05, w1=0.2002203199292427\n",
      "Gradient Descent(75/99): loss =2077436478804092.5, w0=5.82255096535149e-05, w1=0.20022324933079041\n",
      "Gradient Descent(76/99): loss =2076766509290609.8, w0=5.89431823605368e-05, w1=0.2002261780224819\n",
      "Gradient Descent(77/99): loss =2076096864094833.5, w0=5.966075844355126e-05, w1=0.20022910600456462\n",
      "Gradient Descent(78/99): loss =2075427543009988.8, w0=6.037823791181375e-05, w1=0.20023203327728598\n",
      "Gradient Descent(79/99): loss =2074758545829506.5, w0=6.109562077477128e-05, w1=0.20023495984089335\n",
      "Gradient Descent(80/99): loss =2074089872346941.0, w0=6.181290704205819e-05, w1=0.20023788569563394\n",
      "Gradient Descent(81/99): loss =2073421522355996.0, w0=6.253009672349185e-05, w1=0.20024081084175493\n",
      "Gradient Descent(82/99): loss =2072753495650565.0, w0=6.32471898290682e-05, w1=0.20024373527950332\n",
      "Gradient Descent(83/99): loss =2072085792024678.0, w0=6.396418636895731e-05, w1=0.2002466590091259\n",
      "Gradient Descent(84/99): loss =2071418411272495.5, w0=6.468108635349885e-05, w1=0.20024958203086934\n",
      "Gradient Descent(85/99): loss =2070751353188354.0, w0=6.539788979319763e-05, w1=0.20025250434498015\n",
      "Gradient Descent(86/99): loss =2070084617566748.5, w0=6.611459669871907e-05, w1=0.2002554259517046\n",
      "Gradient Descent(87/99): loss =2069418204202308.5, w0=6.683120708088477e-05, w1=0.20025834685128877\n",
      "Gradient Descent(88/99): loss =2068752112889824.5, w0=6.754772095066817e-05, w1=0.2002612670439786\n",
      "Gradient Descent(89/99): loss =2068086343424232.0, w0=6.826413831919014e-05, w1=0.20026418653001973\n",
      "Gradient Descent(90/99): loss =2067420895600622.8, w0=6.898045919771474e-05, w1=0.2002671053096577\n",
      "Gradient Descent(91/99): loss =2066755769214237.8, w0=6.96966835976451e-05, w1=0.20027002338313776\n",
      "Gradient Descent(92/99): loss =2066090964060447.5, w0=7.04128115305192e-05, w1=0.200272940750705\n",
      "Gradient Descent(93/99): loss =2065426479934820.0, w0=7.112884300800595e-05, w1=0.2002758574126043\n",
      "Gradient Descent(94/99): loss =2064762316633030.0, w0=7.184477804190121e-05, w1=0.20027877336908032\n",
      "Gradient Descent(95/99): loss =2064098473950910.0, w0=7.256061664412401e-05, w1=0.20028168862037748\n",
      "Gradient Descent(96/99): loss =2063434951684441.5, w0=7.327635882671279e-05, w1=0.20028460316674007\n",
      "Gradient Descent(97/99): loss =2062771749629773.2, w0=7.399200460182174e-05, w1=0.20028751700841213\n",
      "Gradient Descent(98/99): loss =2062108867583172.2, w0=7.470755398171729e-05, w1=0.20029043014563752\n",
      "Gradient Descent(99/99): loss =2061446305341058.5, w0=7.542300697877464e-05, w1=0.20029334257865988\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =6.881612850879229e+38, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =7.744234201705451e+20, w0=-0.19999999978187943, w1=0.20000000043170046\n",
      "Gradient Descent(3/99): loss =7.725819660488001e+20, w0=-0.19999999957013037, w1=0.20000000086121025\n",
      "Gradient Descent(4/99): loss =7.710860314001867e+20, w0=-0.19999999935795323, w1=0.20000000128920917\n",
      "Gradient Descent(5/99): loss =7.696276229890309e+20, w0=-0.19999999914666045, w1=0.2000000017159927\n",
      "Gradient Descent(6/99): loss =7.681877416098327e+20, w0=-0.19999999893581985, w1=0.20000000214161756\n",
      "Gradient Descent(7/99): loss =7.667643866819089e+20, w0=-0.19999999872553617, w1=0.20000000256616274\n",
      "Gradient Descent(8/99): loss =7.65356996498485e+20, w0=-0.19999999851576045, w1=0.2000000029896824\n",
      "Gradient Descent(9/99): loss =7.639651589659794e+20, w0=-0.19999999830648957, w1=0.20000000341222424\n",
      "Gradient Descent(10/99): loss =7.625884955702525e+20, w0=-0.1999999980977079, w1=0.20000000383382668\n",
      "Gradient Descent(11/99): loss =7.612266506131222e+20, w0=-0.19999999788940429, w1=0.20000000425452177\n",
      "Gradient Descent(12/99): loss =7.598792882910984e+20, w0=-0.19999999768156693, w1=0.20000000467433593\n",
      "Gradient Descent(13/99): loss =7.585460910171133e+20, w0=-0.1999999974741849, w1=0.2000000050932911\n",
      "Gradient Descent(14/99): loss =7.572267579350918e+20, w0=-0.1999999972672476, w1=0.2000000055114056\n",
      "Gradient Descent(15/99): loss =7.559210036220842e+20, w0=-0.19999999706074487, w1=0.20000000592869466\n",
      "Gradient Descent(16/99): loss =7.546285569228213e+20, w0=-0.19999999685466693, w1=0.20000000634517115\n",
      "Gradient Descent(17/99): loss =7.533491599007296e+20, w0=-0.19999999664900442, w1=0.20000000676084584\n",
      "Gradient Descent(18/99): loss =7.520825668871309e+20, w0=-0.19999999644374825, w1=0.2000000071757279\n",
      "Gradient Descent(19/99): loss =7.508285436157488e+20, w0=-0.1999999962388897, w1=0.2000000075898252\n",
      "Gradient Descent(20/99): loss =7.495868664317292e+20, w0=-0.19999999603442034, w1=0.20000000800314444\n",
      "Gradient Descent(21/99): loss =7.483573215665075e+20, w0=-0.19999999583033204, w1=0.20000000841569146\n",
      "Gradient Descent(22/99): loss =7.471397044712774e+20, w0=-0.19999999562661694, w1=0.20000000882747138\n",
      "Gradient Descent(23/99): loss =7.459338192030322e+20, w0=-0.19999999542326746, w1=0.2000000092384887\n",
      "Gradient Descent(24/99): loss =7.44739477858001e+20, w0=-0.19999999522027628, w1=0.2000000096487475\n",
      "Gradient Descent(25/99): loss =7.435565000480636e+20, w0=-0.19999999501763627, w1=0.20000001005825144\n",
      "Gradient Descent(26/99): loss =7.423847124162904e+20, w0=-0.1999999948153406, w1=0.2000000104670038\n",
      "Gradient Descent(27/99): loss =7.412239481882044e+20, w0=-0.1999999946133826, w1=0.2000000108750077\n",
      "Gradient Descent(28/99): loss =7.400740467557899e+20, w0=-0.1999999944117559, w1=0.20000001128226605\n",
      "Gradient Descent(29/99): loss =7.38934853291553e+20, w0=-0.19999999421045425, w1=0.2000000116887815\n",
      "Gradient Descent(30/99): loss =7.378062183902387e+20, w0=-0.19999999400947163, w1=0.20000001209455667\n",
      "Gradient Descent(31/99): loss =7.366879977360401e+20, w0=-0.19999999380880223, w1=0.200000012499594\n",
      "Gradient Descent(32/99): loss =7.355800517933357e+20, w0=-0.19999999360844042, w1=0.20000001290389594\n",
      "Gradient Descent(33/99): loss =7.344822455191533e+20, w0=-0.19999999340838073, w1=0.20000001330746478\n",
      "Gradient Descent(34/99): loss =7.333944480957421e+20, w0=-0.19999999320861786, w1=0.2000000137103028\n",
      "Gradient Descent(35/99): loss =7.323165326817532e+20, w0=-0.1999999930091467, w1=0.20000001411241225\n",
      "Gradient Descent(36/99): loss =7.312483761806496e+20, w0=-0.19999999280996228, w1=0.20000001451379534\n",
      "Gradient Descent(37/99): loss =7.301898590250981e+20, w0=-0.19999999261105977, w1=0.20000001491445427\n",
      "Gradient Descent(38/99): loss =7.291408649761718e+20, w0=-0.19999999241243452, w1=0.2000000153143912\n",
      "Gradient Descent(39/99): loss =7.281012809362945e+20, w0=-0.19999999221408202, w1=0.20000001571360831\n",
      "Gradient Descent(40/99): loss =7.270709967749389e+20, w0=-0.19999999201599786, w1=0.20000001611210777\n",
      "Gradient Descent(41/99): loss =7.260499051661706e+20, w0=-0.19999999181817782, w1=0.20000001650989171\n",
      "Gradient Descent(42/99): loss =7.25037901437188e+20, w0=-0.19999999162061777, w1=0.20000001690696234\n",
      "Gradient Descent(43/99): loss =7.240348834270765e+20, w0=-0.1999999914233137, w1=0.2000000173033218\n",
      "Gradient Descent(44/99): loss =7.230407513550562e+20, w0=-0.19999999122626175, w1=0.20000001769897222\n",
      "Gradient Descent(45/99): loss =7.220554076975529e+20, w0=-0.19999999102945815, w1=0.20000001809391582\n",
      "Gradient Descent(46/99): loss =7.210787570734672e+20, w0=-0.19999999083289927, w1=0.20000001848815477\n",
      "Gradient Descent(47/99): loss =7.20110706137073e+20, w0=-0.19999999063658153, w1=0.20000001888169122\n",
      "Gradient Descent(48/99): loss =7.191511634780029e+20, w0=-0.1999999904405015, w1=0.20000001927452737\n",
      "Gradient Descent(49/99): loss =7.182000395278275e+20, w0=-0.1999999902446559, w1=0.20000001966666542\n",
      "Gradient Descent(50/99): loss =7.172572464727703e+20, w0=-0.19999999004904143, w1=0.20000002005810757\n",
      "Gradient Descent(51/99): loss =7.163226981721301e+20, w0=-0.19999998985365497, w1=0.20000002044885604\n",
      "Gradient Descent(52/99): loss =7.153963100820105e+20, w0=-0.19999998965849347, w1=0.200000020838913\n",
      "Gradient Descent(53/99): loss =7.144779991839915e+20, w0=-0.19999998946355396, w1=0.2000000212282807\n",
      "Gradient Descent(54/99): loss =7.135676839184012e+20, w0=-0.19999998926883356, w1=0.20000002161696134\n",
      "Gradient Descent(55/99): loss =7.126652841218657e+20, w0=-0.19999998907432945, w1=0.20000002200495717\n",
      "Gradient Descent(56/99): loss =7.117707209688462e+20, w0=-0.19999998888003895, w1=0.2000000223922704\n",
      "Gradient Descent(57/99): loss =7.108839169168829e+20, w0=-0.19999998868595936, w1=0.2000000227789033\n",
      "Gradient Descent(58/99): loss =7.100047956552902e+20, w0=-0.19999998849208814, w1=0.2000000231648581\n",
      "Gradient Descent(59/99): loss =7.091332820570812e+20, w0=-0.19999998829842278, w1=0.20000002355013702\n",
      "Gradient Descent(60/99): loss =7.082693021338717e+20, w0=-0.19999998810496086, w1=0.2000000239347423\n",
      "Gradient Descent(61/99): loss =7.074127829935851e+20, w0=-0.1999999879117, w1=0.20000002431867622\n",
      "Gradient Descent(62/99): loss =7.065636528007517e+20, w0=-0.19999998771863792, w1=0.20000002470194103\n",
      "Gradient Descent(63/99): loss =7.057218407392319e+20, w0=-0.19999998752577236, w1=0.20000002508453896\n",
      "Gradient Descent(64/99): loss =7.04887276977194e+20, w0=-0.19999998733310115, w1=0.20000002546647228\n",
      "Gradient Descent(65/99): loss =7.040598926341972e+20, w0=-0.19999998714062214, w1=0.20000002584774323\n",
      "Gradient Descent(66/99): loss =7.03239619750229e+20, w0=-0.1999999869483333, w1=0.2000000262283541\n",
      "Gradient Descent(67/99): loss =7.024263912565756e+20, w0=-0.19999998675623257, w1=0.20000002660830715\n",
      "Gradient Descent(68/99): loss =7.01620140948385e+20, w0=-0.19999998656431803, w1=0.2000000269876046\n",
      "Gradient Descent(69/99): loss =7.008208034588329e+20, w0=-0.19999998637258776, w1=0.20000002736624872\n",
      "Gradient Descent(70/99): loss =7.000283142347492e+20, w0=-0.1999999861810399, w1=0.20000002774424178\n",
      "Gradient Descent(71/99): loss =6.992426095136406e+20, w0=-0.19999998598967264, w1=0.20000002812158602\n",
      "Gradient Descent(72/99): loss =6.984636263019973e+20, w0=-0.1999999857984842, w1=0.20000002849828372\n",
      "Gradient Descent(73/99): loss =6.976913023547931e+20, w0=-0.19999998560747284, w1=0.2000000288743371\n",
      "Gradient Descent(74/99): loss =6.969255761561116e+20, w0=-0.1999999854166369, w1=0.20000002924974844\n",
      "Gradient Descent(75/99): loss =6.961663869008221e+20, w0=-0.19999998522597476, w1=0.20000002962452\n",
      "Gradient Descent(76/99): loss =6.954136744772229e+20, w0=-0.19999998503548477, w1=0.200000029998654\n",
      "Gradient Descent(77/99): loss =6.94667379450604e+20, w0=-0.1999999848451654, w1=0.2000000303721527\n",
      "Gradient Descent(78/99): loss =6.939274430476508e+20, w0=-0.19999998465501512, w1=0.20000003074501832\n",
      "Gradient Descent(79/99): loss =6.93193807141653e+20, w0=-0.19999998446503242, w1=0.20000003111725312\n",
      "Gradient Descent(80/99): loss =6.92466414238448e+20, w0=-0.19999998427521584, w1=0.20000003148885934\n",
      "Gradient Descent(81/99): loss =6.917452074630614e+20, w0=-0.199999984085564, w1=0.2000000318598392\n",
      "Gradient Descent(82/99): loss =6.910301305469901e+20, w0=-0.19999998389607546, w1=0.20000003223019494\n",
      "Gradient Descent(83/99): loss =6.903211278160997e+20, w0=-0.1999999837067489, w1=0.2000000325999288\n",
      "Gradient Descent(84/99): loss =6.896181441790807e+20, w0=-0.19999998351758297, w1=0.20000003296904295\n",
      "Gradient Descent(85/99): loss =6.88921125116436e+20, w0=-0.19999998332857638, w1=0.20000003333753966\n",
      "Gradient Descent(86/99): loss =6.882300166699669e+20, w0=-0.19999998313972786, w1=0.2000000337054211\n",
      "Gradient Descent(87/99): loss =6.875447654327176e+20, w0=-0.19999998295103616, w1=0.20000003407268951\n",
      "Gradient Descent(88/99): loss =6.868653185393609e+20, w0=-0.19999998276250006, w1=0.2000000344393471\n",
      "Gradient Descent(89/99): loss =6.86191623656979e+20, w0=-0.19999998257411838, w1=0.20000003480539605\n",
      "Gradient Descent(90/99): loss =6.855236289762388e+20, w0=-0.19999998238588995, w1=0.20000003517083856\n",
      "Gradient Descent(91/99): loss =6.848612832029135e+20, w0=-0.19999998219781365, w1=0.2000000355356768\n",
      "Gradient Descent(92/99): loss =6.842045355497398e+20, w0=-0.19999998200988833, w1=0.20000003589991297\n",
      "Gradient Descent(93/99): loss =6.835533357285965e+20, w0=-0.1999999818221129, w1=0.20000003626354923\n",
      "Gradient Descent(94/99): loss =6.829076339429655e+20, w0=-0.1999999816344863, w1=0.20000003662658777\n",
      "Gradient Descent(95/99): loss =6.822673808806799e+20, w0=-0.19999998144700745, w1=0.20000003698903074\n",
      "Gradient Descent(96/99): loss =6.816325277069241e+20, w0=-0.19999998125967536, w1=0.20000003735088032\n",
      "Gradient Descent(97/99): loss =6.810030260574792e+20, w0=-0.19999998107248898, w1=0.20000003771213865\n",
      "Gradient Descent(98/99): loss =6.803788280322022e+20, w0=-0.19999998088544735, w1=0.20000003807280786\n",
      "Gradient Descent(99/99): loss =6.79759886188712e+20, w0=-0.19999998069854946, w1=0.20000003843289013\n",
      "Optimizing degree 12/15, model: least_squares_GD, arguments: {'max_iters': 100}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =1.1267307619808637e+43, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =2.4342448389806998e+23, w0=-0.1999999999714385, w1=0.2000000000535338\n",
      "Gradient Descent(3/99): loss =2.4316575800106785e+23, w0=-0.19999999994309095, w1=0.20000000010702848\n",
      "Gradient Descent(4/99): loss =2.4291433800629722e+23, w0=-0.19999999991477174, w1=0.200000000160434\n",
      "Gradient Descent(5/99): loss =2.4266439289749374e+23, w0=-0.19999999988647787, w1=0.2000000002137549\n",
      "Gradient Descent(6/99): loss =2.4241589221623613e+23, w0=-0.19999999985820846, w1=0.20000000026699483\n",
      "Gradient Descent(7/99): loss =2.4216881690984662e+23, w0=-0.19999999982996305, w1=0.20000000032015683\n",
      "Gradient Descent(8/99): loss =2.4192314940021606e+23, w0=-0.1999999998017413, w1=0.20000000037324353\n",
      "Gradient Descent(9/99): loss =2.416788733356116e+23, w0=-0.19999999977354285, w1=0.20000000042625715\n",
      "Gradient Descent(10/99): loss =2.4143597343210592e+23, w0=-0.1999999997453674, w1=0.20000000047919955\n",
      "Gradient Descent(11/99): loss =2.4119443534123334e+23, w0=-0.19999999971721458, w1=0.20000000053207231\n",
      "Gradient Descent(12/99): loss =2.4095424553831388e+23, w0=-0.19999999968908416, w1=0.20000000058487677\n",
      "Gradient Descent(13/99): loss =2.407153912274728e+23, w0=-0.19999999966097584, w1=0.20000000063761408\n",
      "Gradient Descent(14/99): loss =2.4047786026026485e+23, w0=-0.19999999963288934, w1=0.2000000006902852\n",
      "Gradient Descent(15/99): loss =2.402416410654981e+23, w0=-0.1999999996048244, w1=0.20000000074289098\n",
      "Gradient Descent(16/99): loss =2.4000672258834676e+23, w0=-0.1999999995767808, w1=0.20000000079543212\n",
      "Gradient Descent(17/99): loss =2.397730942372497e+23, w0=-0.1999999995487583, w1=0.20000000084790923\n",
      "Gradient Descent(18/99): loss =2.3954074583737622e+23, w0=-0.19999999952075667, w1=0.20000000090032283\n",
      "Gradient Descent(19/99): loss =2.3930966758968293e+23, w0=-0.19999999949277572, w1=0.2000000009526734\n",
      "Gradient Descent(20/99): loss =2.390798500347614e+23, w0=-0.19999999946481523, w1=0.20000000100496132\n",
      "Gradient Descent(21/99): loss =2.3885128402081552e+23, w0=-0.19999999943687502, w1=0.20000000105718696\n",
      "Gradient Descent(22/99): loss =2.3862396067522383e+23, w0=-0.19999999940895488, w1=0.20000000110935062\n",
      "Gradient Descent(23/99): loss =2.3839787137922685e+23, w0=-0.19999999938105464, w1=0.20000000116145258\n",
      "Gradient Descent(24/99): loss =2.3817300774535167e+23, w0=-0.19999999935317414, w1=0.20000000121349312\n",
      "Gradient Descent(25/99): loss =2.379493615972455e+23, w0=-0.1999999993253132, w1=0.20000000126547243\n",
      "Gradient Descent(26/99): loss =2.37726924951633e+23, w0=-0.19999999929747167, w1=0.20000000131739074\n",
      "Gradient Descent(27/99): loss =2.3750569000215615e+23, w0=-0.1999999992696494, w1=0.2000000013692482\n",
      "Gradient Descent(28/99): loss =2.3728564910488314e+23, w0=-0.19999999924184625, w1=0.20000000142104501\n",
      "Gradient Descent(29/99): loss =2.3706679476530157e+23, w0=-0.19999999921406206, w1=0.20000000147278135\n",
      "Gradient Descent(30/99): loss =2.3684911962663565e+23, w0=-0.19999999918629668, w1=0.20000000152445738\n",
      "Gradient Descent(31/99): loss =2.366326164593448e+23, w0=-0.19999999915855002, w1=0.20000000157607323\n",
      "Gradient Descent(32/99): loss =2.3641727815167605e+23, w0=-0.19999999913082192, w1=0.20000000162762907\n",
      "Gradient Descent(33/99): loss =2.3620309770116195e+23, w0=-0.19999999910311225, w1=0.20000000167912504\n",
      "Gradient Descent(34/99): loss =2.3599006820696376e+23, w0=-0.19999999907542093, w1=0.20000000173056126\n",
      "Gradient Descent(35/99): loss =2.3577818286297322e+23, w0=-0.19999999904774782, w1=0.2000000017819379\n",
      "Gradient Descent(36/99): loss =2.3556743495159225e+23, w0=-0.1999999990200928, w1=0.20000000183325503\n",
      "Gradient Descent(37/99): loss =2.3535781783812447e+23, w0=-0.19999999899245577, w1=0.20000000188451283\n",
      "Gradient Descent(38/99): loss =2.35149324965714e+23, w0=-0.1999999989648366, w1=0.2000000019357114\n",
      "Gradient Descent(39/99): loss =2.3494194985077556e+23, w0=-0.19999999893723525, w1=0.2000000019868509\n",
      "Gradient Descent(40/99): loss =2.3473568607886637e+23, w0=-0.19999999890965156, w1=0.20000000203793142\n",
      "Gradient Descent(41/99): loss =2.3453052730095705e+23, w0=-0.19999999888208547, w1=0.20000000208895313\n",
      "Gradient Descent(42/99): loss =2.3432646723005788e+23, w0=-0.1999999988545369, w1=0.20000000213991614\n",
      "Gradient Descent(43/99): loss =2.3412349963816597e+23, w0=-0.19999999882700573, w1=0.20000000219082056\n",
      "Gradient Descent(44/99): loss =2.339216183535033e+23, w0=-0.19999999879949187, w1=0.20000000224166656\n",
      "Gradient Descent(45/99): loss =2.3372081725801264e+23, w0=-0.19999999877199526, w1=0.2000000022924542\n",
      "Gradient Descent(46/99): loss =2.3352109028508755e+23, w0=-0.1999999987445158, w1=0.20000000234318369\n",
      "Gradient Descent(47/99): loss =2.3332243141751348e+23, w0=-0.1999999987170534, w1=0.2000000023938551\n",
      "Gradient Descent(48/99): loss =2.3312483468559733e+23, w0=-0.19999999868960802, w1=0.20000000244446858\n",
      "Gradient Descent(49/99): loss =2.329282941654681e+23, w0=-0.19999999866217955, w1=0.20000000249502425\n",
      "Gradient Descent(50/99): loss =2.3273280397752953e+23, w0=-0.19999999863476792, w1=0.20000000254552225\n",
      "Gradient Descent(51/99): loss =2.3253835828505247e+23, w0=-0.19999999860737305, w1=0.2000000025959627\n",
      "Gradient Descent(52/99): loss =2.323449512928903e+23, w0=-0.1999999985799949, w1=0.20000000264634574\n",
      "Gradient Descent(53/99): loss =2.3215257724630645e+23, w0=-0.19999999855263337, w1=0.20000000269667148\n",
      "Gradient Descent(54/99): loss =2.3196123042990326e+23, w0=-0.1999999985252884, w1=0.20000000274694008\n",
      "Gradient Descent(55/99): loss =2.3177090516664086e+23, w0=-0.19999999849795994, w1=0.20000000279715166\n",
      "Gradient Descent(56/99): loss =2.3158159581693862e+23, w0=-0.1999999984706479, w1=0.20000000284730635\n",
      "Gradient Descent(57/99): loss =2.3139329677784847e+23, w0=-0.19999999844335223, w1=0.20000000289740427\n",
      "Gradient Descent(58/99): loss =2.312060024822978e+23, w0=-0.19999999841607286, w1=0.20000000294744558\n",
      "Gradient Descent(59/99): loss =2.3101970739838837e+23, w0=-0.19999999838880972, w1=0.20000000299743037\n",
      "Gradient Descent(60/99): loss =2.308344060287517e+23, w0=-0.19999999836156276, w1=0.20000000304735882\n",
      "Gradient Descent(61/99): loss =2.306500929099524e+23, w0=-0.19999999833433194, w1=0.20000000309723104\n",
      "Gradient Descent(62/99): loss =2.3046676261193545e+23, w0=-0.19999999830711718, w1=0.20000000314704716\n",
      "Gradient Descent(63/99): loss =2.3028440973751162e+23, w0=-0.19999999827991843, w1=0.20000000319680733\n",
      "Gradient Descent(64/99): loss =2.3010302892188066e+23, w0=-0.19999999825273562, w1=0.20000000324651165\n",
      "Gradient Descent(65/99): loss =2.2992261483218516e+23, w0=-0.19999999822556871, w1=0.20000000329616027\n",
      "Gradient Descent(66/99): loss =2.2974316216709413e+23, w0=-0.19999999819841766, w1=0.20000000334575335\n",
      "Gradient Descent(67/99): loss =2.29564665656412e+23, w0=-0.1999999981712824, w1=0.200000003395291\n",
      "Gradient Descent(68/99): loss =2.2938712006071288e+23, w0=-0.19999999814416286, w1=0.20000000344477334\n",
      "Gradient Descent(69/99): loss =2.292105201709941e+23, w0=-0.199999998117059, w1=0.20000000349420052\n",
      "Gradient Descent(70/99): loss =2.2903486080835024e+23, w0=-0.1999999980899708, w1=0.2000000035435727\n",
      "Gradient Descent(71/99): loss =2.288601368236656e+23, w0=-0.19999999806289814, w1=0.20000000359288997\n",
      "Gradient Descent(72/99): loss =2.2868634309732006e+23, w0=-0.19999999803584104, w1=0.20000000364215248\n",
      "Gradient Descent(73/99): loss =2.2851347453891167e+23, w0=-0.19999999800879942, w1=0.20000000369136037\n",
      "Gradient Descent(74/99): loss =2.2834152608698992e+23, w0=-0.19999999798177323, w1=0.20000000374051377\n",
      "Gradient Descent(75/99): loss =2.2817049270880368e+23, w0=-0.19999999795476242, w1=0.2000000037896128\n",
      "Gradient Descent(76/99): loss =2.2800036940005677e+23, w0=-0.19999999792776696, w1=0.2000000038386576\n",
      "Gradient Descent(77/99): loss =2.2783115118467496e+23, w0=-0.1999999979007868, w1=0.20000000388764833\n",
      "Gradient Descent(78/99): loss =2.276628331145815e+23, w0=-0.19999999787382186, w1=0.2000000039365851\n",
      "Gradient Descent(79/99): loss =2.2749541026948054e+23, w0=-0.19999999784687214, w1=0.20000000398546802\n",
      "Gradient Descent(80/99): loss =2.273288777566472e+23, w0=-0.19999999781993757, w1=0.20000000403429727\n",
      "Gradient Descent(81/99): loss =2.2716323071072597e+23, w0=-0.1999999977930181, w1=0.20000000408307295\n",
      "Gradient Descent(82/99): loss =2.269984642935328e+23, w0=-0.1999999977661137, w1=0.2000000041317952\n",
      "Gradient Descent(83/99): loss =2.2683457369386506e+23, w0=-0.19999999773922433, w1=0.20000000418046415\n",
      "Gradient Descent(84/99): loss =2.2667155412731444e+23, w0=-0.19999999771234991, w1=0.20000000422907996\n",
      "Gradient Descent(85/99): loss =2.2650940083608626e+23, w0=-0.19999999768549043, w1=0.20000000427764272\n",
      "Gradient Descent(86/99): loss =2.2634810908882194e+23, w0=-0.19999999765864585, w1=0.20000000432615261\n",
      "Gradient Descent(87/99): loss =2.2618767418042534e+23, w0=-0.19999999763181608, w1=0.20000000437460974\n",
      "Gradient Descent(88/99): loss =2.260280914318941e+23, w0=-0.19999999760500115, w1=0.20000000442301422\n",
      "Gradient Descent(89/99): loss =2.2586935619015177e+23, w0=-0.19999999757820097, w1=0.2000000044713662\n",
      "Gradient Descent(90/99): loss =2.2571146382788524e+23, w0=-0.1999999975514155, w1=0.20000000451966582\n",
      "Gradient Descent(91/99): loss =2.2555440974338458e+23, w0=-0.19999999752464473, w1=0.2000000045679132\n",
      "Gradient Descent(92/99): loss =2.253981893603835e+23, w0=-0.19999999749788858, w1=0.20000000461610845\n",
      "Gradient Descent(93/99): loss =2.252427981279059e+23, w0=-0.19999999747114702, w1=0.20000000466425175\n",
      "Gradient Descent(94/99): loss =2.2508823152011077e+23, w0=-0.19999999744442004, w1=0.2000000047123432\n",
      "Gradient Descent(95/99): loss =2.249344850361427e+23, w0=-0.19999999741770758, w1=0.2000000047603829\n",
      "Gradient Descent(96/99): loss =2.2478155419998157e+23, w0=-0.19999999739100957, w1=0.20000000480837105\n",
      "Gradient Descent(97/99): loss =2.246294345602948e+23, w0=-0.199999997364326, w1=0.20000000485630773\n",
      "Gradient Descent(98/99): loss =2.244781216902934e+23, w0=-0.19999999733765683, w1=0.20000000490419306\n",
      "Gradient Descent(99/99): loss =2.2432761118758725e+23, w0=-0.19999999731100201, w1=0.20000000495202722\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.1267307619808484e+43, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =1.8911034826239303e+23, w0=-0.1999999999835607, w1=0.2000000000005273\n",
      "Gradient Descent(3/99): loss =1.860929506996454e+23, w0=-0.19999999996886522, w1=0.19999999999742415\n",
      "Gradient Descent(4/99): loss =1.840592773349929e+23, w0=-0.19999999995474102, w1=0.1999999999917262\n",
      "Gradient Descent(5/99): loss =1.825858331794756e+23, w0=-0.1999999999411002, w1=0.199999999983841\n",
      "Gradient Descent(6/99): loss =1.815179891844891e+23, w0=-0.19999999992787032, w1=0.19999999997410595\n",
      "Gradient Descent(7/99): loss =1.8074388195381418e+23, w0=-0.1999999999149899, w1=0.19999999996280335\n",
      "Gradient Descent(8/99): loss =1.8018254479723657e+23, w0=-0.19999999990240683, w1=0.19999999995017054\n",
      "Gradient Descent(9/99): loss =1.7977535263393226e+23, w0=-0.19999999989007677, w1=0.19999999993640774\n",
      "Gradient Descent(10/99): loss =1.7947984591279908e+23, w0=-0.1999999998779621, w1=0.19999999992168438\n",
      "Gradient Descent(11/99): loss =1.792652671692509e+23, w0=-0.1999999998660308, w1=0.19999999990614403\n",
      "Gradient Descent(12/99): loss =1.791093327738981e+23, w0=-0.19999999985425557, w1=0.19999999988990852\n",
      "Gradient Descent(13/99): loss =1.7899589672556356e+23, w0=-0.19999999984261327, w1=0.19999999987308134\n",
      "Gradient Descent(14/99): loss =1.7891325929883594e+23, w0=-0.19999999983108419, w1=0.19999999985575045\n",
      "Gradient Descent(15/99): loss =1.78852942183816e+23, w0=-0.19999999981965152, w1=0.1999999998379907\n",
      "Gradient Descent(16/99): loss =1.7880880126282868e+23, w0=-0.19999999980830097, w1=0.19999999981986566\n",
      "Gradient Descent(17/99): loss =1.7877638385250552e+23, w0=-0.19999999979702038, w1=0.19999999980142952\n",
      "Gradient Descent(18/99): loss =1.7875246299834926e+23, w0=-0.19999999978579938, w1=0.1999999997827283\n",
      "Gradient Descent(19/99): loss =1.7873470002354356e+23, w0=-0.19999999977462918, w1=0.19999999976380128\n",
      "Gradient Descent(20/99): loss =1.7872139999646558e+23, w0=-0.19999999976350227, w1=0.19999999974468183\n",
      "Gradient Descent(21/99): loss =1.7871133452364736e+23, w0=-0.19999999975241228, w1=0.19999999972539834\n",
      "Gradient Descent(22/99): loss =1.7870361332789934e+23, w0=-0.19999999974135374, w1=0.19999999970597498\n",
      "Gradient Descent(23/99): loss =1.7869759117890484e+23, w0=-0.19999999973032204, w1=0.19999999968643237\n",
      "Gradient Descent(24/99): loss =1.786928004432059e+23, w0=-0.19999999971931323, w1=0.19999999966678803\n",
      "Gradient Descent(25/99): loss =1.786889022006972e+23, w0=-0.19999999970832397, w1=0.19999999964705686\n",
      "Gradient Descent(26/99): loss =1.786856508166415e+23, w0=-0.19999999969735138, w1=0.1999999996272516\n",
      "Gradient Descent(27/99): loss =1.7868286826532008e+23, w0=-0.19999999968639307, w1=0.19999999960738302\n",
      "Gradient Descent(28/99): loss =1.786804255210716e+23, w0=-0.19999999967544693, w1=0.19999999958746034\n",
      "Gradient Descent(29/99): loss =1.7867822907138454e+23, w0=-0.1999999996645112, w1=0.19999999956749143\n",
      "Gradient Descent(30/99): loss =1.7867621114218857e+23, w0=-0.19999999965358442, w1=0.19999999954748293\n",
      "Gradient Descent(31/99): loss =1.7867432261357104e+23, w0=-0.19999999964266527, w1=0.19999999952744052\n",
      "Gradient Descent(32/99): loss =1.7867252788539002e+23, w0=-0.1999999996317527, w1=0.19999999950736905\n",
      "Gradient Descent(33/99): loss =1.7867080115608458e+23, w0=-0.19999999962084572, w1=0.19999999948727262\n",
      "Gradient Descent(34/99): loss =1.7866912372571007e+23, w0=-0.1999999996099436, w1=0.19999999946715474\n",
      "Gradient Descent(35/99): loss =1.7866748204128835e+23, w0=-0.19999999959904566, w1=0.1999999994470184\n",
      "Gradient Descent(36/99): loss =1.7866586628015677e+23, w0=-0.19999999958815132, w1=0.19999999942686617\n",
      "Gradient Descent(37/99): loss =1.786642693232369e+23, w0=-0.1999999995772601, w1=0.19999999940670016\n",
      "Gradient Descent(38/99): loss =1.7866268601090043e+23, w0=-0.1999999995663716, w1=0.19999999938652221\n",
      "Gradient Descent(39/99): loss =1.7866111260365092e+23, w0=-0.19999999955548545, w1=0.19999999936633392\n",
      "Gradient Descent(40/99): loss =1.7865954639124788e+23, w0=-0.19999999954460135, w1=0.1999999993461366\n",
      "Gradient Descent(41/99): loss =1.7865798540941352e+23, w0=-0.19999999953371903, w1=0.1999999993259314\n",
      "Gradient Descent(42/99): loss =1.7865642823451546e+23, w0=-0.19999999952283828, w1=0.1999999993057193\n",
      "Gradient Descent(43/99): loss =1.7865487383476002e+23, w0=-0.19999999951195893, w1=0.1999999992855011\n",
      "Gradient Descent(44/99): loss =1.7865332146234148e+23, w0=-0.19999999950108083, w1=0.19999999926527753\n",
      "Gradient Descent(45/99): loss =1.7865177057527866e+23, w0=-0.1999999994902038, w1=0.19999999924504916\n",
      "Gradient Descent(46/99): loss =1.7865022078076362e+23, w0=-0.19999999947932778, w1=0.1999999992248165\n",
      "Gradient Descent(47/99): loss =1.786486717941027e+23, w0=-0.19999999946845265, w1=0.19999999920458\n",
      "Gradient Descent(48/99): loss =1.7864712340896024e+23, w0=-0.19999999945757832, w1=0.19999999918434\n",
      "Gradient Descent(49/99): loss =1.7864557547579057e+23, w0=-0.19999999944670474, w1=0.19999999916409683\n",
      "Gradient Descent(50/99): loss =1.786440278862071e+23, w0=-0.19999999943583183, w1=0.19999999914385075\n",
      "Gradient Descent(51/99): loss =1.7864248056165475e+23, w0=-0.19999999942495955, w1=0.199999999123602\n",
      "Gradient Descent(52/99): loss =1.78640933445198e+23, w0=-0.19999999941408786, w1=0.19999999910335076\n",
      "Gradient Descent(53/99): loss =1.7863938649557092e+23, w0=-0.1999999994032167, w1=0.1999999990830972\n",
      "Gradient Descent(54/99): loss =1.786378396828641e+23, w0=-0.19999999939234606, w1=0.19999999906284147\n",
      "Gradient Descent(55/99): loss =1.78636292985399e+23, w0=-0.1999999993814759, w1=0.19999999904258367\n",
      "Gradient Descent(56/99): loss =1.786347463874623e+23, w0=-0.1999999993706062, w1=0.1999999990223239\n",
      "Gradient Descent(57/99): loss =1.786331998776643e+23, w0=-0.19999999935973695, w1=0.19999999900206228\n",
      "Gradient Descent(58/99): loss =1.7863165344774834e+23, w0=-0.19999999934886814, w1=0.19999999898179885\n",
      "Gradient Descent(59/99): loss =1.7863010709172966e+23, w0=-0.19999999933799972, w1=0.19999999896153367\n",
      "Gradient Descent(60/99): loss =1.7862856080526844e+23, w0=-0.19999999932713172, w1=0.19999999894126683\n",
      "Gradient Descent(61/99): loss =1.7862701458521832e+23, w0=-0.1999999993162641, w1=0.19999999892099835\n",
      "Gradient Descent(62/99): loss =1.7862546842929746e+23, w0=-0.19999999930539689, w1=0.19999999890072825\n",
      "Gradient Descent(63/99): loss =1.7862392233585036e+23, w0=-0.19999999929453002, w1=0.1999999988804566\n",
      "Gradient Descent(64/99): loss =1.7862237630367604e+23, w0=-0.19999999928366355, w1=0.1999999988601834\n",
      "Gradient Descent(65/99): loss =1.7862083033190224e+23, w0=-0.19999999927279744, w1=0.1999999988399087\n",
      "Gradient Descent(66/99): loss =1.7861928441989573e+23, w0=-0.19999999926193168, w1=0.1999999988196325\n",
      "Gradient Descent(67/99): loss =1.7861773856719574e+23, w0=-0.1999999992510663, w1=0.19999999879935484\n",
      "Gradient Descent(68/99): loss =1.7861619277346676e+23, w0=-0.19999999924020123, w1=0.1999999987790757\n",
      "Gradient Descent(69/99): loss =1.786146470384647e+23, w0=-0.19999999922933653, w1=0.19999999875879512\n",
      "Gradient Descent(70/99): loss =1.786131013620108e+23, w0=-0.1999999992184722, w1=0.1999999987385131\n",
      "Gradient Descent(71/99): loss =1.7861155574397374e+23, w0=-0.1999999992076082, w1=0.19999999871822965\n",
      "Gradient Descent(72/99): loss =1.786100101842575e+23, w0=-0.19999999919674452, w1=0.1999999986979448\n",
      "Gradient Descent(73/99): loss =1.7860846468279057e+23, w0=-0.1999999991858812, w1=0.19999999867765852\n",
      "Gradient Descent(74/99): loss =1.786069192395197e+23, w0=-0.19999999917501823, w1=0.19999999865737084\n",
      "Gradient Descent(75/99): loss =1.7860537385440513e+23, w0=-0.1999999991641556, w1=0.19999999863708173\n",
      "Gradient Descent(76/99): loss =1.7860382852741605e+23, w0=-0.19999999915329328, w1=0.19999999861679124\n",
      "Gradient Descent(77/99): loss =1.7860228325852905e+23, w0=-0.19999999914243133, w1=0.19999999859649936\n",
      "Gradient Descent(78/99): loss =1.7860073804772565e+23, w0=-0.1999999991315697, w1=0.1999999985762061\n",
      "Gradient Descent(79/99): loss =1.7859919289499067e+23, w0=-0.19999999912070843, w1=0.1999999985559114\n",
      "Gradient Descent(80/99): loss =1.7859764780031238e+23, w0=-0.19999999910984748, w1=0.19999999853561534\n",
      "Gradient Descent(81/99): loss =1.7859610276367956e+23, w0=-0.19999999909898686, w1=0.19999999851531788\n",
      "Gradient Descent(82/99): loss =1.7859455778508393e+23, w0=-0.19999999908812657, w1=0.19999999849501907\n",
      "Gradient Descent(83/99): loss =1.7859301286451724e+23, w0=-0.19999999907726662, w1=0.19999999847471886\n",
      "Gradient Descent(84/99): loss =1.785914680019721e+23, w0=-0.199999999066407, w1=0.19999999845441727\n",
      "Gradient Descent(85/99): loss =1.7858992319744213e+23, w0=-0.19999999905554774, w1=0.19999999843411428\n",
      "Gradient Descent(86/99): loss =1.7858837845092073e+23, w0=-0.19999999904468882, w1=0.19999999841380992\n",
      "Gradient Descent(87/99): loss =1.7858683376240166e+23, w0=-0.19999999903383023, w1=0.19999999839350419\n",
      "Gradient Descent(88/99): loss =1.7858528913187937e+23, w0=-0.19999999902297197, w1=0.19999999837319707\n",
      "Gradient Descent(89/99): loss =1.785837445593481e+23, w0=-0.19999999901211404, w1=0.19999999835288856\n",
      "Gradient Descent(90/99): loss =1.7858220004480222e+23, w0=-0.19999999900125645, w1=0.1999999983325787\n",
      "Gradient Descent(91/99): loss =1.7858065558823604e+23, w0=-0.1999999989903992, w1=0.19999999831226745\n",
      "Gradient Descent(92/99): loss =1.7857911118964444e+23, w0=-0.19999999897954227, w1=0.1999999982919548\n",
      "Gradient Descent(93/99): loss =1.785775668490216e+23, w0=-0.19999999896868567, w1=0.1999999982716408\n",
      "Gradient Descent(94/99): loss =1.7857602256636242e+23, w0=-0.1999999989578294, w1=0.19999999825132542\n",
      "Gradient Descent(95/99): loss =1.7857447834166132e+23, w0=-0.19999999894697348, w1=0.19999999823100867\n",
      "Gradient Descent(96/99): loss =1.7857293417491313e+23, w0=-0.1999999989361179, w1=0.19999999821069053\n",
      "Gradient Descent(97/99): loss =1.785713900661124e+23, w0=-0.19999999892526263, w1=0.199999998190371\n",
      "Gradient Descent(98/99): loss =1.785698460152538e+23, w0=-0.1999999989144077, w1=0.19999999817005013\n",
      "Gradient Descent(99/99): loss =1.7856830202233216e+23, w0=-0.1999999989035531, w1=0.19999999814972785\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.5568747295995752e+29, w0=0.0, w1=0.2\n",
      "Gradient Descent(2/99): loss =2.527677719996927e+17, w0=2.1473604568828982e-07, w1=0.20000035142667114\n",
      "Gradient Descent(3/99): loss =1.0792190676529748e+16, w0=2.1974202668607617e-07, w1=0.20000051040499595\n",
      "Gradient Descent(4/99): loss =4334180809320510.5, w0=2.5810162407409725e-07, w1=0.2000006929282894\n",
      "Gradient Descent(5/99): loss =4158434283088361.5, w0=2.904615160642193e-07, w1=0.20000086706074133\n",
      "Gradient Descent(6/99): loss =4151971732390089.5, w0=3.2347354781204855e-07, w1=0.20000103994530558\n",
      "Gradient Descent(7/99): loss =4150636312861861.5, w0=3.561912545560828e-07, w1=0.20000121152578637\n",
      "Gradient Descent(8/99): loss =4149642085281511.0, w0=3.888475485657372e-07, w1=0.20000138244893062\n",
      "Gradient Descent(9/99): loss =4148725052037467.5, w0=4.2145006257245755e-07, w1=0.20000155297502617\n",
      "Gradient Descent(10/99): loss =4147832866008413.5, w0=4.540237362543672e-07, w1=0.20000172327108864\n",
      "Gradient Descent(11/99): loss =4146949081514648.5, w0=4.865796129932854e-07, w1=0.2000018934294745\n",
      "Gradient Descent(12/99): loss =4146068258542444.5, w0=5.191245893876028e-07, w1=0.20000206350310273\n",
      "Gradient Descent(13/99): loss =4145188588006771.5, w0=5.516625513907812e-07, w1=0.2000022335217279\n",
      "Gradient Descent(14/99): loss =4144309466608444.5, w0=5.841957494512839e-07, w1=0.2000024035019503\n",
      "Gradient Descent(15/99): loss =4143430692033949.5, w0=6.167254759550941e-07, w1=0.20000257345286185\n",
      "Gradient Descent(16/99): loss =4142552195592860.0, w0=6.492524736417154e-07, w1=0.20000274337930338\n",
      "Gradient Descent(17/99): loss =4141673953324293.0, w0=6.817771681336868e-07, w1=0.20000291328372738\n",
      "Gradient Descent(18/99): loss =4140795956390439.0, w0=7.14299802327163e-07, w1=0.2000030831672622\n",
      "Gradient Descent(19/99): loss =4139918201178131.5, w0=7.468205137653016e-07, w1=0.20000325303031727\n",
      "Gradient Descent(20/99): loss =4139040685961246.5, w0=7.793393792259391e-07, w1=0.20000342287292494\n",
      "Gradient Descent(21/99): loss =4138163409754252.5, w0=8.118564404127968e-07, w1=0.20000359269493173\n",
      "Gradient Descent(22/99): loss =4137286371903024.0, w0=8.443717187607399e-07, w1=0.20000376249610372\n",
      "Gradient Descent(23/99): loss =4136409571927574.5, w0=8.768852239699022e-07, w1=0.20000393227618316\n",
      "Gradient Descent(24/99): loss =4135533009453851.0, w0=9.093969589271903e-07, w1=0.2000041020349176\n",
      "Gradient Descent(25/99): loss =4134656684178979.5, w0=9.41906922546765e-07, w1=0.20000427177207383\n",
      "Gradient Descent(26/99): loss =4133780595850931.0, w0=9.74415111411736e-07, w1=0.20000444148744329\n",
      "Gradient Descent(27/99): loss =4132904744254912.0, w0=1.0069215207252277e-06, w1=0.20000461118084317\n",
      "Gradient Descent(28/99): loss =4132029129203871.5, w0=1.0394261448634725e-06, w1=0.2000047808521154\n",
      "Gradient Descent(29/99): loss =4131153750531542.0, w0=1.0719289776994494e-06, w1=0.20000495050112432\n",
      "Gradient Descent(30/99): loss =4130278608087320.0, w0=1.104430012794101e-06, w1=0.20000512012775432\n",
      "Gradient Descent(31/99): loss =4129403701732493.0, w0=1.136929243510982e-06, w1=0.20000528973190718\n",
      "Gradient Descent(32/99): loss =4128529031337260.5, w0=1.1694266630864963e-06, w1=0.20000545931349983\n",
      "Gradient Descent(33/99): loss =4127654596778726.5, w0=1.2019222646742323e-06, w1=0.20000562887246218\n",
      "Gradient Descent(34/99): loss =4126780397939227.5, w0=1.2344160413740566e-06, w1=0.2000057984087352\n",
      "Gradient Descent(35/99): loss =4125906434705122.0, w0=1.2669079862521169e-06, w1=0.2000059679222694\n",
      "Gradient Descent(36/99): loss =4125032706965924.0, w0=1.2993980923552986e-06, w1=0.2000061374130233\n",
      "Gradient Descent(37/99): loss =4124159214613577.0, w0=1.3318863527222016e-06, w1=0.20000630688096235\n",
      "Gradient Descent(38/99): loss =4123285957541986.5, w0=1.3643727603918327e-06, w1=0.20000647632605772\n",
      "Gradient Descent(39/99): loss =4122412935646626.5, w0=1.39685730841073e-06, w1=0.20000664574828564\n",
      "Gradient Descent(40/99): loss =4121540148824225.0, w0=1.4293399898389458e-06, w1=0.2000068151476265\n",
      "Gradient Descent(41/99): loss =4120667596972586.0, w0=1.4618207977551524e-06, w1=0.20000698452406426\n",
      "Gradient Descent(42/99): loss =4119795279990366.5, w0=1.494299725261042e-06, w1=0.20000715387758594\n",
      "Gradient Descent(43/99): loss =4118923197777006.5, w0=1.52677676548514e-06, w1=0.20000732320818113\n",
      "Gradient Descent(44/99): loss =4118051350232617.0, w0=1.559251911586112e-06, w1=0.2000074925158416\n",
      "Gradient Descent(45/99): loss =4117179737257888.5, w0=1.5917251567556325e-06, w1=0.200007661800561\n",
      "Gradient Descent(46/99): loss =4116308358754050.0, w0=1.6241964942208643e-06, w1=0.20000783106233458\n",
      "Gradient Descent(47/99): loss =4115437214622833.0, w0=1.6566659172465962e-06, w1=0.2000080003011589\n",
      "Gradient Descent(48/99): loss =4114566304766419.5, w0=1.6891334191370733e-06, w1=0.20000816951703165\n",
      "Gradient Descent(49/99): loss =4113695629087442.5, w0=1.7215989932375561e-06, w1=0.20000833870995155\n",
      "Gradient Descent(50/99): loss =4112825187488904.5, w0=1.7540626329356384e-06, w1=0.2000085078799181\n",
      "Gradient Descent(51/99): loss =4111954979874255.5, w0=1.786524331662347e-06, w1=0.20000867702693156\n",
      "Gradient Descent(52/99): loss =4111085006147278.0, w0=1.8189840828930562e-06, w1=0.20000884615099274\n",
      "Gradient Descent(53/99): loss =4110215266212157.5, w0=1.8514418801482283e-06, w1=0.20000901525210293\n",
      "Gradient Descent(54/99): loss =4109345759973420.0, w0=1.8838977169940083e-06, w1=0.20000918433026396\n",
      "Gradient Descent(55/99): loss =4108476487335950.0, w0=1.916351587042685e-06, w1=0.20000935338547793\n",
      "Gradient Descent(56/99): loss =4107607448204954.0, w0=1.9488034839530403e-06, w1=0.2000095224177473\n",
      "Gradient Descent(57/99): loss =4106738642486003.0, w0=1.981253401430591e-06, w1=0.2000096914270748\n",
      "Gradient Descent(58/99): loss =4105870070084980.5, w0=2.013701333227749e-06, w1=0.2000098604134634\n",
      "Gradient Descent(59/99): loss =4105001730908094.0, w0=2.046147273143898e-06, w1=0.20001002937691617\n",
      "Gradient Descent(60/99): loss =4104133624861854.5, w0=2.078591215025407e-06, w1=0.2000101983174365\n",
      "Gradient Descent(61/99): loss =4103265751853124.0, w0=2.111033152765584e-06, w1=0.2000103672350278\n",
      "Gradient Descent(62/99): loss =4102398111789035.0, w0=2.143473080304578e-06, w1=0.20001053612969366\n",
      "Gradient Descent(63/99): loss =4101530704577035.0, w0=2.1759109916292387e-06, w1=0.20001070500143778\n",
      "Gradient Descent(64/99): loss =4100663530124876.5, w0=2.208346880772939e-06, w1=0.2000108738502639\n",
      "Gradient Descent(65/99): loss =4099796588340607.5, w0=2.2407807418153646e-06, w1=0.20001104267617587\n",
      "Gradient Descent(66/99): loss =4098929879132549.0, w0=2.2732125688822763e-06, w1=0.20001121147917758\n",
      "Gradient Descent(67/99): loss =4098063402409322.0, w0=2.30564235614525e-06, w1=0.20001138025927298\n",
      "Gradient Descent(68/99): loss =4097197158079842.0, w0=2.338070097821397e-06, w1=0.2000115490164661\n",
      "Gradient Descent(69/99): loss =4096331146053281.0, w0=2.370495788173068e-06, w1=0.20001171775076096\n",
      "Gradient Descent(70/99): loss =4095465366239085.5, w0=2.4029194215075425e-06, w1=0.20001188646216164\n",
      "Gradient Descent(71/99): loss =4094599818547000.0, w0=2.435340992176711e-06, w1=0.20001205515067227\n",
      "Gradient Descent(72/99): loss =4093734502887000.5, w0=2.4677604945767434e-06, w1=0.20001222381629696\n",
      "Gradient Descent(73/99): loss =4092869419169356.5, w0=2.5001779231477546e-06, w1=0.20001239245903984\n",
      "Gradient Descent(74/99): loss =4092004567304584.0, w0=2.532593272373462e-06, w1=0.2000125610789051\n",
      "Gradient Descent(75/99): loss =4091139947203459.0, w0=2.565006536780842e-06, w1=0.20001272967589692\n",
      "Gradient Descent(76/99): loss =4090275558776997.5, w0=2.597417710939779e-06, w1=0.20001289825001947\n",
      "Gradient Descent(77/99): loss =4089411401936494.5, w0=2.629826789462717e-06, w1=0.20001306680127698\n",
      "Gradient Descent(78/99): loss =4088547476593478.0, w0=2.662233767004311e-06, w1=0.20001323532967366\n",
      "Gradient Descent(79/99): loss =4087683782659702.0, w0=2.6946386382610727e-06, w1=0.2000134038352137\n",
      "Gradient Descent(80/99): loss =4086820320047193.0, w0=2.727041397971023e-06, w1=0.20001357231790137\n",
      "Gradient Descent(81/99): loss =4085957088668187.5, w0=2.7594420409133415e-06, w1=0.20001374077774087\n",
      "Gradient Descent(82/99): loss =4085094088435190.0, w0=2.79184056190802e-06, w1=0.20001390921473644\n",
      "Gradient Descent(83/99): loss =4084231319260888.0, w0=2.8242369558155173e-06, w1=0.2000140776288923\n",
      "Gradient Descent(84/99): loss =4083368781058249.0, w0=2.8566312175364145e-06, w1=0.20001424602021273\n",
      "Gradient Descent(85/99): loss =4082506473740412.5, w0=2.8890233420110766e-06, w1=0.20001441438870193\n",
      "Gradient Descent(86/99): loss =4081644397220807.0, w0=2.9214133242193127e-06, w1=0.20001458273436418\n",
      "Gradient Descent(87/99): loss =4080782551413035.5, w0=2.9538011591800433e-06, w1=0.2000147510572037\n",
      "Gradient Descent(88/99): loss =4079920936230917.0, w0=2.986186841950967e-06, w1=0.20001491935722476\n",
      "Gradient Descent(89/99): loss =4079059551588502.5, w0=3.0185703676282337e-06, w1=0.20001508763443157\n",
      "Gradient Descent(90/99): loss =4078198397400035.0, w0=3.0509517313461184e-06, w1=0.2000152558888284\n",
      "Gradient Descent(91/99): loss =4077337473580020.0, w0=3.0833309282767004e-06, w1=0.2000154241204195\n",
      "Gradient Descent(92/99): loss =4076476780043091.0, w0=3.115707953629545e-06, w1=0.2000155923292091\n",
      "Gradient Descent(93/99): loss =4075616316704144.5, w0=3.1480828026513883e-06, w1=0.20001576051520145\n",
      "Gradient Descent(94/99): loss =4074756083478248.5, w0=3.180455470625828e-06, w1=0.2000159286784008\n",
      "Gradient Descent(95/99): loss =4073896080280688.0, w0=3.2128259528730137e-06, w1=0.20001609681881136\n",
      "Gradient Descent(96/99): loss =4073036307026938.0, w0=3.245194244749343e-06, w1=0.2000162649364374\n",
      "Gradient Descent(97/99): loss =4072176763632668.0, w0=3.2775603416471626e-06, w1=0.20001643303128316\n",
      "Gradient Descent(98/99): loss =4071317450013751.5, w0=3.3099242389944687e-06, w1=0.20001660110335287\n",
      "Gradient Descent(99/99): loss =4070458366086214.5, w0=3.3422859322546155e-06, w1=0.2000167691526508\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.1267307619808637e+43, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =2.43187504103518e+23, w0=-0.19999999997197102, w1=0.20000000005385135\n",
      "Gradient Descent(3/99): loss =2.428656045378149e+23, w0=-0.19999999994450202, w1=0.2000000001075663\n",
      "Gradient Descent(4/99): loss =2.4260464601506282e+23, w0=-0.1999999999169933, w1=0.20000000016117955\n",
      "Gradient Descent(5/99): loss =2.4234827198033546e+23, w0=-0.19999999988954587, w1=0.20000000021470649\n",
      "Gradient Descent(6/99): loss =2.4209379393825642e+23, w0=-0.19999999986213318, w1=0.20000000026815\n",
      "Gradient Descent(7/99): loss =2.4184103144000846e+23, w0=-0.19999999983476077, w1=0.2000000003215141\n",
      "Gradient Descent(8/99): loss =2.415899491770268e+23, w0=-0.1999999998074265, w1=0.20000000037480173\n",
      "Gradient Descent(9/99): loss =2.413405214404138e+23, w0=-0.19999999978013008, w1=0.20000000042801547\n",
      "Gradient Descent(10/99): loss =2.4109272404592008e+23, w0=-0.19999999975287086, w1=0.20000000048115746\n",
      "Gradient Descent(11/99): loss =2.4084653381068438e+23, w0=-0.19999999972564828, w1=0.20000000053422956\n",
      "Gradient Descent(12/99): loss =2.406019284459814e+23, w0=-0.19999999969846177, w1=0.20000000058723333\n",
      "Gradient Descent(13/99): loss =2.4035888649251284e+23, w0=-0.19999999967131082, w1=0.20000000064017007\n",
      "Gradient Descent(14/99): loss =2.4011738726474882e+23, w0=-0.19999999964419488, w1=0.2000000006930409\n",
      "Gradient Descent(15/99): loss =2.3987741080270276e+23, w0=-0.19999999961711343, w1=0.2000000007458468\n",
      "Gradient Descent(16/99): loss =2.3963893782922804e+23, w0=-0.19999999959006598, w1=0.20000000079858854\n",
      "Gradient Descent(17/99): loss =2.3940194971181196e+23, w0=-0.19999999956305206, w1=0.20000000085126682\n",
      "Gradient Descent(18/99): loss =2.391664284280136e+23, w0=-0.19999999953607117, w1=0.20000000090388223\n",
      "Gradient Descent(19/99): loss =2.3893235653391666e+23, w0=-0.19999999950912287, w1=0.2000000009564353\n",
      "Gradient Descent(20/99): loss =2.386997171351055e+23, w0=-0.1999999994822067, w1=0.20000000100892645\n",
      "Gradient Descent(21/99): loss =2.384684938597897e+23, w0=-0.19999999945532224, w1=0.20000000106135607\n",
      "Gradient Descent(22/99): loss =2.382386708337838e+23, w0=-0.19999999942846905, w1=0.20000000111372448\n",
      "Gradient Descent(23/99): loss =2.380102326571067e+23, w0=-0.1999999994016467, w1=0.20000000116603198\n",
      "Gradient Descent(24/99): loss =2.377831643820226e+23, w0=-0.19999999937485477, w1=0.20000000121827882\n",
      "Gradient Descent(25/99): loss =2.375574514923678e+23, w0=-0.1999999993480929, w1=0.20000000127046524\n",
      "Gradient Descent(26/99): loss =2.3733307988404553e+23, w0=-0.19999999932136064, w1=0.20000000132259144\n",
      "Gradient Descent(27/99): loss =2.371100358465853e+23, w0=-0.19999999929465767, w1=0.20000000137465757\n",
      "Gradient Descent(28/99): loss =2.3688830604568393e+23, w0=-0.19999999926798356, w1=0.20000000142666383\n",
      "Gradient Descent(29/99): loss =2.3666787750665224e+23, w0=-0.199999999241338, w1=0.20000000147861036\n",
      "Gradient Descent(30/99): loss =2.364487375987071e+23, w0=-0.19999999921472056, w1=0.20000000153049732\n",
      "Gradient Descent(31/99): loss =2.3623087402005285e+23, w0=-0.19999999918813094, w1=0.20000000158232484\n",
      "Gradient Descent(32/99): loss =2.3601427478370085e+23, w0=-0.19999999916156877, w1=0.200000001634093\n",
      "Gradient Descent(33/99): loss =2.357989282039862e+23, w0=-0.19999999913503375, w1=0.20000000168580198\n",
      "Gradient Descent(34/99): loss =2.3558482288373965e+23, w0=-0.1999999991085255, w1=0.20000000173745186\n",
      "Gradient Descent(35/99): loss =2.353719477020783e+23, w0=-0.19999999908204374, w1=0.20000000178904276\n",
      "Gradient Descent(36/99): loss =2.351602918027831e+23, w0=-0.19999999905558816, w1=0.20000000184057476\n",
      "Gradient Descent(37/99): loss =2.349498445832294e+23, w0=-0.1999999990291584, w1=0.20000000189204797\n",
      "Gradient Descent(38/99): loss =2.347405956838442e+23, w0=-0.19999999900275423, w1=0.2000000019434625\n",
      "Gradient Descent(39/99): loss =2.3453253497806176e+23, w0=-0.19999999897637527, w1=0.20000000199481846\n",
      "Gradient Descent(40/99): loss =2.3432565256275125e+23, w0=-0.1999999989500213, w1=0.20000000204611593\n",
      "Gradient Descent(41/99): loss =2.3411993874909647e+23, w0=-0.199999998923692, w1=0.20000000209735502\n",
      "Gradient Descent(42/99): loss =2.339153840539004e+23, w0=-0.1999999988973871, w1=0.2000000021485358\n",
      "Gradient Descent(43/99): loss =2.3371197919129745e+23, w0=-0.19999999887110634, w1=0.2000000021996584\n",
      "Gradient Descent(44/99): loss =2.335097150648503e+23, w0=-0.19999999884484942, w1=0.2000000022507229\n",
      "Gradient Descent(45/99): loss =2.333085827600156e+23, w0=-0.19999999881861613, w1=0.2000000023017294\n",
      "Gradient Descent(46/99): loss =2.3310857353695876e+23, w0=-0.19999999879240618, w1=0.20000000235267798\n",
      "Gradient Descent(47/99): loss =2.3290967882369925e+23, w0=-0.19999999876621932, w1=0.20000000240356874\n",
      "Gradient Descent(48/99): loss =2.3271189020957612e+23, w0=-0.19999999874005533, w1=0.2000000024544018\n",
      "Gradient Descent(49/99): loss =2.325151994390111e+23, w0=-0.19999999871391394, w1=0.20000000250517724\n",
      "Gradient Descent(50/99): loss =2.323195984055606e+23, w0=-0.19999999868779494, w1=0.20000000255589515\n",
      "Gradient Descent(51/99): loss =2.3212507914624025e+23, w0=-0.1999999986616981, w1=0.20000000260655562\n",
      "Gradient Descent(52/99): loss =2.3193163383610917e+23, w0=-0.19999999863562315, w1=0.20000000265715878\n",
      "Gradient Descent(53/99): loss =2.3173925478310244e+23, w0=-0.1999999986095699, w1=0.2000000027077047\n",
      "Gradient Descent(54/99): loss =2.31547934423099e+23, w0=-0.19999999858353815, w1=0.20000000275819352\n",
      "Gradient Descent(55/99): loss =2.3135766531521378e+23, w0=-0.19999999855752768, w1=0.2000000028086253\n",
      "Gradient Descent(56/99): loss =2.311684401373052e+23, w0=-0.19999999853153827, w1=0.20000000285900013\n",
      "Gradient Descent(57/99): loss =2.3098025168168484e+23, w0=-0.1999999985055697, w1=0.20000000290931816\n",
      "Gradient Descent(58/99): loss =2.3079309285102314e+23, w0=-0.19999999847962183, w1=0.20000000295957945\n",
      "Gradient Descent(59/99): loss =2.3060695665443803e+23, w0=-0.1999999984536944, w1=0.20000000300978413\n",
      "Gradient Descent(60/99): loss =2.3042183620376208e+23, w0=-0.19999999842778723, w1=0.20000000305993226\n",
      "Gradient Descent(61/99): loss =2.3023772470997647e+23, w0=-0.19999999840190016, w1=0.200000003110024\n",
      "Gradient Descent(62/99): loss =2.3005461547980456e+23, w0=-0.199999998376033, w1=0.20000000316005942\n",
      "Gradient Descent(63/99): loss =2.2987250191245972e+23, w0=-0.19999999835018553, w1=0.20000000321003863\n",
      "Gradient Descent(64/99): loss =2.2969137749653717e+23, w0=-0.1999999983243576, w1=0.20000000325996173\n",
      "Gradient Descent(65/99): loss =2.2951123580704573e+23, w0=-0.19999999829854906, w1=0.20000000330982884\n",
      "Gradient Descent(66/99): loss =2.2933207050256917e+23, w0=-0.1999999982727597, w1=0.20000000335964005\n",
      "Gradient Descent(67/99): loss =2.291538753225576e+23, w0=-0.19999999824698936, w1=0.20000000340939547\n",
      "Gradient Descent(68/99): loss =2.2897664408473482e+23, w0=-0.1999999982212379, w1=0.20000000345909522\n",
      "Gradient Descent(69/99): loss =2.2880037068262315e+23, w0=-0.19999999819550512, w1=0.2000000035087394\n",
      "Gradient Descent(70/99): loss =2.286250490831748e+23, w0=-0.19999999816979092, w1=0.20000000355832812\n",
      "Gradient Descent(71/99): loss =2.2845067332451004e+23, w0=-0.19999999814409508, w1=0.20000000360786147\n",
      "Gradient Descent(72/99): loss =2.282772375137526e+23, w0=-0.19999999811841748, w1=0.20000000365733958\n",
      "Gradient Descent(73/99): loss =2.2810473582495908e+23, w0=-0.19999999809275798, w1=0.20000000370676255\n",
      "Gradient Descent(74/99): loss =2.2793316249714195e+23, w0=-0.1999999980671164, w1=0.20000000375613047\n",
      "Gradient Descent(75/99): loss =2.2776251183237416e+23, w0=-0.19999999804149265, w1=0.2000000038054435\n",
      "Gradient Descent(76/99): loss =2.2759277819397867e+23, w0=-0.19999999801588655, w1=0.2000000038547017\n",
      "Gradient Descent(77/99): loss =2.274239560047953e+23, w0=-0.19999999799029797, w1=0.2000000039039052\n",
      "Gradient Descent(78/99): loss =2.2725603974552213e+23, w0=-0.19999999796472676, w1=0.20000000395305412\n",
      "Gradient Descent(79/99): loss =2.270890239531278e+23, w0=-0.1999999979391728, w1=0.20000000400214854\n",
      "Gradient Descent(80/99): loss =2.269229032193318e+23, w0=-0.19999999791363599, w1=0.20000000405118862\n",
      "Gradient Descent(81/99): loss =2.267576721891506e+23, w0=-0.19999999788811615, w1=0.20000000410017443\n",
      "Gradient Descent(82/99): loss =2.2659332555950264e+23, w0=-0.19999999786261316, w1=0.2000000041491061\n",
      "Gradient Descent(83/99): loss =2.2642985807787552e+23, w0=-0.1999999978371269, w1=0.20000000419798372\n",
      "Gradient Descent(84/99): loss =2.2626726454104674e+23, w0=-0.19999999781165728, w1=0.20000000424680744\n",
      "Gradient Descent(85/99): loss =2.261055397938596e+23, w0=-0.19999999778620417, w1=0.20000000429557735\n",
      "Gradient Descent(86/99): loss =2.2594467872804962e+23, w0=-0.19999999776076743, w1=0.20000000434429355\n",
      "Gradient Descent(87/99): loss =2.2578467628111824e+23, w0=-0.19999999773534696, w1=0.20000000439295618\n",
      "Gradient Descent(88/99): loss =2.256255274352572e+23, w0=-0.19999999770994265, w1=0.20000000444156532\n",
      "Gradient Descent(89/99): loss =2.2546722721631193e+23, w0=-0.19999999768455437, w1=0.2000000044901211\n",
      "Gradient Descent(90/99): loss =2.253097706927911e+23, w0=-0.19999999765918203, w1=0.20000000453862363\n",
      "Gradient Descent(91/99): loss =2.2515315297491504e+23, w0=-0.1999999976338255, w1=0.20000000458707304\n",
      "Gradient Descent(92/99): loss =2.2499736921370334e+23, w0=-0.1999999976084847, w1=0.2000000046354694\n",
      "Gradient Descent(93/99): loss =2.2484241460009783e+23, w0=-0.1999999975831595, w1=0.20000000468381288\n",
      "Gradient Descent(94/99): loss =2.24688284364123e+23, w0=-0.19999999755784983, w1=0.20000000473210355\n",
      "Gradient Descent(95/99): loss =2.245349737740779e+23, w0=-0.19999999753255557, w1=0.20000000478034155\n",
      "Gradient Descent(96/99): loss =2.2438247813576026e+23, w0=-0.19999999750727662, w1=0.20000000482852698\n",
      "Gradient Descent(97/99): loss =2.242307927917226e+23, w0=-0.1999999974820129, w1=0.20000000487665998\n",
      "Gradient Descent(98/99): loss =2.2407991312055568e+23, w0=-0.19999999745676428, w1=0.20000000492474063\n",
      "Gradient Descent(99/99): loss =2.2392983453619982e+23, w0=-0.19999999743153068, w1=0.20000000497276904\n",
      "Optimizing degree 13/15, model: least_squares_GD, arguments: {'max_iters': 100}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =1.8497076732144655e+47, w0=-0.20000000000000004, w1=0.20000000000000004\n",
      "Gradient Descent(2/99): loss =7.6995963114333315e+25, w0=-0.19999999999629034, w1=0.20000000000672238\n",
      "Gradient Descent(3/99): loss =7.695062419870414e+25, w0=-0.19999999999259793, w1=0.20000000001344279\n",
      "Gradient Descent(4/99): loss =7.690612522972328e+25, w0=-0.19999999998890766, w1=0.2000000000201567\n",
      "Gradient Descent(5/99): loss =7.686178465617543e+25, w0=-0.19999999998521933, w1=0.20000000002686433\n",
      "Gradient Descent(6/99): loss =7.681760020437825e+25, w0=-0.19999999998153292, w1=0.20000000003356588\n",
      "Gradient Descent(7/99): loss =7.677357043334542e+25, w0=-0.1999999999778484, w1=0.2000000000402615\n",
      "Gradient Descent(8/99): loss =7.672969400164072e+25, w0=-0.19999999997416573, w1=0.20000000004695134\n",
      "Gradient Descent(9/99): loss =7.668596965261077e+25, w0=-0.1999999999704849, w1=0.20000000005363552\n",
      "Gradient Descent(10/99): loss =7.664239620445166e+25, w0=-0.19999999996680587, w1=0.20000000006031413\n",
      "Gradient Descent(11/99): loss =7.65989725418534e+25, w0=-0.19999999996312864, w1=0.20000000006698726\n",
      "Gradient Descent(12/99): loss =7.655569760889698e+25, w0=-0.19999999995945322, w1=0.200000000073655\n",
      "Gradient Descent(13/99): loss =7.651257040296453e+25, w0=-0.19999999995577955, w1=0.20000000008031743\n",
      "Gradient Descent(14/99): loss =7.646958996947434e+25, w0=-0.19999999995210763, w1=0.20000000008697458\n",
      "Gradient Descent(15/99): loss =7.642675539729478e+25, w0=-0.19999999994843745, w1=0.2000000000936265\n",
      "Gradient Descent(16/99): loss =7.6384065814721275e+25, w0=-0.199999999944769, w1=0.20000000010027325\n",
      "Gradient Descent(17/99): loss =7.634152038592437e+25, w0=-0.19999999994110226, w1=0.20000000010691485\n",
      "Gradient Descent(18/99): loss =7.629911830779695e+25, w0=-0.19999999993743722, w1=0.20000000011355135\n",
      "Gradient Descent(19/99): loss =7.625685880714051e+25, w0=-0.19999999993377385, w1=0.20000000012018276\n",
      "Gradient Descent(20/99): loss =7.621474113814392e+25, w0=-0.19999999993011214, w1=0.20000000012680913\n",
      "Gradient Descent(21/99): loss =7.617276458011518e+25, w0=-0.1999999999264521, w1=0.20000000013343044\n",
      "Gradient Descent(22/99): loss =7.613092843543417e+25, w0=-0.1999999999227937, w1=0.20000000014004676\n",
      "Gradient Descent(23/99): loss =7.608923202770012e+25, w0=-0.19999999991913692, w1=0.20000000014665809\n",
      "Gradient Descent(24/99): loss =7.6047674700050135e+25, w0=-0.19999999991548176, w1=0.2000000001532644\n",
      "Gradient Descent(25/99): loss =7.600625581363172e+25, w0=-0.1999999999118282, w1=0.20000000015986577\n",
      "Gradient Descent(26/99): loss =7.596497474621068e+25, w0=-0.19999999990817627, w1=0.2000000001664622\n",
      "Gradient Descent(27/99): loss =7.592383089090257e+25, w0=-0.1999999999045259, w1=0.20000000017305367\n",
      "Gradient Descent(28/99): loss =7.588282365501335e+25, w0=-0.19999999990087713, w1=0.2000000001796402\n",
      "Gradient Descent(29/99): loss =7.584195245898084e+25, w0=-0.1999999998972299, w1=0.2000000001862218\n",
      "Gradient Descent(30/99): loss =7.5801216735405805e+25, w0=-0.19999999989358425, w1=0.20000000019279848\n",
      "Gradient Descent(31/99): loss =7.576061592816492e+25, w0=-0.19999999988994013, w1=0.20000000019937025\n",
      "Gradient Descent(32/99): loss =7.572014949159827e+25, w0=-0.19999999988629757, w1=0.20000000020593714\n",
      "Gradient Descent(33/99): loss =7.567981688976487e+25, w0=-0.19999999988265654, w1=0.2000000002124991\n",
      "Gradient Descent(34/99): loss =7.563961759576007e+25, w0=-0.19999999987901704, w1=0.2000000002190562\n",
      "Gradient Descent(35/99): loss =7.5599551091089155e+25, w0=-0.19999999987537903, w1=0.2000000002256084\n",
      "Gradient Descent(36/99): loss =7.555961686509275e+25, w0=-0.19999999987174255, w1=0.20000000023215575\n",
      "Gradient Descent(37/99): loss =7.551981441441952e+25, w0=-0.19999999986810757, w1=0.2000000002386982\n",
      "Gradient Descent(38/99): loss =7.54801432425421e+25, w0=-0.1999999998644741, w1=0.20000000024523581\n",
      "Gradient Descent(39/99): loss =7.544060285931254e+25, w0=-0.19999999986084208, w1=0.20000000025176856\n",
      "Gradient Descent(40/99): loss =7.540119278055367e+25, w0=-0.19999999985721156, w1=0.20000000025829645\n",
      "Gradient Descent(41/99): loss =7.536191252768464e+25, w0=-0.19999999985358252, w1=0.2000000002648195\n",
      "Gradient Descent(42/99): loss =7.532276162737568e+25, w0=-0.19999999984995495, w1=0.2000000002713377\n",
      "Gradient Descent(43/99): loss =7.528373961123201e+25, w0=-0.19999999984632882, w1=0.2000000002778511\n",
      "Gradient Descent(44/99): loss =7.5244846015502515e+25, w0=-0.19999999984270417, w1=0.20000000028435963\n",
      "Gradient Descent(45/99): loss =7.5206080380812074e+25, w0=-0.19999999983908096, w1=0.20000000029086337\n",
      "Gradient Descent(46/99): loss =7.516744225191612e+25, w0=-0.19999999983545919, w1=0.20000000029736228\n",
      "Gradient Descent(47/99): loss =7.5128931177473824e+25, w0=-0.19999999983183886, w1=0.2000000003038564\n",
      "Gradient Descent(48/99): loss =7.509054670984091e+25, w0=-0.19999999982821995, w1=0.2000000003103457\n",
      "Gradient Descent(49/99): loss =7.5052288404877514e+25, w0=-0.19999999982460248, w1=0.2000000003168302\n",
      "Gradient Descent(50/99): loss =7.501415582177277e+25, w0=-0.19999999982098643, w1=0.20000000032330992\n",
      "Gradient Descent(51/99): loss =7.497614852288252e+25, w0=-0.1999999998173718, w1=0.20000000032978485\n",
      "Gradient Descent(52/99): loss =7.493826607358054e+25, w0=-0.19999999981375857, w1=0.20000000033625498\n",
      "Gradient Descent(53/99): loss =7.4900508042120945e+25, w0=-0.19999999981014677, w1=0.20000000034272034\n",
      "Gradient Descent(54/99): loss =7.4862873999512145e+25, w0=-0.19999999980653635, w1=0.20000000034918092\n",
      "Gradient Descent(55/99): loss =7.482536351940045e+25, w0=-0.19999999980292735, w1=0.20000000035563675\n",
      "Gradient Descent(56/99): loss =7.478797617796263e+25, w0=-0.19999999979931973, w1=0.20000000036208782\n",
      "Gradient Descent(57/99): loss =7.475071155380723e+25, w0=-0.1999999997957135, w1=0.20000000036853413\n",
      "Gradient Descent(58/99): loss =7.4713569227883635e+25, w0=-0.19999999979210867, w1=0.2000000003749757\n",
      "Gradient Descent(59/99): loss =7.467654878339792e+25, w0=-0.1999999997885052, w1=0.20000000038141255\n",
      "Gradient Descent(60/99): loss =7.463964980573543e+25, w0=-0.1999999997849031, w1=0.20000000038784466\n",
      "Gradient Descent(61/99): loss =7.460287188238926e+25, w0=-0.19999999978130237, w1=0.20000000039427204\n",
      "Gradient Descent(62/99): loss =7.4566214602894555e+25, w0=-0.19999999977770302, w1=0.2000000004006947\n",
      "Gradient Descent(63/99): loss =7.452967755876689e+25, w0=-0.19999999977410504, w1=0.20000000040711266\n",
      "Gradient Descent(64/99): loss =7.4493260343446715e+25, w0=-0.19999999977050842, w1=0.2000000004135259\n",
      "Gradient Descent(65/99): loss =7.445696255224675e+25, w0=-0.19999999976691316, w1=0.20000000041993443\n",
      "Gradient Descent(66/99): loss =7.442078378230401e+25, w0=-0.19999999976331925, w1=0.20000000042633828\n",
      "Gradient Descent(67/99): loss =7.4384723632535355e+25, w0=-0.19999999975972668, w1=0.20000000043273744\n",
      "Gradient Descent(68/99): loss =7.434878170359607e+25, w0=-0.19999999975613547, w1=0.2000000004391319\n",
      "Gradient Descent(69/99): loss =7.431295759784156e+25, w0=-0.1999999997525456, w1=0.20000000044552171\n",
      "Gradient Descent(70/99): loss =7.4277250919292e+25, w0=-0.19999999974895705, w1=0.20000000045190686\n",
      "Gradient Descent(71/99): loss =7.424166127359944e+25, w0=-0.19999999974536986, w1=0.20000000045828734\n",
      "Gradient Descent(72/99): loss =7.420618826801706e+25, w0=-0.199999999741784, w1=0.20000000046466315\n",
      "Gradient Descent(73/99): loss =7.417083151137104e+25, w0=-0.19999999973819946, w1=0.20000000047103433\n",
      "Gradient Descent(74/99): loss =7.413559061403378e+25, w0=-0.19999999973461624, w1=0.20000000047740085\n",
      "Gradient Descent(75/99): loss =7.4100465187899586e+25, w0=-0.19999999973103436, w1=0.20000000048376274\n",
      "Gradient Descent(76/99): loss =7.406545484636155e+25, w0=-0.1999999997274538, w1=0.20000000049012\n",
      "Gradient Descent(77/99): loss =7.403055920429012e+25, w0=-0.19999999972387456, w1=0.20000000049647265\n",
      "Gradient Descent(78/99): loss =7.399577787801336e+25, w0=-0.1999999997202966, w1=0.20000000050282069\n",
      "Gradient Descent(79/99): loss =7.396111048529783e+25, w0=-0.19999999971672, w1=0.2000000005091641\n",
      "Gradient Descent(80/99): loss =7.3926556645331465e+25, w0=-0.1999999997131447, w1=0.20000000051550293\n",
      "Gradient Descent(81/99): loss =7.389211597870683e+25, w0=-0.1999999997095707, w1=0.20000000052183717\n",
      "Gradient Descent(82/99): loss =7.385778810740602e+25, w0=-0.199999999705998, w1=0.2000000005281668\n",
      "Gradient Descent(83/99): loss =7.382357265478602e+25, w0=-0.1999999997024266, w1=0.20000000053449185\n",
      "Gradient Descent(84/99): loss =7.378946924556498e+25, w0=-0.19999999969885648, w1=0.20000000054081235\n",
      "Gradient Descent(85/99): loss =7.375547750580945e+25, w0=-0.19999999969528767, w1=0.20000000054712827\n",
      "Gradient Descent(86/99): loss =7.3721597062922335e+25, w0=-0.19999999969172014, w1=0.20000000055343964\n",
      "Gradient Descent(87/99): loss =7.368782754563145e+25, w0=-0.1999999996881539, w1=0.20000000055974645\n",
      "Gradient Descent(88/99): loss =7.365416858397821e+25, w0=-0.19999999968458895, w1=0.20000000056604872\n",
      "Gradient Descent(89/99): loss =7.3620619809308e+25, w0=-0.1999999996810253, w1=0.20000000057234643\n",
      "Gradient Descent(90/99): loss =7.3587180854259915e+25, w0=-0.19999999967746293, w1=0.20000000057863962\n",
      "Gradient Descent(91/99): loss =7.355385135275757e+25, w0=-0.19999999967390183, w1=0.20000000058492828\n",
      "Gradient Descent(92/99): loss =7.352063094000035e+25, w0=-0.19999999967034202, w1=0.20000000059121242\n",
      "Gradient Descent(93/99): loss =7.348751925245467e+25, w0=-0.19999999966678347, w1=0.20000000059749207\n",
      "Gradient Descent(94/99): loss =7.345451592784608e+25, w0=-0.19999999966322618, w1=0.2000000006037672\n",
      "Gradient Descent(95/99): loss =7.3421620605151635e+25, w0=-0.19999999965967016, w1=0.2000000006100378\n",
      "Gradient Descent(96/99): loss =7.338883292459196e+25, w0=-0.19999999965611542, w1=0.20000000061630394\n",
      "Gradient Descent(97/99): loss =7.335615252762466e+25, w0=-0.19999999965256193, w1=0.2000000006225656\n",
      "Gradient Descent(98/99): loss =7.332357905693682e+25, w0=-0.19999999964900972, w1=0.20000000062882278\n",
      "Gradient Descent(99/99): loss =7.329111215643897e+25, w0=-0.19999999964545875, w1=0.20000000063507548\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.8497076732144634e+47, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =6.167668475407497e+25, w0=-0.19999999999779522, w1=0.20000000000003892\n",
      "Gradient Descent(3/99): loss =6.087887212334329e+25, w0=-0.1999999999957882, w1=0.1999999999996705\n",
      "Gradient Descent(4/99): loss =6.03266789646143e+25, w0=-0.19999999999385193, w1=0.19999999999899631\n",
      "Gradient Descent(5/99): loss =5.992242388483131e+25, w0=-0.19999999999197593, w1=0.19999999999806187\n",
      "Gradient Descent(6/99): loss =5.962640526767323e+25, w0=-0.19999999999015147, w1=0.19999999999690554\n",
      "Gradient Descent(7/99): loss =5.940959384185634e+25, w0=-0.19999999998837112, w1=0.1999999999955599\n",
      "Gradient Descent(8/99): loss =5.925075554570539e+25, w0=-0.19999999998662846, w1=0.19999999999405257\n",
      "Gradient Descent(9/99): loss =5.913435388202334e+25, w0=-0.19999999998491808, w1=0.19999999999240708\n",
      "Gradient Descent(10/99): loss =5.904901877844315e+25, w0=-0.19999999998323528, w1=0.1999999999906435\n",
      "Gradient Descent(11/99): loss =5.898642801331881e+25, w0=-0.1999999999815761, w1=0.1999999999887789\n",
      "Gradient Descent(12/99): loss =5.89404895348155e+25, w0=-0.19999999997993717, w1=0.19999999998682794\n",
      "Gradient Descent(13/99): loss =5.890674346231141e+25, w0=-0.19999999997831552, w1=0.19999999998480308\n",
      "Gradient Descent(14/99): loss =5.8881924603628285e+25, w0=-0.19999999997670867, w1=0.19999999998271498\n",
      "Gradient Descent(15/99): loss =5.886364232414933e+25, w0=-0.1999999999751145, w1=0.19999999998057277\n",
      "Gradient Descent(16/99): loss =5.885014624713598e+25, w0=-0.19999999997353116, w1=0.19999999997838425\n",
      "Gradient Descent(17/99): loss =5.884015475038113e+25, w0=-0.1999999999719571, w1=0.19999999997615606\n",
      "Gradient Descent(18/99): loss =5.8832729416683435e+25, w0=-0.199999999970391, w1=0.19999999997389392\n",
      "Gradient Descent(19/99): loss =5.882718311850848e+25, w0=-0.19999999996883172, w1=0.19999999997160273\n",
      "Gradient Descent(20/99): loss =5.882301272292474e+25, w0=-0.19999999996727827, w1=0.19999999996928666\n",
      "Gradient Descent(21/99): loss =5.8819849820219715e+25, w0=-0.1999999999657298, w1=0.19999999996694928\n",
      "Gradient Descent(22/99): loss =5.88174246479086e+25, w0=-0.19999999996418563, w1=0.19999999996459364\n",
      "Gradient Descent(23/99): loss =5.881553967573929e+25, w0=-0.1999999999626451, w1=0.19999999996222234\n",
      "Gradient Descent(24/99): loss =5.881405026423224e+25, w0=-0.1999999999611077, w1=0.19999999995983764\n",
      "Gradient Descent(25/99): loss =5.881285050241847e+25, w0=-0.199999999959573, w1=0.19999999995744144\n",
      "Gradient Descent(26/99): loss =5.88118628378219e+25, w0=-0.1999999999580406, w1=0.1999999999550354\n",
      "Gradient Descent(27/99): loss =5.881103048318842e+25, w0=-0.1999999999565102, w1=0.1999999999526209\n",
      "Gradient Descent(28/99): loss =5.881031185641808e+25, w0=-0.19999999995498147, w1=0.19999999995019918\n",
      "Gradient Descent(29/99): loss =5.880967650926763e+25, w0=-0.1999999999534542, w1=0.1999999999477712\n",
      "Gradient Descent(30/99): loss =5.880910214618386e+25, w0=-0.19999999995192816, w1=0.1999999999453379\n",
      "Gradient Descent(31/99): loss =5.880857244136853e+25, w0=-0.1999999999504032, w1=0.1999999999429\n",
      "Gradient Descent(32/99): loss =5.880807544034088e+25, w0=-0.19999999994887918, w1=0.19999999994045814\n",
      "Gradient Descent(33/99): loss =5.880760238949277e+25, w0=-0.19999999994735596, w1=0.1999999999380129\n",
      "Gradient Descent(34/99): loss =5.880714687903779e+25, w0=-0.1999999999458334, w1=0.19999999993556472\n",
      "Gradient Descent(35/99): loss =5.8806704215440715e+25, w0=-0.19999999994431145, w1=0.19999999993311401\n",
      "Gradient Descent(36/99): loss =5.880627096188237e+25, w0=-0.19999999994279, w1=0.19999999993066114\n",
      "Gradient Descent(37/99): loss =5.880584460176792e+25, w0=-0.199999999941269, w1=0.19999999992820638\n",
      "Gradient Descent(38/99): loss =5.880542329233247e+25, w0=-0.19999999993974835, w1=0.19999999992575\n",
      "Gradient Descent(39/99): loss =5.880500568421937e+25, w0=-0.19999999993822803, w1=0.19999999992329218\n",
      "Gradient Descent(40/99): loss =5.880459078936843e+25, w0=-0.19999999993670803, w1=0.19999999992083314\n",
      "Gradient Descent(41/99): loss =5.880417788427665e+25, w0=-0.19999999993518827, w1=0.19999999991837303\n",
      "Gradient Descent(42/99): loss =5.880376643916209e+25, w0=-0.1999999999336687, w1=0.19999999991591197\n",
      "Gradient Descent(43/99): loss =5.880335606609396e+25, w0=-0.19999999993214934, w1=0.1999999999134501\n",
      "Gradient Descent(44/99): loss =5.880294648101161e+25, w0=-0.19999999993063014, w1=0.19999999991098752\n",
      "Gradient Descent(45/99): loss =5.880253747591257e+25, w0=-0.19999999992911108, w1=0.1999999999085243\n",
      "Gradient Descent(46/99): loss =5.880212889848773e+25, w0=-0.19999999992759215, w1=0.1999999999060605\n",
      "Gradient Descent(47/99): loss =5.880172063720946e+25, w0=-0.19999999992607334, w1=0.19999999990359618\n",
      "Gradient Descent(48/99): loss =5.880131261041158e+25, w0=-0.19999999992455464, w1=0.19999999990113143\n",
      "Gradient Descent(49/99): loss =5.880090475829449e+25, w0=-0.19999999992303602, w1=0.1999999998986663\n",
      "Gradient Descent(50/99): loss =5.8800497037069665e+25, w0=-0.19999999992151749, w1=0.19999999989620076\n",
      "Gradient Descent(51/99): loss =5.880008941467323e+25, w0=-0.19999999991999903, w1=0.1999999998937349\n",
      "Gradient Descent(52/99): loss =5.87996818676263e+25, w0=-0.19999999991848064, w1=0.19999999989126876\n",
      "Gradient Descent(53/99): loss =5.879927437873622e+25, w0=-0.19999999991696232, w1=0.19999999988880232\n",
      "Gradient Descent(54/99): loss =5.8798866935413715e+25, w0=-0.19999999991544407, w1=0.19999999988633563\n",
      "Gradient Descent(55/99): loss =5.879845952843996e+25, w0=-0.19999999991392586, w1=0.1999999998838687\n",
      "Gradient Descent(56/99): loss =5.879805215106448e+25, w0=-0.19999999991240772, w1=0.19999999988140155\n",
      "Gradient Descent(57/99): loss =5.879764479834398e+25, w0=-0.1999999999108896, w1=0.1999999998789342\n",
      "Gradient Descent(58/99): loss =5.8797237466658545e+25, w0=-0.19999999990937153, w1=0.19999999987646663\n",
      "Gradient Descent(59/99): loss =5.879683015335727e+25, w0=-0.1999999999078535, w1=0.19999999987399888\n",
      "Gradient Descent(60/99): loss =5.879642285649897e+25, w0=-0.19999999990633552, w1=0.19999999987153094\n",
      "Gradient Descent(61/99): loss =5.879601557466188e+25, w0=-0.19999999990481757, w1=0.19999999986906283\n",
      "Gradient Descent(62/99): loss =5.879560830680469e+25, w0=-0.19999999990329967, w1=0.19999999986659456\n",
      "Gradient Descent(63/99): loss =5.879520105216491e+25, w0=-0.1999999999017818, w1=0.19999999986412612\n",
      "Gradient Descent(64/99): loss =5.87947938101837e+25, w0=-0.19999999990026396, w1=0.1999999998616575\n",
      "Gradient Descent(65/99): loss =5.879438658045189e+25, w0=-0.19999999989874614, w1=0.19999999985918873\n",
      "Gradient Descent(66/99): loss =5.879397936266953e+25, w0=-0.1999999998972284, w1=0.19999999985671982\n",
      "Gradient Descent(67/99): loss =5.879357215661693e+25, w0=-0.19999999989571066, w1=0.19999999985425074\n",
      "Gradient Descent(68/99): loss =5.8793164962132845e+25, w0=-0.19999999989419295, w1=0.19999999985178152\n",
      "Gradient Descent(69/99): loss =5.879275777909896e+25, w0=-0.19999999989267528, w1=0.19999999984931216\n",
      "Gradient Descent(70/99): loss =5.879235060742866e+25, w0=-0.19999999989115763, w1=0.19999999984684266\n",
      "Gradient Descent(71/99): loss =5.879194344705826e+25, w0=-0.19999999988964, w1=0.19999999984437303\n",
      "Gradient Descent(72/99): loss =5.879153629794075e+25, w0=-0.19999999988812242, w1=0.19999999984190323\n",
      "Gradient Descent(73/99): loss =5.879112916004167e+25, w0=-0.19999999988660486, w1=0.19999999983943328\n",
      "Gradient Descent(74/99): loss =5.879072203333554e+25, w0=-0.19999999988508735, w1=0.1999999998369632\n",
      "Gradient Descent(75/99): loss =5.879031491780339e+25, w0=-0.19999999988356987, w1=0.199999999834493\n",
      "Gradient Descent(76/99): loss =5.878990781343131e+25, w0=-0.19999999988205241, w1=0.19999999983202263\n",
      "Gradient Descent(77/99): loss =5.878950072020888e+25, w0=-0.199999999880535, w1=0.19999999982955213\n",
      "Gradient Descent(78/99): loss =5.87890936381281e+25, w0=-0.1999999998790176, w1=0.19999999982708153\n",
      "Gradient Descent(79/99): loss =5.878868656718316e+25, w0=-0.19999999987750022, w1=0.19999999982461078\n",
      "Gradient Descent(80/99): loss =5.878827950736931e+25, w0=-0.19999999987598288, w1=0.1999999998221399\n",
      "Gradient Descent(81/99): loss =5.878787245868318e+25, w0=-0.19999999987446557, w1=0.19999999981966887\n",
      "Gradient Descent(82/99): loss =5.878746542112189e+25, w0=-0.19999999987294828, w1=0.1999999998171977\n",
      "Gradient Descent(83/99): loss =5.878705839468318e+25, w0=-0.19999999987143102, w1=0.1999999998147264\n",
      "Gradient Descent(84/99): loss =5.8786651379365284e+25, w0=-0.1999999998699138, w1=0.19999999981225497\n",
      "Gradient Descent(85/99): loss =5.878624437516655e+25, w0=-0.1999999998683966, w1=0.1999999998097834\n",
      "Gradient Descent(86/99): loss =5.878583738208578e+25, w0=-0.1999999998668794, w1=0.19999999980731167\n",
      "Gradient Descent(87/99): loss =5.878543040012177e+25, w0=-0.19999999986536227, w1=0.19999999980483985\n",
      "Gradient Descent(88/99): loss =5.878502342927333e+25, w0=-0.19999999986384515, w1=0.19999999980236788\n",
      "Gradient Descent(89/99): loss =5.878461646953969e+25, w0=-0.19999999986232808, w1=0.19999999979989577\n",
      "Gradient Descent(90/99): loss =5.878420952091988e+25, w0=-0.19999999986081105, w1=0.19999999979742353\n",
      "Gradient Descent(91/99): loss =5.8783802583412935e+25, w0=-0.19999999985929404, w1=0.19999999979495114\n",
      "Gradient Descent(92/99): loss =5.878339565701812e+25, w0=-0.19999999985777706, w1=0.19999999979247862\n",
      "Gradient Descent(93/99): loss =5.878298874173458e+25, w0=-0.1999999998562601, w1=0.199999999790006\n",
      "Gradient Descent(94/99): loss =5.878258183756165e+25, w0=-0.19999999985474318, w1=0.19999999978753322\n",
      "Gradient Descent(95/99): loss =5.878217494449851e+25, w0=-0.19999999985322628, w1=0.1999999997850603\n",
      "Gradient Descent(96/99): loss =5.878176806254442e+25, w0=-0.1999999998517094, w1=0.19999999978258726\n",
      "Gradient Descent(97/99): loss =5.87813611916986e+25, w0=-0.19999999985019257, w1=0.19999999978011407\n",
      "Gradient Descent(98/99): loss =5.878095433196027e+25, w0=-0.19999999984867575, w1=0.19999999977764074\n",
      "Gradient Descent(99/99): loss =5.8780547483328856e+25, w0=-0.19999999984715897, w1=0.1999999997751673\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.5605518648858298e+32, w0=0.0, w1=0.2\n",
      "Gradient Descent(2/99): loss =2.5276777484557997e+17, w0=6.990309964249377e-09, w1=0.20000001426770014\n",
      "Gradient Descent(3/99): loss =1.0839796597653314e+16, w0=7.645876710258035e-09, w1=0.20000002298284167\n",
      "Gradient Descent(4/99): loss =6555729687597208.0, w0=9.102378301298597e-09, w1=0.20000003198877706\n",
      "Gradient Descent(5/99): loss =6471592880245995.0, w0=1.0426449712981504e-08, w1=0.20000004068106528\n",
      "Gradient Descent(6/99): loss =6466256551284447.0, w0=1.1752010810570713e-08, w1=0.20000004924565534\n",
      "Gradient Descent(7/99): loss =6464127644435102.0, w0=1.3067433186066974e-08, w1=0.20000005772291027\n",
      "Gradient Descent(8/99): loss =6462745045502910.0, w0=1.4378048575228596e-08, w1=0.20000006614748836\n",
      "Gradient Descent(9/99): loss =6461637245394187.0, w0=1.568548150466943e-08, w1=0.2000000745393973\n",
      "Gradient Descent(10/99): loss =6460633594557374.0, w0=1.6990952349875152e-08, w1=0.20000008291110644\n",
      "Gradient Descent(11/99): loss =6459669529841957.0, w0=1.8295183252869138e-08, w1=0.20000009127025017\n",
      "Gradient Descent(12/99): loss =6458720572541837.0, w0=1.9598622680911675e-08, w1=0.20000009962151472\n",
      "Gradient Descent(13/99): loss =6457777440547391.0, w0=2.0901546199917972e-08, w1=0.2000001079677699\n",
      "Gradient Descent(14/99): loss =6456836613956089.0, w0=2.2204123503562594e-08, w1=0.20000011631076967\n",
      "Gradient Descent(15/99): loss =6455896757825004.0, w0=2.3506459025152775e-08, w1=0.20000012465158262\n",
      "Gradient Descent(16/99): loss =6454957365755531.0, w0=2.480861703136352e-08, w1=0.20000013299085695\n",
      "Gradient Descent(17/99): loss =6454018245561397.0, w0=2.611063706124602e-08, w1=0.20000014132898333\n",
      "Gradient Descent(18/99): loss =6453079324235503.0, w0=2.7412543433535767e-08, w1=0.20000014966619506\n",
      "Gradient Descent(19/99): loss =6452140573989283.0, w0=2.8714351099817646e-08, w1=0.20000015800262952\n",
      "Gradient Descent(20/99): loss =6451201984201736.0, w0=3.001606924795658e-08, w1=0.20000016633836595\n",
      "Gradient Descent(21/99): loss =6450263550777850.0, w0=3.131770352030144e-08, w1=0.20000017467344847\n",
      "Gradient Descent(22/99): loss =6449325272109597.0, w0=3.261925737901591e-08, w1=0.20000018300790026\n",
      "Gradient Descent(23/99): loss =6448387147541361.0, w0=3.392073294632373e-08, w1=0.20000019134173216\n",
      "Gradient Descent(24/99): loss =6447449176785769.0, w0=3.5222131521496866e-08, w1=0.20000019967494792\n",
      "Gradient Descent(25/99): loss =6446511359700452.0, w0=3.6523453898855254e-08, w1=0.20000020800754725\n",
      "Gradient Descent(26/99): loss =6445573696202093.0, w0=3.7824700563289106e-08, w1=0.20000021633952783\n",
      "Gradient Descent(27/99): loss =6444636186232813.0, w0=3.9125871810409053e-08, w1=0.20000022467088632\n",
      "Gradient Descent(28/99): loss =6443698829746715.0, w0=4.042696782032284e-08, w1=0.20000023300161907\n",
      "Gradient Descent(29/99): loss =6442761626704188.0, w0=4.1727988702889365e-08, w1=0.20000024133172242\n",
      "Gradient Descent(30/99): loss =6441824577069334.0, w0=4.3028934525436983e-08, w1=0.20000024966119295\n",
      "Gradient Descent(31/99): loss =6440887680808663.0, w0=4.4329805329707245e-08, w1=0.20000025799002752\n",
      "Gradient Descent(32/99): loss =6439950937890364.0, w0=4.563060114218366e-08, w1=0.2000002663182233\n",
      "Gradient Descent(33/99): loss =6439014348283832.0, w0=4.693132198036396e-08, w1=0.2000002746457778\n",
      "Gradient Descent(34/99): loss =6438077911959366.0, w0=4.8231967856548566e-08, w1=0.20000028297268885\n",
      "Gradient Descent(35/99): loss =6437141628887929.0, w0=4.953253878011149e-08, w1=0.20000029129895452\n",
      "Gradient Descent(36/99): loss =6436205499040996.0, w0=5.0833034758846905e-08, w1=0.2000002996245732\n",
      "Gradient Descent(37/99): loss =6435269522390424.0, w0=5.2133455799754854e-08, w1=0.20000030794954343\n",
      "Gradient Descent(38/99): loss =6434333698908361.0, w0=5.3433801909488915e-08, w1=0.20000031627386403\n",
      "Gradient Descent(39/99): loss =6433398028567193.0, w0=5.473407309460161e-08, w1=0.20000032459753392\n",
      "Gradient Descent(40/99): loss =6432462511339461.0, w0=5.60342693616703e-08, w1=0.2000003329205522\n",
      "Gradient Descent(41/99): loss =6431527147197853.0, w0=5.733439071735386e-08, w1=0.2000003412429181\n",
      "Gradient Descent(42/99): loss =6430591936115145.0, w0=5.863443716841021e-08, w1=0.200000349564631\n",
      "Gradient Descent(43/99): loss =6429656878064216.0, w0=5.993440872169273e-08, w1=0.20000035788569032\n",
      "Gradient Descent(44/99): loss =6428721973017985.0, w0=6.123430538413623e-08, w1=0.20000036620609557\n",
      "Gradient Descent(45/99): loss =6427787220949426.0, w0=6.253412716273861e-08, w1=0.20000037452584635\n",
      "Gradient Descent(46/99): loss =6426852621831571.0, w0=6.383387406454156e-08, w1=0.20000038284494234\n",
      "Gradient Descent(47/99): loss =6425918175637463.0, w0=6.513354609661217e-08, w1=0.20000039116338325\n",
      "Gradient Descent(48/99): loss =6424983882340173.0, w0=6.643314326602637e-08, w1=0.20000039948116882\n",
      "Gradient Descent(49/99): loss =6424049741912819.0, w0=6.773266557985445e-08, w1=0.2000004077982989\n",
      "Gradient Descent(50/99): loss =6423115754328505.0, w0=6.903211304514878e-08, w1=0.20000041611477326\n",
      "Gradient Descent(51/99): loss =6422181919560382.0, w0=7.033148566893353e-08, w1=0.20000042443059182\n",
      "Gradient Descent(52/99): loss =6421248237581590.0, w0=7.163078345819626e-08, w1=0.20000043274575446\n",
      "Gradient Descent(53/99): loss =6420314708365302.0, w0=7.293000641988107e-08, w1=0.20000044106026113\n",
      "Gradient Descent(54/99): loss =6419381331884692.0, w0=7.422915456088329e-08, w1=0.2000004493741117\n",
      "Gradient Descent(55/99): loss =6418448108112952.0, w0=7.552822788804513e-08, w1=0.20000045768730618\n",
      "Gradient Descent(56/99): loss =6417515037023284.0, w0=7.682722640815261e-08, w1=0.2000004659998445\n",
      "Gradient Descent(57/99): loss =6416582118588887.0, w0=7.812615012793306e-08, w1=0.20000047431172666\n",
      "Gradient Descent(58/99): loss =6415649352782979.0, w0=7.94249990540535e-08, w1=0.20000048262295264\n",
      "Gradient Descent(59/99): loss =6414716739578803.0, w0=8.07237731931195e-08, w1=0.20000049093352246\n",
      "Gradient Descent(60/99): loss =6413784278949581.0, w0=8.202247255167459e-08, w1=0.20000049924343608\n",
      "Gradient Descent(61/99): loss =6412851970868555.0, w0=8.33210971362e-08, w1=0.20000050755269355\n",
      "Gradient Descent(62/99): loss =6411919815308987.0, w0=8.461964695311466e-08, w1=0.2000005158612949\n",
      "Gradient Descent(63/99): loss =6410987812244132.0, w0=8.591812200877555e-08, w1=0.2000005241692401\n",
      "Gradient Descent(64/99): loss =6410055961647255.0, w0=8.721652230947822e-08, w1=0.20000053247652924\n",
      "Gradient Descent(65/99): loss =6409124263491640.0, w0=8.851484786145744e-08, w1=0.20000054078316232\n",
      "Gradient Descent(66/99): loss =6408192717750561.0, w0=8.981309867088797e-08, w1=0.2000005490891394\n",
      "Gradient Descent(67/99): loss =6407261324397321.0, w0=9.111127474388547e-08, w1=0.20000055739446046\n",
      "Gradient Descent(68/99): loss =6406330083405201.0, w0=9.240937608650744e-08, w1=0.2000005656991256\n",
      "Gradient Descent(69/99): loss =6405398994747521.0, w0=9.370740270475421e-08, w1=0.20000057400313487\n",
      "Gradient Descent(70/99): loss =6404468058397586.0, w0=9.500535460457001e-08, w1=0.2000005823064883\n",
      "Gradient Descent(71/99): loss =6403537274328719.0, w0=9.630323179184399e-08, w1=0.2000005906091859\n",
      "Gradient Descent(72/99): loss =6402606642514241.0, w0=9.760103427241131e-08, w1=0.20000059891122776\n",
      "Gradient Descent(73/99): loss =6401676162927489.0, w0=9.889876205205423e-08, w1=0.20000060721261392\n",
      "Gradient Descent(74/99): loss =6400745835541800.0, w0=1.001964151365032e-07, w1=0.20000061551334444\n",
      "Gradient Descent(75/99): loss =6399815660330523.0, w0=1.014939935314379e-07, w1=0.20000062381341938\n",
      "Gradient Descent(76/99): loss =6398885637267008.0, w0=1.0279149724248831e-07, w1=0.20000063211283875\n",
      "Gradient Descent(77/99): loss =6397955766324614.0, w0=1.0408892627523576e-07, w1=0.20000064041160265\n",
      "Gradient Descent(78/99): loss =6397026047476707.0, w0=1.0538628063521393e-07, w1=0.2000006487097111\n",
      "Gradient Descent(79/99): loss =6396096480696672.0, w0=1.0668356032790997e-07, w1=0.20000065700716418\n",
      "Gradient Descent(80/99): loss =6395167065957865.0, w0=1.0798076535876534e-07, w1=0.20000066530396193\n",
      "Gradient Descent(81/99): loss =6394237803233690.0, w0=1.092778957331769e-07, w1=0.20000067360010443\n",
      "Gradient Descent(82/99): loss =6393308692497527.0, w0=1.1057495145649782e-07, w1=0.2000006818955917\n",
      "Gradient Descent(83/99): loss =6392379733722777.0, w0=1.1187193253403851e-07, w1=0.20000069019042382\n",
      "Gradient Descent(84/99): loss =6391450926882843.0, w0=1.1316883897106755e-07, w1=0.20000069848460084\n",
      "Gradient Descent(85/99): loss =6390522271951140.0, w0=1.1446567077281257e-07, w1=0.20000070677812282\n",
      "Gradient Descent(86/99): loss =6389593768901074.0, w0=1.1576242794446114e-07, w1=0.20000071507098982\n",
      "Gradient Descent(87/99): loss =6388665417706074.0, w0=1.1705911049116161e-07, w1=0.20000072336320188\n",
      "Gradient Descent(88/99): loss =6387737218339570.0, w0=1.1835571841802396e-07, w1=0.20000073165475907\n",
      "Gradient Descent(89/99): loss =6386809170774985.0, w0=1.1965225173012054e-07, w1=0.20000073994566145\n",
      "Gradient Descent(90/99): loss =6385881274985768.0, w0=1.2094871043248702e-07, w1=0.20000074823590908\n",
      "Gradient Descent(91/99): loss =6384953530945367.0, w0=1.22245094530123e-07, w1=0.200000756525502\n",
      "Gradient Descent(92/99): loss =6384025938627217.0, w0=1.2354140402799292e-07, w1=0.20000076481444032\n",
      "Gradient Descent(93/99): loss =6383098498004791.0, w0=1.2483763893102667e-07, w1=0.20000077310272404\n",
      "Gradient Descent(94/99): loss =6382171209051545.0, w0=1.2613379924412042e-07, w1=0.20000078139035327\n",
      "Gradient Descent(95/99): loss =6381244071740952.0, w0=1.2742988497213728e-07, w1=0.200000789677328\n",
      "Gradient Descent(96/99): loss =6380317086046482.0, w0=1.2872589611990797e-07, w1=0.20000079796364836\n",
      "Gradient Descent(97/99): loss =6379390251941614.0, w0=1.300218326922316e-07, w1=0.20000080624931438\n",
      "Gradient Descent(98/99): loss =6378463569399838.0, w0=1.3131769469387625e-07, w1=0.2000008145343261\n",
      "Gradient Descent(99/99): loss =6377537038394645.0, w0=1.326134821295796e-07, w1=0.20000082281868362\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.849707673214465e+47, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =7.695847433221028e+25, w0=-0.19999999999634457, w1=0.20000000000674537\n",
      "Gradient Descent(3/99): loss =7.690073538167835e+25, w0=-0.1999999999927412, w1=0.20000000001348206\n",
      "Gradient Descent(4/99): loss =7.6854996510398925e+25, w0=-0.19999999998913406, w1=0.20000000002021162\n",
      "Gradient Descent(5/99): loss =7.68098381632275e+25, w0=-0.19999999998553122, w1=0.20000000002693483\n",
      "Gradient Descent(6/99): loss =7.6764872899459634e+25, w0=-0.19999999998193102, w1=0.20000000003365184\n",
      "Gradient Descent(7/99): loss =7.672008459030253e+25, w0=-0.19999999997833376, w1=0.2000000000403629\n",
      "Gradient Descent(8/99): loss =7.667547114054442e+25, w0=-0.19999999997473933, w1=0.2000000000470681\n",
      "Gradient Descent(9/99): loss =7.6631031037839655e+25, w0=-0.19999999997114773, w1=0.20000000005376767\n",
      "Gradient Descent(10/99): loss =7.658676283548988e+25, w0=-0.1999999999675589, w1=0.20000000006046165\n",
      "Gradient Descent(11/99): loss =7.654266512920509e+25, w0=-0.19999999996397286, w1=0.20000000006715019\n",
      "Gradient Descent(12/99): loss =7.649873655259075e+25, w0=-0.19999999996038956, w1=0.20000000007383337\n",
      "Gradient Descent(13/99): loss =7.6454975774162464e+25, w0=-0.19999999995680898, w1=0.20000000008051125\n",
      "Gradient Descent(14/99): loss =7.6411381494903205e+25, w0=-0.1999999999532311, w1=0.2000000000871839\n",
      "Gradient Descent(15/99): loss =7.636795244624039e+25, w0=-0.19999999994965587, w1=0.2000000000938514\n",
      "Gradient Descent(16/99): loss =7.632468738834202e+25, w0=-0.19999999994608328, w1=0.2000000001005138\n",
      "Gradient Descent(17/99): loss =7.6281585108658924e+25, w0=-0.1999999999425133, w1=0.20000000010717114\n",
      "Gradient Descent(18/99): loss =7.62386444206606e+25, w0=-0.19999999993894593, w1=0.20000000011382346\n",
      "Gradient Descent(19/99): loss =7.619586416272023e+25, w0=-0.19999999993538115, w1=0.20000000012047078\n",
      "Gradient Descent(20/99): loss =7.615324319711972e+25, w0=-0.1999999999318189, w1=0.20000000012711314\n",
      "Gradient Descent(21/99): loss =7.611078040914928e+25, w0=-0.1999999999282592, w1=0.20000000013375058\n",
      "Gradient Descent(22/99): loss =7.606847470628305e+25, w0=-0.19999999992470202, w1=0.2000000001403831\n",
      "Gradient Descent(23/99): loss =7.602632501741778e+25, w0=-0.1999999999211473, w1=0.20000000014701072\n",
      "Gradient Descent(24/99): loss =7.598433029216245e+25, w0=-0.19999999991759507, w1=0.20000000015363348\n",
      "Gradient Descent(25/99): loss =7.59424895001716e+25, w0=-0.1999999999140453, w1=0.20000000016025135\n",
      "Gradient Descent(26/99): loss =7.590080163051503e+25, w0=-0.19999999991049794, w1=0.2000000001668644\n",
      "Gradient Descent(27/99): loss =7.585926569107926e+25, w0=-0.199999999906953, w1=0.2000000001734726\n",
      "Gradient Descent(28/99): loss =7.58178807079976e+25, w0=-0.19999999990341044, w1=0.200000000180076\n",
      "Gradient Descent(29/99): loss =7.577664572510377e+25, w0=-0.19999999989987025, w1=0.20000000018667455\n",
      "Gradient Descent(30/99): loss =7.573555980340893e+25, w0=-0.1999999998963324, w1=0.2000000001932683\n",
      "Gradient Descent(31/99): loss =7.56946220205983e+25, w0=-0.1999999998927969, w1=0.20000000019985725\n",
      "Gradient Descent(32/99): loss =7.565383147054688e+25, w0=-0.1999999998892637, w1=0.20000000020644143\n",
      "Gradient Descent(33/99): loss =7.5613187262852396e+25, w0=-0.1999999998857328, w1=0.2000000002130208\n",
      "Gradient Descent(34/99): loss =7.557268852238484e+25, w0=-0.19999999988220418, w1=0.2000000002195954\n",
      "Gradient Descent(35/99): loss =7.5532334388850425e+25, w0=-0.1999999998786778, w1=0.20000000022616524\n",
      "Gradient Descent(36/99): loss =7.549212401637143e+25, w0=-0.19999999987515368, w1=0.20000000023273032\n",
      "Gradient Descent(37/99): loss =7.54520565730788e+25, w0=-0.19999999987163178, w1=0.20000000023929063\n",
      "Gradient Descent(38/99): loss =7.5412131240718275e+25, w0=-0.1999999998681121, w1=0.2000000002458462\n",
      "Gradient Descent(39/99): loss =7.537234721426937e+25, w0=-0.1999999998645946, w1=0.200000000252397\n",
      "Gradient Descent(40/99): loss =7.533270370157602e+25, w0=-0.19999999986107928, w1=0.20000000025894307\n",
      "Gradient Descent(41/99): loss =7.529319992298926e+25, w0=-0.1999999998575661, w1=0.2000000002654844\n",
      "Gradient Descent(42/99): loss =7.52538351110213e+25, w0=-0.1999999998540551, w1=0.200000000272021\n",
      "Gradient Descent(43/99): loss =7.5214608510009474e+25, w0=-0.1999999998505462, w1=0.20000000027855286\n",
      "Gradient Descent(44/99): loss =7.517551937579189e+25, w0=-0.19999999984703942, w1=0.20000000028508\n",
      "Gradient Descent(45/99): loss =7.513656697539206e+25, w0=-0.19999999984353473, w1=0.20000000029160242\n",
      "Gradient Descent(46/99): loss =7.509775058671425e+25, w0=-0.1999999998400321, w1=0.20000000029812012\n",
      "Gradient Descent(47/99): loss =7.505906949824721e+25, w0=-0.19999999983653155, w1=0.2000000003046331\n",
      "Gradient Descent(48/99): loss =7.502052300877772e+25, w0=-0.19999999983303304, w1=0.20000000031114137\n",
      "Gradient Descent(49/99): loss =7.498211042711309e+25, w0=-0.19999999982953656, w1=0.20000000031764495\n",
      "Gradient Descent(50/99): loss =7.494383107181128e+25, w0=-0.1999999998260421, w1=0.2000000003241438\n",
      "Gradient Descent(51/99): loss =7.4905684270920216e+25, w0=-0.19999999982254965, w1=0.20000000033063797\n",
      "Gradient Descent(52/99): loss =7.486766936172432e+25, w0=-0.1999999998190592, w1=0.20000000033712745\n",
      "Gradient Descent(53/99): loss =7.482978569049961e+25, w0=-0.1999999998155707, w1=0.20000000034361223\n",
      "Gradient Descent(54/99): loss =7.479203261227512e+25, w0=-0.1999999998120842, w1=0.20000000035009233\n",
      "Gradient Descent(55/99): loss =7.475440949060303e+25, w0=-0.19999999980859962, w1=0.20000000035656776\n",
      "Gradient Descent(56/99): loss =7.471691569733433e+25, w0=-0.19999999980511698, w1=0.2000000003630385\n",
      "Gradient Descent(57/99): loss =7.467955061240234e+25, w0=-0.19999999980163627, w1=0.20000000036950458\n",
      "Gradient Descent(58/99): loss =7.4642313623612035e+25, w0=-0.19999999979815747, w1=0.200000000375966\n",
      "Gradient Descent(59/99): loss =7.46052041264366e+25, w0=-0.19999999979468058, w1=0.20000000038242274\n",
      "Gradient Descent(60/99): loss =7.456822152381935e+25, w0=-0.19999999979120558, w1=0.20000000038887483\n",
      "Gradient Descent(61/99): loss =7.453136522598192e+25, w0=-0.19999999978773245, w1=0.20000000039532226\n",
      "Gradient Descent(62/99): loss =7.4494634650238435e+25, w0=-0.19999999978426117, w1=0.20000000040176505\n",
      "Gradient Descent(63/99): loss =7.445802922081494e+25, w0=-0.19999999978079175, w1=0.20000000040820318\n",
      "Gradient Descent(64/99): loss =7.442154836867472e+25, w0=-0.19999999977732416, w1=0.20000000041463667\n",
      "Gradient Descent(65/99): loss =7.438519153134812e+25, w0=-0.1999999997738584, w1=0.20000000042106553\n",
      "Gradient Descent(66/99): loss =7.434895815276856e+25, w0=-0.19999999977039445, w1=0.20000000042748975\n",
      "Gradient Descent(67/99): loss =7.431284768311234e+25, w0=-0.1999999997669323, w1=0.20000000043390936\n",
      "Gradient Descent(68/99): loss =7.427685957864403e+25, w0=-0.19999999976347194, w1=0.20000000044032434\n",
      "Gradient Descent(69/99): loss =7.424099330156617e+25, w0=-0.19999999976001334, w1=0.20000000044673472\n",
      "Gradient Descent(70/99): loss =7.420524831987355e+25, w0=-0.19999999975655652, w1=0.20000000045314048\n",
      "Gradient Descent(71/99): loss =7.416962410721183e+25, w0=-0.19999999975310145, w1=0.20000000045954164\n",
      "Gradient Descent(72/99): loss =7.413412014274036e+25, w0=-0.19999999974964813, w1=0.2000000004659382\n",
      "Gradient Descent(73/99): loss =7.4098735910999e+25, w0=-0.19999999974619656, w1=0.20000000047233013\n",
      "Gradient Descent(74/99): loss =7.406347090177912e+25, w0=-0.1999999997427467, w1=0.2000000004787175\n",
      "Gradient Descent(75/99): loss =7.402832460999849e+25, w0=-0.19999999973929858, w1=0.20000000048510025\n",
      "Gradient Descent(76/99): loss =7.399329653557928e+25, w0=-0.19999999973585214, w1=0.20000000049147842\n",
      "Gradient Descent(77/99): loss =7.395838618333068e+25, w0=-0.1999999997324074, w1=0.20000000049785202\n",
      "Gradient Descent(78/99): loss =7.392359306283367e+25, w0=-0.19999999972896434, w1=0.20000000050422104\n",
      "Gradient Descent(79/99): loss =7.388891668833093e+25, w0=-0.19999999972552296, w1=0.2000000005105855\n",
      "Gradient Descent(80/99): loss =7.385435657861834e+25, w0=-0.19999999972208324, w1=0.2000000005169454\n",
      "Gradient Descent(81/99): loss =7.381991225694038e+25, w0=-0.19999999971864518, w1=0.20000000052330072\n",
      "Gradient Descent(82/99): loss =7.37855832508894e+25, w0=-0.19999999971520876, w1=0.2000000005296515\n",
      "Gradient Descent(83/99): loss =7.375136909230599e+25, w0=-0.19999999971177398, w1=0.20000000053599773\n",
      "Gradient Descent(84/99): loss =7.3717269317184235e+25, w0=-0.19999999970834084, w1=0.2000000005423394\n",
      "Gradient Descent(85/99): loss =7.36832834655784e+25, w0=-0.1999999997049093, w1=0.20000000054867656\n",
      "Gradient Descent(86/99): loss =7.364941108151332e+25, w0=-0.1999999997014794, w1=0.2000000005550092\n",
      "Gradient Descent(87/99): loss =7.361565171289606e+25, w0=-0.19999999969805105, w1=0.20000000056133727\n",
      "Gradient Descent(88/99): loss =7.358200491143219e+25, w0=-0.19999999969462431, w1=0.20000000056766082\n",
      "Gradient Descent(89/99): loss =7.354847023254264e+25, w0=-0.19999999969119917, w1=0.20000000057397987\n",
      "Gradient Descent(90/99): loss =7.351504723528367e+25, w0=-0.19999999968777557, w1=0.2000000005802944\n",
      "Gradient Descent(91/99): loss =7.3481735482269535e+25, w0=-0.19999999968435356, w1=0.20000000058660444\n",
      "Gradient Descent(92/99): loss =7.344853453959687e+25, w0=-0.1999999996809331, w1=0.20000000059290995\n",
      "Gradient Descent(93/99): loss =7.341544397677161e+25, w0=-0.1999999996775142, w1=0.20000000059921097\n",
      "Gradient Descent(94/99): loss =7.3382463366637595e+25, w0=-0.19999999967409682, w1=0.20000000060550752\n",
      "Gradient Descent(95/99): loss =7.3349592285307735e+25, w0=-0.19999999967068097, w1=0.20000000061179957\n",
      "Gradient Descent(96/99): loss =7.331683031209721e+25, w0=-0.19999999966726664, w1=0.20000000061808712\n",
      "Gradient Descent(97/99): loss =7.328417702945788e+25, w0=-0.19999999966385384, w1=0.2000000006243702\n",
      "Gradient Descent(98/99): loss =7.3251632022915405e+25, w0=-0.19999999966044255, w1=0.20000000063064882\n",
      "Gradient Descent(99/99): loss =7.321919488100766e+25, w0=-0.19999999965703275, w1=0.20000000063692297\n",
      "Optimizing degree 14/15, model: least_squares_GD, arguments: {'max_iters': 100}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =3.042027210881236e+51, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =2.450593753144629e+28, w0=-0.19999999999951254, w1=0.20000000000084614\n",
      "Gradient Descent(3/99): loss =2.449794905668303e+28, w0=-0.19999999999902648, w1=0.2000000000016922\n",
      "Gradient Descent(4/99): loss =2.4490061299203176e+28, w0=-0.1999999999985406, w1=0.2000000000025378\n",
      "Gradient Descent(5/99): loss =2.448219056528556e+28, w0=-0.19999999999805487, w1=0.2000000000033829\n",
      "Gradient Descent(6/99): loss =2.4474336685367984e+28, w0=-0.1999999999975693, w1=0.20000000000422752\n",
      "Gradient Descent(7/99): loss =2.4466499550656306e+28, w0=-0.19999999999708384, w1=0.2000000000050717\n",
      "Gradient Descent(8/99): loss =2.445867905905007e+28, w0=-0.19999999999659857, w1=0.20000000000591545\n",
      "Gradient Descent(9/99): loss =2.445087511424396e+28, w0=-0.19999999999611343, w1=0.20000000000675874\n",
      "Gradient Descent(10/99): loss =2.4443087625098675e+28, w0=-0.19999999999562842, w1=0.2000000000076016\n",
      "Gradient Descent(11/99): loss =2.443531650511178e+28, w0=-0.19999999999514356, w1=0.200000000008444\n",
      "Gradient Descent(12/99): loss =2.442756167196724e+28, w0=-0.19999999999465887, w1=0.200000000009286\n",
      "Gradient Descent(13/99): loss =2.4419823047149396e+28, w0=-0.1999999999941743, w1=0.20000000001012758\n",
      "Gradient Descent(14/99): loss =2.4412100555608033e+28, w0=-0.1999999999936899, w1=0.2000000000109687\n",
      "Gradient Descent(15/99): loss =2.440439412546608e+28, w0=-0.1999999999932056, w1=0.20000000001180943\n",
      "Gradient Descent(16/99): loss =2.439670368776198e+28, w0=-0.19999999999272147, w1=0.20000000001264973\n",
      "Gradient Descent(17/99): loss =2.4389029176220674e+28, w0=-0.19999999999223747, w1=0.2000000000134896\n",
      "Gradient Descent(18/99): loss =2.438137052704948e+28, w0=-0.1999999999917536, w1=0.2000000000143291\n",
      "Gradient Descent(19/99): loss =2.4373727678753714e+28, w0=-0.19999999999126988, w1=0.20000000001516818\n",
      "Gradient Descent(20/99): loss =2.436610057197067e+28, w0=-0.1999999999907863, w1=0.20000000001600685\n",
      "Gradient Descent(21/99): loss =2.4358489149318054e+28, w0=-0.19999999999030282, w1=0.20000000001684512\n",
      "Gradient Descent(22/99): loss =2.4350893355256216e+28, w0=-0.1999999999898195, w1=0.20000000001768298\n",
      "Gradient Descent(23/99): loss =2.4343313135961475e+28, w0=-0.1999999999893363, w1=0.20000000001852042\n",
      "Gradient Descent(24/99): loss =2.4335748439210133e+28, w0=-0.19999999998885323, w1=0.20000000001935747\n",
      "Gradient Descent(25/99): loss =2.4328199214271235e+28, w0=-0.19999999998837031, w1=0.2000000000201941\n",
      "Gradient Descent(26/99): loss =2.432066541180787e+28, w0=-0.1999999999878875, w1=0.20000000002103036\n",
      "Gradient Descent(27/99): loss =2.4313146983785734e+28, w0=-0.19999999998740484, w1=0.2000000000218662\n",
      "Gradient Descent(28/99): loss =2.4305643883388314e+28, w0=-0.1999999999869223, w1=0.20000000002270163\n",
      "Gradient Descent(29/99): loss =2.4298156064938455e+28, w0=-0.19999999998643989, w1=0.20000000002353668\n",
      "Gradient Descent(30/99): loss =2.429068348382519e+28, w0=-0.1999999999859576, w1=0.20000000002437132\n",
      "Gradient Descent(31/99): loss =2.428322609643606e+28, w0=-0.19999999998547546, w1=0.20000000002520557\n",
      "Gradient Descent(32/99): loss =2.4275783860093893e+28, w0=-0.19999999998499343, w1=0.2000000000260394\n",
      "Gradient Descent(33/99): loss =2.4268356732998085e+28, w0=-0.19999999998451154, w1=0.20000000002687285\n",
      "Gradient Descent(34/99): loss =2.4260944674169807e+28, w0=-0.19999999998402976, w1=0.2000000000277059\n",
      "Gradient Descent(35/99): loss =2.4253547643401133e+28, w0=-0.19999999998354812, w1=0.20000000002853854\n",
      "Gradient Descent(36/99): loss =2.424616560120736e+28, w0=-0.19999999998306658, w1=0.2000000000293708\n",
      "Gradient Descent(37/99): loss =2.423879850878287e+28, w0=-0.1999999999825852, w1=0.20000000003020266\n",
      "Gradient Descent(38/99): loss =2.4231446327959537e+28, w0=-0.1999999999821039, w1=0.20000000003103413\n",
      "Gradient Descent(39/99): loss =2.4224109021168485e+28, w0=-0.19999999998162277, w1=0.2000000000318652\n",
      "Gradient Descent(40/99): loss =2.4216786551403806e+28, w0=-0.19999999998114174, w1=0.20000000003269586\n",
      "Gradient Descent(41/99): loss =2.4209478882189246e+28, w0=-0.19999999998066084, w1=0.20000000003352614\n",
      "Gradient Descent(42/99): loss =2.4202185977546645e+28, w0=-0.19999999998018006, w1=0.200000000034356\n",
      "Gradient Descent(43/99): loss =2.4194907801966895e+28, w0=-0.1999999999796994, w1=0.20000000003518548\n",
      "Gradient Descent(44/99): loss =2.4187644320382545e+28, w0=-0.19999999997921886, w1=0.20000000003601456\n",
      "Gradient Descent(45/99): loss =2.418039549814227e+28, w0=-0.19999999997873844, w1=0.20000000003684326\n",
      "Gradient Descent(46/99): loss =2.41731613009872e+28, w0=-0.19999999997825813, w1=0.20000000003767157\n",
      "Gradient Descent(47/99): loss =2.4165941695028652e+28, w0=-0.19999999997777795, w1=0.20000000003849946\n",
      "Gradient Descent(48/99): loss =2.415873664672735e+28, w0=-0.1999999999772979, w1=0.20000000003932697\n",
      "Gradient Descent(49/99): loss =2.4151546122874095e+28, w0=-0.19999999997681794, w1=0.20000000004015409\n",
      "Gradient Descent(50/99): loss =2.41443700905717e+28, w0=-0.19999999997633813, w1=0.2000000000409808\n",
      "Gradient Descent(51/99): loss =2.4137208517218066e+28, w0=-0.19999999997585843, w1=0.20000000004180715\n",
      "Gradient Descent(52/99): loss =2.4130061370490455e+28, w0=-0.19999999997537884, w1=0.20000000004263307\n",
      "Gradient Descent(53/99): loss =2.412292861833069e+28, w0=-0.19999999997489937, w1=0.2000000000434586\n",
      "Gradient Descent(54/99): loss =2.411581022893147e+28, w0=-0.19999999997442003, w1=0.20000000004428375\n",
      "Gradient Descent(55/99): loss =2.410870617072347e+28, w0=-0.1999999999739408, w1=0.2000000000451085\n",
      "Gradient Descent(56/99): loss =2.410161641236329e+28, w0=-0.19999999997346168, w1=0.20000000004593288\n",
      "Gradient Descent(57/99): loss =2.409454092272245e+28, w0=-0.19999999997298268, w1=0.20000000004675686\n",
      "Gradient Descent(58/99): loss =2.408747967087664e+28, w0=-0.19999999997250378, w1=0.20000000004758045\n",
      "Gradient Descent(59/99): loss =2.408043262609608e+28, w0=-0.19999999997202503, w1=0.20000000004840363\n",
      "Gradient Descent(60/99): loss =2.40733997578364e+28, w0=-0.19999999997154638, w1=0.2000000000492264\n",
      "Gradient Descent(61/99): loss =2.4066381035730057e+28, w0=-0.19999999997106785, w1=0.2000000000500488\n",
      "Gradient Descent(62/99): loss =2.405937642957829e+28, w0=-0.19999999997058943, w1=0.20000000005087082\n",
      "Gradient Descent(63/99): loss =2.4052385909343795e+28, w0=-0.19999999997011111, w1=0.20000000005169244\n",
      "Gradient Descent(64/99): loss =2.4045409445143646e+28, w0=-0.1999999999696329, w1=0.20000000005251367\n",
      "Gradient Descent(65/99): loss =2.4038447007242785e+28, w0=-0.19999999996915485, w1=0.20000000005333451\n",
      "Gradient Descent(66/99): loss =2.4031498566048e+28, w0=-0.1999999999686769, w1=0.20000000005415497\n",
      "Gradient Descent(67/99): loss =2.4024564092102197e+28, w0=-0.19999999996819906, w1=0.20000000005497504\n",
      "Gradient Descent(68/99): loss =2.401764355607904e+28, w0=-0.19999999996772133, w1=0.2000000000557947\n",
      "Gradient Descent(69/99): loss =2.4010736928778064e+28, w0=-0.1999999999672437, w1=0.200000000056614\n",
      "Gradient Descent(70/99): loss =2.400384418111985e+28, w0=-0.1999999999667662, w1=0.2000000000574329\n",
      "Gradient Descent(71/99): loss =2.3996965284141978e+28, w0=-0.1999999999662888, w1=0.20000000005825141\n",
      "Gradient Descent(72/99): loss =2.3990100208994583e+28, w0=-0.19999999996581153, w1=0.20000000005906954\n",
      "Gradient Descent(73/99): loss =2.3983248926936855e+28, w0=-0.19999999996533435, w1=0.20000000005988727\n",
      "Gradient Descent(74/99): loss =2.397641140933336e+28, w0=-0.1999999999648573, w1=0.20000000006070462\n",
      "Gradient Descent(75/99): loss =2.396958762765063e+28, w0=-0.19999999996438034, w1=0.20000000006152158\n",
      "Gradient Descent(76/99): loss =2.3962777553454263e+28, w0=-0.1999999999639035, w1=0.20000000006233815\n",
      "Gradient Descent(77/99): loss =2.3955981158405687e+28, w0=-0.19999999996342677, w1=0.20000000006315433\n",
      "Gradient Descent(78/99): loss =2.3949198414259775e+28, w0=-0.19999999996295015, w1=0.20000000006397012\n",
      "Gradient Descent(79/99): loss =2.394242929286203e+28, w0=-0.19999999996247364, w1=0.20000000006478552\n",
      "Gradient Descent(80/99): loss =2.393567376614632e+28, w0=-0.19999999996199724, w1=0.20000000006560054\n",
      "Gradient Descent(81/99): loss =2.3928931806132584e+28, w0=-0.19999999996152096, w1=0.2000000000664152\n",
      "Gradient Descent(82/99): loss =2.3922203384924783e+28, w0=-0.19999999996104478, w1=0.20000000006722946\n",
      "Gradient Descent(83/99): loss =2.391548847470884e+28, w0=-0.19999999996056872, w1=0.20000000006804333\n",
      "Gradient Descent(84/99): loss =2.3908787047750893e+28, w0=-0.19999999996009277, w1=0.20000000006885682\n",
      "Gradient Descent(85/99): loss =2.3902099076395512e+28, w0=-0.19999999995961693, w1=0.20000000006966992\n",
      "Gradient Descent(86/99): loss =2.389542453306408e+28, w0=-0.1999999999591412, w1=0.20000000007048263\n",
      "Gradient Descent(87/99): loss =2.3888763390253257e+28, w0=-0.19999999995866558, w1=0.20000000007129498\n",
      "Gradient Descent(88/99): loss =2.3882115620533625e+28, w0=-0.19999999995819007, w1=0.20000000007210694\n",
      "Gradient Descent(89/99): loss =2.387548119654825e+28, w0=-0.19999999995771467, w1=0.20000000007291852\n",
      "Gradient Descent(90/99): loss =2.3868860091011503e+28, w0=-0.19999999995723938, w1=0.2000000000737297\n",
      "Gradient Descent(91/99): loss =2.3862252276707806e+28, w0=-0.1999999999567642, w1=0.2000000000745405\n",
      "Gradient Descent(92/99): loss =2.38556577264906e+28, w0=-0.19999999995628914, w1=0.20000000007535093\n",
      "Gradient Descent(93/99): loss =2.3849076413281207e+28, w0=-0.1999999999558142, w1=0.20000000007616098\n",
      "Gradient Descent(94/99): loss =2.3842508310067966e+28, w0=-0.19999999995533935, w1=0.20000000007697064\n",
      "Gradient Descent(95/99): loss =2.38359533899052e+28, w0=-0.1999999999548646, w1=0.20000000007777993\n",
      "Gradient Descent(96/99): loss =2.38294116259124e+28, w0=-0.19999999995438994, w1=0.20000000007858884\n",
      "Gradient Descent(97/99): loss =2.3822882991273407e+28, w0=-0.1999999999539154, w1=0.20000000007939736\n",
      "Gradient Descent(98/99): loss =2.3816367459235606e+28, w0=-0.19999999995344098, w1=0.20000000008020552\n",
      "Gradient Descent(99/99): loss =2.380986500310925e+28, w0=-0.19999999995296666, w1=0.2000000000810133\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =3.042027210881237e+51, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =2.0157478998274897e+28, w0=-0.19999999999970097, w1=0.2000000000000029\n",
      "Gradient Descent(3/99): loss =1.99479824436388e+28, w0=-0.19999999999942453, w1=0.1999999999999596\n",
      "Gradient Descent(4/99): loss =1.9799174838026018e+28, w0=-0.19999999999915682, w1=0.19999999999988022\n",
      "Gradient Descent(5/99): loss =1.9688712932166084e+28, w0=-0.19999999999889664, w1=0.19999999999976983\n",
      "Gradient Descent(6/99): loss =1.9606700652003077e+28, w0=-0.19999999999864293, w1=0.19999999999963278\n",
      "Gradient Descent(7/99): loss =1.9545799364786608e+28, w0=-0.19999999999839477, w1=0.1999999999994728\n",
      "Gradient Descent(8/99): loss =1.9500565382275597e+28, w0=-0.1999999999981514, w1=0.19999999999929305\n",
      "Gradient Descent(9/99): loss =1.9466959791089704e+28, w0=-0.19999999999791218, w1=0.19999999999909632\n",
      "Gradient Descent(10/99): loss =1.944198546523453e+28, w0=-0.19999999999767654, w1=0.19999999999888496\n",
      "Gradient Descent(11/99): loss =1.9423418087121747e+28, w0=-0.19999999999744394, w1=0.199999999998661\n",
      "Gradient Descent(12/99): loss =1.9409606734221968e+28, w0=-0.19999999999721402, w1=0.1999999999984262\n",
      "Gradient Descent(13/99): loss =1.9399325986784157e+28, w0=-0.19999999999698637, w1=0.19999999999818202\n",
      "Gradient Descent(14/99): loss =1.9391666216318916e+28, w0=-0.19999999999676069, w1=0.1999999999979298\n",
      "Gradient Descent(15/99): loss =1.9385952181801886e+28, w0=-0.19999999999653667, w1=0.19999999999767065\n",
      "Gradient Descent(16/99): loss =1.9381682620797656e+28, w0=-0.19999999999631413, w1=0.1999999999974055\n",
      "Gradient Descent(17/99): loss =1.9378485415806318e+28, w0=-0.19999999999609283, w1=0.19999999999713522\n",
      "Gradient Descent(18/99): loss =1.9376084317384127e+28, w0=-0.19999999999587262, w1=0.1999999999968605\n",
      "Gradient Descent(19/99): loss =1.9374274243565197e+28, w0=-0.19999999999565335, w1=0.19999999999658194\n",
      "Gradient Descent(20/99): loss =1.9372902944435662e+28, w0=-0.19999999999543489, w1=0.19999999999630008\n",
      "Gradient Descent(21/99): loss =1.9371857391158659e+28, w0=-0.19999999999521711, w1=0.19999999999601537\n",
      "Gradient Descent(22/99): loss =1.9371053671862004e+28, w0=-0.19999999999499996, w1=0.1999999999957282\n",
      "Gradient Descent(23/99): loss =1.9370429490710475e+28, w0=-0.1999999999947833, w1=0.1999999999954389\n",
      "Gradient Descent(24/99): loss =1.936993859941227e+28, w0=-0.19999999999456708, w1=0.1999999999951478\n",
      "Gradient Descent(25/99): loss =1.9369546663272962e+28, w0=-0.19999999999435125, w1=0.19999999999485513\n",
      "Gradient Descent(26/99): loss =1.936922819220771e+28, w0=-0.19999999999413576, w1=0.19999999999456108\n",
      "Gradient Descent(27/99): loss =1.936896426235226e+28, w0=-0.19999999999392054, w1=0.19999999999426588\n",
      "Gradient Descent(28/99): loss =1.9368740824599856e+28, w0=-0.19999999999370557, w1=0.19999999999396964\n",
      "Gradient Descent(29/99): loss =1.936854744886542e+28, w0=-0.19999999999349083, w1=0.19999999999367254\n",
      "Gradient Descent(30/99): loss =1.9368376391829248e+28, w0=-0.19999999999327625, w1=0.1999999999933747\n",
      "Gradient Descent(31/99): loss =1.93682219048306e+28, w0=-0.19999999999306184, w1=0.1999999999930762\n",
      "Gradient Descent(32/99): loss =1.9368079720047713e+28, w0=-0.19999999999284757, w1=0.19999999999277712\n",
      "Gradient Descent(33/99): loss =1.9367946669037078e+28, w0=-0.1999999999926334, w1=0.19999999999247756\n",
      "Gradient Descent(34/99): loss =1.9367820399536457e+28, w0=-0.19999999999241935, w1=0.19999999999217757\n",
      "Gradient Descent(35/99): loss =1.9367699165218954e+28, w0=-0.19999999999220539, w1=0.19999999999187723\n",
      "Gradient Descent(36/99): loss =1.9367581669605786e+28, w0=-0.1999999999919915, w1=0.19999999999157655\n",
      "Gradient Descent(37/99): loss =1.9367466950186767e+28, w0=-0.1999999999917777, w1=0.19999999999127563\n",
      "Gradient Descent(38/99): loss =1.936735429239113e+28, w0=-0.19999999999156395, w1=0.19999999999097445\n",
      "Gradient Descent(39/99): loss =1.9367243165719046e+28, w0=-0.19999999999135024, w1=0.19999999999067308\n",
      "Gradient Descent(40/99): loss =1.9367133176325648e+28, w0=-0.19999999999113657, w1=0.19999999999037152\n",
      "Gradient Descent(41/99): loss =1.9367024031819084e+28, w0=-0.19999999999092294, w1=0.1999999999900698\n",
      "Gradient Descent(42/99): loss =1.9366915515126604e+28, w0=-0.19999999999070936, w1=0.19999999998976792\n",
      "Gradient Descent(43/99): loss =1.9366807465092617e+28, w0=-0.1999999999904958, w1=0.19999999998946594\n",
      "Gradient Descent(44/99): loss =1.936669976207465e+28, w0=-0.19999999999028228, w1=0.19999999998916385\n",
      "Gradient Descent(45/99): loss =1.9366592317249666e+28, w0=-0.19999999999006876, w1=0.19999999998886167\n",
      "Gradient Descent(46/99): loss =1.9366485064675101e+28, w0=-0.19999999998985526, w1=0.1999999999885594\n",
      "Gradient Descent(47/99): loss =1.9366377955394933e+28, w0=-0.1999999999896418, w1=0.19999999998825707\n",
      "Gradient Descent(48/99): loss =1.936627095306409e+28, w0=-0.19999999998942833, w1=0.19999999998795467\n",
      "Gradient Descent(49/99): loss =1.936616403069973e+28, w0=-0.1999999999892149, w1=0.19999999998765222\n",
      "Gradient Descent(50/99): loss =1.9366057168269768e+28, w0=-0.19999999998900145, w1=0.1999999999873497\n",
      "Gradient Descent(51/99): loss =1.936595035090227e+28, w0=-0.19999999998878804, w1=0.19999999998704718\n",
      "Gradient Descent(52/99): loss =1.9365843567556197e+28, w0=-0.19999999998857462, w1=0.1999999999867446\n",
      "Gradient Descent(53/99): loss =1.9365736810034687e+28, w0=-0.1999999999883612, w1=0.19999999998644197\n",
      "Gradient Descent(54/99): loss =1.9365630072252306e+28, w0=-0.19999999998814783, w1=0.1999999999861393\n",
      "Gradient Descent(55/99): loss =1.9365523349691228e+28, w0=-0.19999999998793444, w1=0.1999999999858366\n",
      "Gradient Descent(56/99): loss =1.9365416638997296e+28, w0=-0.19999999998772106, w1=0.19999999998553386\n",
      "Gradient Descent(57/99): loss =1.9365309937680483e+28, w0=-0.19999999998750767, w1=0.1999999999852311\n",
      "Gradient Descent(58/99): loss =1.9365203243892054e+28, w0=-0.19999999998729429, w1=0.19999999998492832\n",
      "Gradient Descent(59/99): loss =1.9365096556259547e+28, w0=-0.19999999998708093, w1=0.19999999998462553\n",
      "Gradient Descent(60/99): loss =1.936498987376401e+28, w0=-0.19999999998686757, w1=0.19999999998432272\n",
      "Gradient Descent(61/99): loss =1.9364883195648922e+28, w0=-0.19999999998665421, w1=0.19999999998401988\n",
      "Gradient Descent(62/99): loss =1.936477652135268e+28, w0=-0.19999999998644086, w1=0.199999999983717\n",
      "Gradient Descent(63/99): loss =1.9364669850458253e+28, w0=-0.1999999999862275, w1=0.19999999998341414\n",
      "Gradient Descent(64/99): loss =1.9364563182656102e+28, w0=-0.19999999998601414, w1=0.19999999998311124\n",
      "Gradient Descent(65/99): loss =1.9364456517716308e+28, w0=-0.1999999999858008, w1=0.19999999998280832\n",
      "Gradient Descent(66/99): loss =1.9364349855468272e+28, w0=-0.19999999998558748, w1=0.1999999999825054\n",
      "Gradient Descent(67/99): loss =1.9364243195785258e+28, w0=-0.19999999998537416, w1=0.19999999998220244\n",
      "Gradient Descent(68/99): loss =1.9364136538573117e+28, w0=-0.19999999998516083, w1=0.1999999999818995\n",
      "Gradient Descent(69/99): loss =1.9364029883762028e+28, w0=-0.1999999999849475, w1=0.1999999999815965\n",
      "Gradient Descent(70/99): loss =1.9363923231300068e+28, w0=-0.19999999998473417, w1=0.19999999998129353\n",
      "Gradient Descent(71/99): loss =1.9363816581148687e+28, w0=-0.19999999998452084, w1=0.19999999998099052\n",
      "Gradient Descent(72/99): loss =1.9363709933279283e+28, w0=-0.1999999999843075, w1=0.19999999998068752\n",
      "Gradient Descent(73/99): loss =1.9363603287670548e+28, w0=-0.1999999999840942, w1=0.19999999998038448\n",
      "Gradient Descent(74/99): loss =1.9363496644306666e+28, w0=-0.1999999999838809, w1=0.19999999998008144\n",
      "Gradient Descent(75/99): loss =1.9363390003175861e+28, w0=-0.1999999999836676, w1=0.19999999997977838\n",
      "Gradient Descent(76/99): loss =1.9363283364269428e+28, w0=-0.1999999999834543, w1=0.1999999999794753\n",
      "Gradient Descent(77/99): loss =1.9363176727580822e+28, w0=-0.199999999983241, w1=0.1999999999791722\n",
      "Gradient Descent(78/99): loss =1.9363070093105187e+28, w0=-0.1999999999830277, w1=0.19999999997886908\n",
      "Gradient Descent(79/99): loss =1.9362963460838891e+28, w0=-0.1999999999828144, w1=0.19999999997856596\n",
      "Gradient Descent(80/99): loss =1.9362856830779252e+28, w0=-0.1999999999826011, w1=0.19999999997826282\n",
      "Gradient Descent(81/99): loss =1.936275020292421e+28, w0=-0.19999999998238782, w1=0.19999999997795967\n",
      "Gradient Descent(82/99): loss =1.9362643577272257e+28, w0=-0.19999999998217455, w1=0.19999999997765652\n",
      "Gradient Descent(83/99): loss =1.9362536953822222e+28, w0=-0.19999999998196127, w1=0.19999999997735335\n",
      "Gradient Descent(84/99): loss =1.9362430332573248e+28, w0=-0.199999999981748, w1=0.19999999997705017\n",
      "Gradient Descent(85/99): loss =1.936232371352466e+28, w0=-0.19999999998153473, w1=0.19999999997674697\n",
      "Gradient Descent(86/99): loss =1.9362217096675922e+28, w0=-0.19999999998132145, w1=0.19999999997644377\n",
      "Gradient Descent(87/99): loss =1.936211048202667e+28, w0=-0.19999999998110818, w1=0.19999999997614054\n",
      "Gradient Descent(88/99): loss =1.9362003869576545e+28, w0=-0.1999999999808949, w1=0.1999999999758373\n",
      "Gradient Descent(89/99): loss =1.936189725932533e+28, w0=-0.19999999998068163, w1=0.19999999997553405\n",
      "Gradient Descent(90/99): loss =1.9361790651272792e+28, w0=-0.19999999998046836, w1=0.1999999999752308\n",
      "Gradient Descent(91/99): loss =1.9361684045418751e+28, w0=-0.1999999999802551, w1=0.1999999999749275\n",
      "Gradient Descent(92/99): loss =1.9361577441763096e+28, w0=-0.19999999998004186, w1=0.19999999997462423\n",
      "Gradient Descent(93/99): loss =1.9361470840305653e+28, w0=-0.19999999997982862, w1=0.1999999999743209\n",
      "Gradient Descent(94/99): loss =1.9361364241046342e+28, w0=-0.19999999997961537, w1=0.1999999999740176\n",
      "Gradient Descent(95/99): loss =1.9361257643985021e+28, w0=-0.19999999997940213, w1=0.19999999997371426\n",
      "Gradient Descent(96/99): loss =1.9361151049121628e+28, w0=-0.19999999997918888, w1=0.19999999997341092\n",
      "Gradient Descent(97/99): loss =1.9361044456456051e+28, w0=-0.19999999997897563, w1=0.19999999997310755\n",
      "Gradient Descent(98/99): loss =1.9360937865988205e+28, w0=-0.1999999999787624, w1=0.19999999997280418\n",
      "Gradient Descent(99/99): loss =1.9360831277718025e+28, w0=-0.19999999997854914, w1=0.19999999997250079\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.569561288842866e+35, w0=0.0, w1=0.2\n",
      "Gradient Descent(2/99): loss =2.528136031891774e+17, w0=2.2241503707596267e-10, w1=0.2000000005606941\n",
      "Gradient Descent(3/99): loss =1.188581933301813e+16, w0=2.5487970045338313e-10, w1=0.20000000097180842\n",
      "Gradient Descent(4/99): loss =8989321836893408.0, w0=3.0671699363080666e-10, w1=0.20000000137939\n",
      "Gradient Descent(5/99): loss =8940507944835401.0, w0=3.554667567133118e-10, w1=0.20000000177437838\n",
      "Gradient Descent(6/99): loss =8933264939989931.0, w0=4.0392124970234027e-10, w1=0.20000000216226702\n",
      "Gradient Descent(7/99): loss =8929805312813358.0, w0=4.5199454866187805e-10, w1=0.20000000254539646\n",
      "Gradient Descent(8/99): loss =8927792694239271.0, w0=4.998390574762517e-10, w1=0.20000000292543\n",
      "Gradient Descent(9/99): loss =8926394940521877.0, w0=5.475315133316662e-10, w1=0.20000000330343823\n",
      "Gradient Descent(10/99): loss =8925259305272644.0, w0=5.951244951045804e-10, w1=0.20000000368012127\n",
      "Gradient Descent(11/99): loss =8924235457056127.0, w0=6.426520559393406e-10, w1=0.20000000405593565\n",
      "Gradient Descent(12/99): loss =8923259318847902.0, w0=6.901364548551961e-10, w1=0.20000000443117907\n",
      "Gradient Descent(13/99): loss =8922303578410265.0, w0=7.375922184163525e-10, w1=0.20000000480604557\n",
      "Gradient Descent(14/99): loss =8921356594123481.0, w0=7.850288287662533e-10, w1=0.20000000518066155\n",
      "Gradient Descent(15/99): loss =8920413403817345.0, w0=8.324524745663053e-10, w1=0.20000000555510933\n",
      "Gradient Descent(16/99): loss =8919471892376984.0, w0=8.798671945315039e-10, w1=0.20000000592944242\n",
      "Gradient Descent(17/99): loss =8918531158222904.0, w0=9.272756239354945e-10, w1=0.2000000063036956\n",
      "Gradient Descent(18/99): loss =8917590817038332.0, w0=9.746794819619373e-10, w1=0.20000000667789142\n",
      "Gradient Descent(19/99): loss =8916650704988954.0, w0=1.0220798898594988e-09, w1=0.2000000070520444\n",
      "Gradient Descent(20/99): loss =8915710752222368.0, w0=1.0694775786368703e-09, w1=0.20000000742616397\n",
      "Gradient Descent(21/99): loss =8914770928947752.0, w0=1.1168730246440336e-09, w1=0.20000000780025615\n",
      "Gradient Descent(22/99): loss =8913831222452222.0, w0=1.1642665380746679e-09, w1=0.20000000817432476\n",
      "Gradient Descent(23/99): loss =8912891627303806.0, w0=1.2116583207339738e-09, w1=0.20000000854837222\n",
      "Gradient Descent(24/99): loss =8911952141175169.0, w0=1.2590485037423777e-09, w1=0.20000000892240005\n",
      "Gradient Descent(25/99): loss =8911012763063368.0, w0=1.3064371721413542e-09, w1=0.20000000929640918\n",
      "Gradient Descent(26/99): loss =8910073492530580.0, w0=1.3538243809492452e-09, w1=0.20000000967040016\n",
      "Gradient Descent(27/99): loss =8909134329380489.0, w0=1.401210165636096e-09, w1=0.2000000100443733\n",
      "Gradient Descent(28/99): loss =8908195273520179.0, w0=1.4485945489557648e-09, w1=0.20000001041832874\n",
      "Gradient Descent(29/99): loss =8907256324901115.0, w0=1.4959775454006232e-09, w1=0.20000001079226656\n",
      "Gradient Descent(30/99): loss =8906317483493934.0, w0=1.5433591641048287e-09, w1=0.20000001116618676\n",
      "Gradient Descent(31/99): loss =8905378749277682.0, w0=1.5907394107353457e-09, w1=0.20000001154008934\n",
      "Gradient Descent(32/99): loss =8904440122235142.0, w0=1.638118288722645e-09, w1=0.20000001191397423\n",
      "Gradient Descent(33/99): loss =8903501602350748.0, w0=1.6854958000607824e-09, w1=0.2000000122878414\n",
      "Gradient Descent(34/99): loss =8902563189609808.0, w0=1.7328719458267582e-09, w1=0.2000000126616908\n",
      "Gradient Descent(35/99): loss =8901624883998052.0, w0=1.7802467265169743e-09, w1=0.20000001303552237\n",
      "Gradient Descent(36/99): loss =8900686685501418.0, w0=1.8276201422646035e-09, w1=0.20000001340933607\n",
      "Gradient Descent(37/99): loss =8899748594106038.0, w0=1.8749921929794955e-09, w1=0.20000001378313184\n",
      "Gradient Descent(38/99): loss =8898810609798074.0, w0=1.922362878437756e-09, w1=0.20000001415690966\n",
      "Gradient Descent(39/99): loss =8897872732563812.0, w0=1.9697321983386895e-09, w1=0.20000001453066948\n",
      "Gradient Descent(40/99): loss =8896934962389549.0, w0=2.017100152340625e-09, w1=0.2000000149044113\n",
      "Gradient Descent(41/99): loss =8895997299261672.0, w0=2.0644667400831233e-09, w1=0.20000001527813507\n",
      "Gradient Descent(42/99): loss =8895059743166575.0, w0=2.111831961200438e-09, w1=0.20000001565184075\n",
      "Gradient Descent(43/99): loss =8894122294090649.0, w0=2.1591958153294005e-09, w1=0.20000001602552833\n",
      "Gradient Descent(44/99): loss =8893184952020342.0, w0=2.2065583021137692e-09, w1=0.20000001639919782\n",
      "Gradient Descent(45/99): loss =8892247716942095.0, w0=2.2539194212063796e-09, w1=0.20000001677284915\n",
      "Gradient Descent(46/99): loss =8891310588842378.0, w0=2.3012791722699337e-09, w1=0.20000001714648236\n",
      "Gradient Descent(47/99): loss =8890373567707680.0, w0=2.348637554976988e-09, w1=0.20000001752009738\n",
      "Gradient Descent(48/99): loss =8889436653524493.0, w0=2.3959945690094773e-09, w1=0.20000001789369426\n",
      "Gradient Descent(49/99): loss =8888499846279305.0, w0=2.4433502140579965e-09, w1=0.20000001826727296\n",
      "Gradient Descent(50/99): loss =8887563145958619.0, w0=2.4907044898209775e-09, w1=0.20000001864083344\n",
      "Gradient Descent(51/99): loss =8886626552548955.0, w0=2.538057396003837e-09, w1=0.20000001901437575\n",
      "Gradient Descent(52/99): loss =8885690066036837.0, w0=2.5854089323181514e-09, w1=0.20000001938789985\n",
      "Gradient Descent(53/99): loss =8884753686408799.0, w0=2.632759098480879e-09, w1=0.20000001976140575\n",
      "Gradient Descent(54/99): loss =8883817413651347.0, w0=2.6801078942136436e-09, w1=0.2000000201348934\n",
      "Gradient Descent(55/99): loss =8882881247751050.0, w0=2.7274553192420877e-09, w1=0.20000002050836285\n",
      "Gradient Descent(56/99): loss =8881945188694460.0, w0=2.774801373295289e-09, w1=0.2000000208818141\n",
      "Gradient Descent(57/99): loss =8881009236468107.0, w0=2.822146056105246e-09, w1=0.2000000212552471\n",
      "Gradient Descent(58/99): loss =8880073391058545.0, w0=2.8694893674064183e-09, w1=0.20000002162866187\n",
      "Gradient Descent(59/99): loss =8879137652452356.0, w0=2.916831306935324e-09, w1=0.20000002200205844\n",
      "Gradient Descent(60/99): loss =8878202020636084.0, w0=2.9641718744301854e-09, w1=0.20000002237543676\n",
      "Gradient Descent(61/99): loss =8877266495596301.0, w0=3.011511069630619e-09, w1=0.20000002274879686\n",
      "Gradient Descent(62/99): loss =8876331077319592.0, w0=3.058848892277364e-09, w1=0.20000002312213871\n",
      "Gradient Descent(63/99): loss =8875395765792550.0, w0=3.1061853421120472e-09, w1=0.20000002349546234\n",
      "Gradient Descent(64/99): loss =8874460561001731.0, w0=3.1535204188769765e-09, w1=0.20000002386876772\n",
      "Gradient Descent(65/99): loss =8873525462933733.0, w0=3.200854122314963e-09, w1=0.20000002424205487\n",
      "Gradient Descent(66/99): loss =8872590471575149.0, w0=3.2481864521691648e-09, w1=0.2000000246153238\n",
      "Gradient Descent(67/99): loss =8871655586912595.0, w0=3.2955174081829556e-09, w1=0.20000002498857447\n",
      "Gradient Descent(68/99): loss =8870720808932641.0, w0=3.3428469900998075e-09, w1=0.20000002536180694\n",
      "Gradient Descent(69/99): loss =8869786137621918.0, w0=3.3901751976631913e-09, w1=0.20000002573502118\n",
      "Gradient Descent(70/99): loss =8868851572967026.0, w0=3.4375020306164922e-09, w1=0.20000002610821718\n",
      "Gradient Descent(71/99): loss =8867917114954587.0, w0=3.484827488702936e-09, w1=0.20000002648139495\n",
      "Gradient Descent(72/99): loss =8866982763571200.0, w0=3.532151571665528e-09, w1=0.20000002685455448\n",
      "Gradient Descent(73/99): loss =8866048518803509.0, w0=3.5794742792469975e-09, w1=0.20000002722769578\n",
      "Gradient Descent(74/99): loss =8865114380638125.0, w0=3.626795611189757e-09, w1=0.20000002760081884\n",
      "Gradient Descent(75/99): loss =8864180349061690.0, w0=3.6741155672358615e-09, w1=0.20000002797392366\n",
      "Gradient Descent(76/99): loss =8863246424060841.0, w0=3.721434147126979e-09, w1=0.20000002834701025\n",
      "Gradient Descent(77/99): loss =8862312605622200.0, w0=3.7687513506043645e-09, w1=0.2000000287200786\n",
      "Gradient Descent(78/99): loss =8861378893732417.0, w0=3.81606717740884e-09, w1=0.20000002909312875\n",
      "Gradient Descent(79/99): loss =8860445288378139.0, w0=3.863381627280776e-09, w1=0.20000002946616066\n",
      "Gradient Descent(80/99): loss =8859511789546026.0, w0=3.91069469996008e-09, w1=0.20000002983917434\n",
      "Gradient Descent(81/99): loss =8858578397222704.0, w0=3.958006395186187e-09, w1=0.20000003021216978\n",
      "Gradient Descent(82/99): loss =8857645111394864.0, w0=4.005316712698051e-09, w1=0.200000030585147\n",
      "Gradient Descent(83/99): loss =8856711932049139.0, w0=4.052625652234141e-09, w1=0.200000030958106\n",
      "Gradient Descent(84/99): loss =8855778859172214.0, w0=4.099933213532438e-09, w1=0.20000003133104677\n",
      "Gradient Descent(85/99): loss =8854845892750753.0, w0=4.147239396330434e-09, w1=0.20000003170396932\n",
      "Gradient Descent(86/99): loss =8853913032771399.0, w0=4.194544200365134e-09, w1=0.20000003207687364\n",
      "Gradient Descent(87/99): loss =8852980279220873.0, w0=4.241847625373056e-09, w1=0.20000003244975975\n",
      "Gradient Descent(88/99): loss =8852047632085820.0, w0=4.2891496710902354e-09, w1=0.20000003282262763\n",
      "Gradient Descent(89/99): loss =8851115091352927.0, w0=4.33645033725223e-09, w1=0.2000000331954773\n",
      "Gradient Descent(90/99): loss =8850182657008900.0, w0=4.383749623594123e-09, w1=0.20000003356830873\n",
      "Gradient Descent(91/99): loss =8849250329040413.0, w0=4.431047529850528e-09, w1=0.20000003394112195\n",
      "Gradient Descent(92/99): loss =8848318107434156.0, w0=4.4783440557556e-09, w1=0.20000003431391694\n",
      "Gradient Descent(93/99): loss =8847385992176840.0, w0=4.525639201043036e-09, w1=0.20000003468669372\n",
      "Gradient Descent(94/99): loss =8846453983255149.0, w0=4.572932965446087e-09, w1=0.2000000350594523\n",
      "Gradient Descent(95/99): loss =8845522080655798.0, w0=4.620225348697563e-09, w1=0.20000003543219266\n",
      "Gradient Descent(96/99): loss =8844590284365487.0, w0=4.667516350529842e-09, w1=0.2000000358049148\n",
      "Gradient Descent(97/99): loss =8843658594370919.0, w0=4.714805970674878e-09, w1=0.20000003617761872\n",
      "Gradient Descent(98/99): loss =8842727010658811.0, w0=4.762094208864209e-09, w1=0.20000003655030443\n",
      "Gradient Descent(99/99): loss =8841795533215896.0, w0=4.809381064828966e-09, w1=0.20000003692297194\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =3.042027210881237e+51, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =2.4500501459502303e+28, w0=-0.19999999999951792, w1=0.20000000000084778\n",
      "Gradient Descent(3/99): loss =2.448987613217134e+28, w0=-0.19999999999904094, w1=0.200000000001695\n",
      "Gradient Descent(4/99): loss =2.4481828368392906e+28, w0=-0.1999999999985636, w1=0.20000000000254167\n",
      "Gradient Descent(5/99): loss =2.4473856808690888e+28, w0=-0.19999999999808657, w1=0.20000000000338788\n",
      "Gradient Descent(6/99): loss =2.4465904980275463e+28, w0=-0.19999999999760976, w1=0.20000000000423362\n",
      "Gradient Descent(7/99): loss =2.445797144613518e+28, w0=-0.19999999999713314, w1=0.20000000000507892\n",
      "Gradient Descent(8/99): loss =2.445005608610638e+28, w0=-0.19999999999665674, w1=0.20000000000592374\n",
      "Gradient Descent(9/99): loss =2.4442158814083287e+28, w0=-0.19999999999618054, w1=0.20000000000676812\n",
      "Gradient Descent(10/99): loss =2.443427954688985e+28, w0=-0.19999999999570456, w1=0.2000000000076121\n",
      "Gradient Descent(11/99): loss =2.442641820326856e+28, w0=-0.19999999999522877, w1=0.2000000000084556\n",
      "Gradient Descent(12/99): loss =2.441857470363366e+28, w0=-0.1999999999947532, w1=0.2000000000092987\n",
      "Gradient Descent(13/99): loss =2.4410748969892203e+28, w0=-0.19999999999427784, w1=0.2000000000101414\n",
      "Gradient Descent(14/99): loss =2.4402940925303113e+28, w0=-0.1999999999938027, w1=0.20000000001098367\n",
      "Gradient Descent(15/99): loss =2.4395150494365252e+28, w0=-0.19999999999332774, w1=0.20000000001182552\n",
      "Gradient Descent(16/99): loss =2.4387377602727534e+28, w0=-0.19999999999285298, w1=0.20000000001266696\n",
      "Gradient Descent(17/99): loss =2.4379622177116365e+28, w0=-0.1999999999923784, w1=0.20000000001350798\n",
      "Gradient Descent(18/99): loss =2.437188414527653e+28, w0=-0.19999999999190407, w1=0.20000000001434862\n",
      "Gradient Descent(19/99): loss =2.4364163435921945e+28, w0=-0.19999999999142992, w1=0.20000000001518883\n",
      "Gradient Descent(20/99): loss =2.4356459978694715e+28, w0=-0.19999999999095597, w1=0.20000000001602866\n",
      "Gradient Descent(21/99): loss =2.434877370413008e+28, w0=-0.1999999999904822, w1=0.2000000000168681\n",
      "Gradient Descent(22/99): loss =2.4341104543626364e+28, w0=-0.19999999999000864, w1=0.20000000001770712\n",
      "Gradient Descent(23/99): loss =2.4333452429418515e+28, w0=-0.19999999998953527, w1=0.20000000001854576\n",
      "Gradient Descent(24/99): loss =2.4325817294554728e+28, w0=-0.1999999999890621, w1=0.200000000019384\n",
      "Gradient Descent(25/99): loss =2.4318199072875215e+28, w0=-0.1999999999885891, w1=0.20000000002022186\n",
      "Gradient Descent(26/99): loss =2.431059769899309e+28, w0=-0.19999999998811632, w1=0.2000000000210593\n",
      "Gradient Descent(27/99): loss =2.430301310827636e+28, w0=-0.19999999998764373, w1=0.20000000002189636\n",
      "Gradient Descent(28/99): loss =2.4295445236831453e+28, w0=-0.19999999998717133, w1=0.20000000002273302\n",
      "Gradient Descent(29/99): loss =2.428789402148761e+28, w0=-0.1999999999866991, w1=0.2000000000235693\n",
      "Gradient Descent(30/99): loss =2.4280359399781898e+28, w0=-0.19999999998622706, w1=0.20000000002440518\n",
      "Gradient Descent(31/99): loss =2.427284130994522e+28, w0=-0.1999999999857552, w1=0.20000000002524068\n",
      "Gradient Descent(32/99): loss =2.426533969088865e+28, w0=-0.19999999998528356, w1=0.2000000000260758\n",
      "Gradient Descent(33/99): loss =2.4257854482190404e+28, w0=-0.19999999998481208, w1=0.2000000000269105\n",
      "Gradient Descent(34/99): loss =2.4250385624083033e+28, w0=-0.1999999999843408, w1=0.20000000002774485\n",
      "Gradient Descent(35/99): loss =2.4242933057441385e+28, w0=-0.1999999999838697, w1=0.2000000000285788\n",
      "Gradient Descent(36/99): loss =2.423549672377051e+28, w0=-0.19999999998339876, w1=0.20000000002941234\n",
      "Gradient Descent(37/99): loss =2.4228076565194096e+28, w0=-0.19999999998292803, w1=0.20000000003024554\n",
      "Gradient Descent(38/99): loss =2.4220672524443155e+28, w0=-0.19999999998245746, w1=0.20000000003107835\n",
      "Gradient Descent(39/99): loss =2.421328454484496e+28, w0=-0.1999999999819871, w1=0.20000000003191076\n",
      "Gradient Descent(40/99): loss =2.4205912570312234e+28, w0=-0.19999999998151688, w1=0.2000000000327428\n",
      "Gradient Descent(41/99): loss =2.4198556545332664e+28, w0=-0.19999999998104687, w1=0.20000000003357443\n",
      "Gradient Descent(42/99): loss =2.4191216414958444e+28, w0=-0.19999999998057702, w1=0.20000000003440568\n",
      "Gradient Descent(43/99): loss =2.41838921247963e+28, w0=-0.19999999998010734, w1=0.20000000003523655\n",
      "Gradient Descent(44/99): loss =2.4176583620997554e+28, w0=-0.19999999997963785, w1=0.20000000003606702\n",
      "Gradient Descent(45/99): loss =2.4169290850248374e+28, w0=-0.19999999997916854, w1=0.20000000003689714\n",
      "Gradient Descent(46/99): loss =2.4162013759760376e+28, w0=-0.19999999997869938, w1=0.20000000003772686\n",
      "Gradient Descent(47/99): loss =2.4154752297261244e+28, w0=-0.19999999997823042, w1=0.2000000000385562\n",
      "Gradient Descent(48/99): loss =2.4147506410985643e+28, w0=-0.19999999997776163, w1=0.20000000003938515\n",
      "Gradient Descent(49/99): loss =2.414027604966636e+28, w0=-0.199999999977293, w1=0.2000000000402137\n",
      "Gradient Descent(50/99): loss =2.4133061162525458e+28, w0=-0.19999999997682455, w1=0.2000000000410419\n",
      "Gradient Descent(51/99): loss =2.4125861699265734e+28, w0=-0.19999999997635626, w1=0.2000000000418697\n",
      "Gradient Descent(52/99): loss =2.41186776100623e+28, w0=-0.19999999997588813, w1=0.20000000004269713\n",
      "Gradient Descent(53/99): loss =2.4111508845554387e+28, w0=-0.19999999997542017, w1=0.20000000004352417\n",
      "Gradient Descent(54/99): loss =2.4104355356837233e+28, w0=-0.1999999999749524, w1=0.2000000000443508\n",
      "Gradient Descent(55/99): loss =2.4097217095454132e+28, w0=-0.1999999999744848, w1=0.2000000000451771\n",
      "Gradient Descent(56/99): loss =2.4090094013388806e+28, w0=-0.19999999997401738, w1=0.200000000046003\n",
      "Gradient Descent(57/99): loss =2.4082986063057535e+28, w0=-0.1999999999735501, w1=0.2000000000468285\n",
      "Gradient Descent(58/99): loss =2.4075893197302006e+28, w0=-0.199999999973083, w1=0.20000000004765361\n",
      "Gradient Descent(59/99): loss =2.4068815369381787e+28, w0=-0.19999999997261603, w1=0.20000000004847837\n",
      "Gradient Descent(60/99): loss =2.40617525329672e+28, w0=-0.19999999997214923, w1=0.20000000004930274\n",
      "Gradient Descent(61/99): loss =2.4054704642132343e+28, w0=-0.1999999999716826, w1=0.20000000005012672\n",
      "Gradient Descent(62/99): loss =2.4047671651348125e+28, w0=-0.19999999997121615, w1=0.20000000005095034\n",
      "Gradient Descent(63/99): loss =2.4040653515475556e+28, w0=-0.19999999997074985, w1=0.20000000005177357\n",
      "Gradient Descent(64/99): loss =2.4033650189759064e+28, w0=-0.19999999997028373, w1=0.2000000000525964\n",
      "Gradient Descent(65/99): loss =2.402666162982012e+28, w0=-0.19999999996981774, w1=0.20000000005341886\n",
      "Gradient Descent(66/99): loss =2.401968779165067e+28, w0=-0.19999999996935192, w1=0.20000000005424096\n",
      "Gradient Descent(67/99): loss =2.4012728631607075e+28, w0=-0.19999999996888626, w1=0.20000000005506266\n",
      "Gradient Descent(68/99): loss =2.4005784106403817e+28, w0=-0.19999999996842077, w1=0.20000000005588398\n",
      "Gradient Descent(69/99): loss =2.399885417310765e+28, w0=-0.19999999996795542, w1=0.20000000005670493\n",
      "Gradient Descent(70/99): loss =2.399193878913166e+28, w0=-0.19999999996749024, w1=0.2000000000575255\n",
      "Gradient Descent(71/99): loss =2.398503791222932e+28, w0=-0.19999999996702522, w1=0.20000000005834567\n",
      "Gradient Descent(72/99): loss =2.3978151500489107e+28, w0=-0.19999999996656034, w1=0.2000000000591655\n",
      "Gradient Descent(73/99): loss =2.3971279512328766e+28, w0=-0.19999999996609563, w1=0.20000000005998492\n",
      "Gradient Descent(74/99): loss =2.3964421906489825e+28, w0=-0.19999999996563106, w1=0.20000000006080398\n",
      "Gradient Descent(75/99): loss =2.3957578642032412e+28, w0=-0.19999999996516665, w1=0.20000000006162266\n",
      "Gradient Descent(76/99): loss =2.3950749678329947e+28, w0=-0.19999999996470239, w1=0.20000000006244095\n",
      "Gradient Descent(77/99): loss =2.3943934975063874e+28, w0=-0.19999999996423828, w1=0.20000000006325888\n",
      "Gradient Descent(78/99): loss =2.393713449221887e+28, w0=-0.19999999996377432, w1=0.20000000006407642\n",
      "Gradient Descent(79/99): loss =2.3930348190077644e+28, w0=-0.19999999996331053, w1=0.2000000000648936\n",
      "Gradient Descent(80/99): loss =2.3923576029216334e+28, w0=-0.19999999996284687, w1=0.2000000000657104\n",
      "Gradient Descent(81/99): loss =2.3916817970499494e+28, w0=-0.19999999996238338, w1=0.2000000000665268\n",
      "Gradient Descent(82/99): loss =2.391007397507573e+28, w0=-0.19999999996192003, w1=0.20000000006734284\n",
      "Gradient Descent(83/99): loss =2.3903344004372835e+28, w0=-0.19999999996145681, w1=0.2000000000681585\n",
      "Gradient Descent(84/99): loss =2.3896628020093537e+28, w0=-0.19999999996099377, w1=0.20000000006897378\n",
      "Gradient Descent(85/99): loss =2.38899259842109e+28, w0=-0.19999999996053086, w1=0.20000000006978869\n",
      "Gradient Descent(86/99): loss =2.3883237858964246e+28, w0=-0.1999999999600681, w1=0.20000000007060323\n",
      "Gradient Descent(87/99): loss =2.3876563606854675e+28, w0=-0.1999999999596055, w1=0.20000000007141738\n",
      "Gradient Descent(88/99): loss =2.386990319064109e+28, w0=-0.19999999995914303, w1=0.20000000007223118\n",
      "Gradient Descent(89/99): loss =2.3863256573336094e+28, w0=-0.1999999999586807, w1=0.20000000007304458\n",
      "Gradient Descent(90/99): loss =2.3856623718201874e+28, w0=-0.19999999995821852, w1=0.20000000007385763\n",
      "Gradient Descent(91/99): loss =2.385000458874652e+28, w0=-0.1999999999577565, w1=0.20000000007467028\n",
      "Gradient Descent(92/99): loss =2.3843399148719886e+28, w0=-0.19999999995729462, w1=0.20000000007548258\n",
      "Gradient Descent(93/99): loss =2.383680736211017e+28, w0=-0.19999999995683287, w1=0.20000000007629448\n",
      "Gradient Descent(94/99): loss =2.3830229193139894e+28, w0=-0.19999999995637127, w1=0.20000000007710603\n",
      "Gradient Descent(95/99): loss =2.3823664606262467e+28, w0=-0.1999999999559098, w1=0.20000000007791718\n",
      "Gradient Descent(96/99): loss =2.381711356615861e+28, w0=-0.19999999995544848, w1=0.20000000007872798\n",
      "Gradient Descent(97/99): loss =2.3810576037732778e+28, w0=-0.1999999999549873, w1=0.2000000000795384\n",
      "Gradient Descent(98/99): loss =2.380405198610984e+28, w0=-0.19999999995452625, w1=0.20000000008034843\n",
      "Gradient Descent(99/99): loss =2.3797541376631695e+28, w0=-0.19999999995406534, w1=0.2000000000811581\n",
      "Optimizing degree 15/15, model: least_squares_GD, arguments: {'max_iters': 100}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =5.008942965305851e+55, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =7.83753196239194e+30, w0=-0.19999999999993537, w1=0.2000000000001068\n",
      "Gradient Descent(3/99): loss =7.836123549335014e+30, w0=-0.19999999999987084, w1=0.20000000000021353\n",
      "Gradient Descent(4/99): loss =7.834723926645096e+30, w0=-0.1999999999998063, w1=0.20000000000032025\n",
      "Gradient Descent(5/99): loss =7.833326135007044e+30, w0=-0.1999999999997418, w1=0.20000000000042695\n",
      "Gradient Descent(6/99): loss =7.831930161780607e+30, w0=-0.1999999999996773, w1=0.20000000000053358\n",
      "Gradient Descent(7/99): loss =7.830535998768164e+30, w0=-0.1999999999996128, w1=0.2000000000006402\n",
      "Gradient Descent(8/99): loss =7.829143638222366e+30, w0=-0.19999999999954832, w1=0.20000000000074677\n",
      "Gradient Descent(9/99): loss =7.827753072789565e+30, w0=-0.19999999999948384, w1=0.20000000000085333\n",
      "Gradient Descent(10/99): loss =7.826364295469016e+30, w0=-0.1999999999994194, w1=0.20000000000095983\n",
      "Gradient Descent(11/99): loss =7.824977299578664e+30, w0=-0.19999999999935494, w1=0.2000000000010663\n",
      "Gradient Descent(12/99): loss =7.82359207872626e+30, w0=-0.19999999999929052, w1=0.20000000000117274\n",
      "Gradient Descent(13/99): loss =7.822208626784609e+30, w0=-0.1999999999992261, w1=0.20000000000127915\n",
      "Gradient Descent(14/99): loss =7.820826937870321e+30, w0=-0.19999999999916168, w1=0.20000000000138551\n",
      "Gradient Descent(15/99): loss =7.819447006325264e+30, w0=-0.1999999999990973, w1=0.20000000000149185\n",
      "Gradient Descent(16/99): loss =7.818068826700247e+30, w0=-0.1999999999990329, w1=0.20000000000159815\n",
      "Gradient Descent(17/99): loss =7.816692393740543e+30, w0=-0.19999999999896853, w1=0.20000000000170443\n",
      "Gradient Descent(18/99): loss =7.815317702373008e+30, w0=-0.19999999999890417, w1=0.20000000000181067\n",
      "Gradient Descent(19/99): loss =7.8139447476944e+30, w0=-0.1999999999988398, w1=0.20000000000191687\n",
      "Gradient Descent(20/99): loss =7.812573524960808e+30, w0=-0.19999999999877546, w1=0.20000000000202303\n",
      "Gradient Descent(21/99): loss =7.811204029578022e+30, w0=-0.19999999999871113, w1=0.20000000000212917\n",
      "Gradient Descent(22/99): loss =7.809836257092625e+30, w0=-0.19999999999864682, w1=0.20000000000223528\n",
      "Gradient Descent(23/99): loss =7.808470203183892e+30, w0=-0.1999999999985825, w1=0.20000000000234136\n",
      "Gradient Descent(24/99): loss =7.807105863656159e+30, w0=-0.1999999999985182, w1=0.20000000000244741\n",
      "Gradient Descent(25/99): loss =7.805743234431818e+30, w0=-0.19999999999845391, w1=0.2000000000025534\n",
      "Gradient Descent(26/99): loss =7.804382311544755e+30, w0=-0.19999999999838963, w1=0.20000000000265938\n",
      "Gradient Descent(27/99): loss =7.803023091134211e+30, w0=-0.19999999999832535, w1=0.20000000000276533\n",
      "Gradient Descent(28/99): loss =7.801665569439086e+30, w0=-0.1999999999982611, w1=0.20000000000287124\n",
      "Gradient Descent(29/99): loss =7.800309742792506e+30, w0=-0.19999999999819684, w1=0.20000000000297713\n",
      "Gradient Descent(30/99): loss =7.798955607616776e+30, w0=-0.19999999999813262, w1=0.200000000003083\n",
      "Gradient Descent(31/99): loss =7.797603160418656e+30, w0=-0.1999999999980684, w1=0.2000000000031888\n",
      "Gradient Descent(32/99): loss =7.796252397784844e+30, w0=-0.19999999999800416, w1=0.20000000000329457\n",
      "Gradient Descent(33/99): loss =7.79490331637776e+30, w0=-0.19999999999793996, w1=0.20000000000340032\n",
      "Gradient Descent(34/99): loss =7.793555912931574e+30, w0=-0.19999999999787577, w1=0.20000000000350604\n",
      "Gradient Descent(35/99): loss =7.792210184248447e+30, w0=-0.19999999999781157, w1=0.20000000000361173\n",
      "Gradient Descent(36/99): loss =7.790866127194965e+30, w0=-0.1999999999977474, w1=0.2000000000037174\n",
      "Gradient Descent(37/99): loss =7.789523738698806e+30, w0=-0.19999999999768323, w1=0.200000000003823\n",
      "Gradient Descent(38/99): loss =7.788183015745572e+30, w0=-0.19999999999761908, w1=0.2000000000039286\n",
      "Gradient Descent(39/99): loss =7.786843955375771e+30, w0=-0.19999999999755494, w1=0.20000000000403415\n",
      "Gradient Descent(40/99): loss =7.785506554682027e+30, w0=-0.1999999999974908, w1=0.20000000000413967\n",
      "Gradient Descent(41/99): loss =7.784170810806366e+30, w0=-0.19999999999742668, w1=0.20000000000424517\n",
      "Gradient Descent(42/99): loss =7.782836720937719e+30, w0=-0.19999999999736257, w1=0.20000000000435064\n",
      "Gradient Descent(43/99): loss =7.7815042823095e+30, w0=-0.19999999999729845, w1=0.20000000000445606\n",
      "Gradient Descent(44/99): loss =7.780173492197357e+30, w0=-0.19999999999723436, w1=0.20000000000456145\n",
      "Gradient Descent(45/99): loss =7.778844347917039e+30, w0=-0.19999999999717027, w1=0.2000000000046668\n",
      "Gradient Descent(46/99): loss =7.777516846822361e+30, w0=-0.19999999999710621, w1=0.20000000000477214\n",
      "Gradient Descent(47/99): loss =7.776190986303274e+30, w0=-0.19999999999704215, w1=0.20000000000487744\n",
      "Gradient Descent(48/99): loss =7.774866763784081e+30, w0=-0.1999999999969781, w1=0.20000000000498272\n",
      "Gradient Descent(49/99): loss =7.7735441767217e+30, w0=-0.19999999999691406, w1=0.20000000000508794\n",
      "Gradient Descent(50/99): loss =7.772223222604051e+30, w0=-0.19999999999685003, w1=0.20000000000519313\n",
      "Gradient Descent(51/99): loss =7.770903898948516e+30, w0=-0.199999999996786, w1=0.2000000000052983\n",
      "Gradient Descent(52/99): loss =7.769586203300485e+30, w0=-0.199999999996722, w1=0.20000000000540344\n",
      "Gradient Descent(53/99): loss =7.76827013323198e+30, w0=-0.199999999996658, w1=0.20000000000550855\n",
      "Gradient Descent(54/99): loss =7.76695568634036e+30, w0=-0.19999999999659399, w1=0.20000000000561363\n",
      "Gradient Descent(55/99): loss =7.765642860247085e+30, w0=-0.19999999999653, w1=0.2000000000057187\n",
      "Gradient Descent(56/99): loss =7.764331652596553e+30, w0=-0.19999999999646603, w1=0.20000000000582369\n",
      "Gradient Descent(57/99): loss =7.763022061054982e+30, w0=-0.19999999999640206, w1=0.20000000000592866\n",
      "Gradient Descent(58/99): loss =7.761714083309398e+30, w0=-0.1999999999963381, w1=0.2000000000060336\n",
      "Gradient Descent(59/99): loss =7.76040771706661e+30, w0=-0.19999999999627416, w1=0.20000000000613852\n",
      "Gradient Descent(60/99): loss =7.75910296005231e+30, w0=-0.1999999999962102, w1=0.2000000000062434\n",
      "Gradient Descent(61/99): loss =7.75779981001016e+30, w0=-0.1999999999961463, w1=0.20000000000634827\n",
      "Gradient Descent(62/99): loss =7.756498264700968e+30, w0=-0.19999999999608237, w1=0.20000000000645307\n",
      "Gradient Descent(63/99): loss =7.755198321901909e+30, w0=-0.19999999999601845, w1=0.20000000000655785\n",
      "Gradient Descent(64/99): loss =7.753899979405721e+30, w0=-0.19999999999595455, w1=0.2000000000066626\n",
      "Gradient Descent(65/99): loss =7.752603235020083e+30, w0=-0.19999999999589066, w1=0.20000000000676732\n",
      "Gradient Descent(66/99): loss =7.751308086566848e+30, w0=-0.1999999999958268, w1=0.20000000000687201\n",
      "Gradient Descent(67/99): loss =7.750014531881497e+30, w0=-0.19999999999576293, w1=0.20000000000697668\n",
      "Gradient Descent(68/99): loss =7.748722568812458e+30, w0=-0.19999999999569906, w1=0.20000000000708132\n",
      "Gradient Descent(69/99): loss =7.747432195220587e+30, w0=-0.19999999999563522, w1=0.2000000000071859\n",
      "Gradient Descent(70/99): loss =7.746143408978604e+30, w0=-0.1999999999955714, w1=0.20000000000729046\n",
      "Gradient Descent(71/99): loss =7.744856207970615e+30, w0=-0.19999999999550755, w1=0.20000000000739498\n",
      "Gradient Descent(72/99): loss =7.743570590091567e+30, w0=-0.19999999999544374, w1=0.20000000000749948\n",
      "Gradient Descent(73/99): loss =7.742286553246856e+30, w0=-0.19999999999537993, w1=0.20000000000760396\n",
      "Gradient Descent(74/99): loss =7.741004095351871e+30, w0=-0.19999999999531612, w1=0.2000000000077084\n",
      "Gradient Descent(75/99): loss =7.739723214331558e+30, w0=-0.19999999999525234, w1=0.2000000000078128\n",
      "Gradient Descent(76/99): loss =7.738443908120083e+30, w0=-0.19999999999518855, w1=0.20000000000791715\n",
      "Gradient Descent(77/99): loss =7.737166174660409e+30, w0=-0.19999999999512477, w1=0.20000000000802148\n",
      "Gradient Descent(78/99): loss =7.735890011903983e+30, w0=-0.19999999999506102, w1=0.2000000000081258\n",
      "Gradient Descent(79/99): loss =7.734615417810416e+30, w0=-0.19999999999499726, w1=0.20000000000823007\n",
      "Gradient Descent(80/99): loss =7.733342390347131e+30, w0=-0.1999999999949335, w1=0.20000000000833432\n",
      "Gradient Descent(81/99): loss =7.732070927489118e+30, w0=-0.19999999999486978, w1=0.20000000000843854\n",
      "Gradient Descent(82/99): loss =7.730801027218599e+30, w0=-0.19999999999480605, w1=0.2000000000085427\n",
      "Gradient Descent(83/99): loss =7.729532687524807e+30, w0=-0.19999999999474233, w1=0.20000000000864684\n",
      "Gradient Descent(84/99): loss =7.728265906403734e+30, w0=-0.19999999999467863, w1=0.20000000000875096\n",
      "Gradient Descent(85/99): loss =7.727000681857873e+30, w0=-0.19999999999461493, w1=0.20000000000885504\n",
      "Gradient Descent(86/99): loss =7.725737011895987e+30, w0=-0.19999999999455123, w1=0.2000000000089591\n",
      "Gradient Descent(87/99): loss =7.72447489453293e+30, w0=-0.19999999999448756, w1=0.20000000000906312\n",
      "Gradient Descent(88/99): loss =7.723214327789422e+30, w0=-0.1999999999944239, w1=0.20000000000916712\n",
      "Gradient Descent(89/99): loss =7.721955309691864e+30, w0=-0.19999999999436022, w1=0.20000000000927107\n",
      "Gradient Descent(90/99): loss =7.72069783827215e+30, w0=-0.19999999999429657, w1=0.20000000000937498\n",
      "Gradient Descent(91/99): loss =7.719441911567511e+30, w0=-0.19999999999423293, w1=0.20000000000947887\n",
      "Gradient Descent(92/99): loss =7.71818752762035e+30, w0=-0.1999999999941693, w1=0.20000000000958273\n",
      "Gradient Descent(93/99): loss =7.716934684478068e+30, w0=-0.19999999999410567, w1=0.20000000000968657\n",
      "Gradient Descent(94/99): loss =7.71568338019294e+30, w0=-0.19999999999404205, w1=0.20000000000979037\n",
      "Gradient Descent(95/99): loss =7.714433612821978e+30, w0=-0.19999999999397844, w1=0.20000000000989415\n",
      "Gradient Descent(96/99): loss =7.713185380426804e+30, w0=-0.19999999999391485, w1=0.20000000000999787\n",
      "Gradient Descent(97/99): loss =7.711938681073484e+30, w0=-0.19999999999385126, w1=0.20000000001010157\n",
      "Gradient Descent(98/99): loss =7.710693512832469e+30, w0=-0.19999999999378767, w1=0.20000000001020524\n",
      "Gradient Descent(99/99): loss =7.709449873778453e+30, w0=-0.19999999999372411, w1=0.20000000001030888\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =5.008942965305849e+55, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =6.596869939702126e+30, w0=-0.1999999999999591, w1=0.20000000000000023\n",
      "Gradient Descent(3/99): loss =6.542239058343514e+30, w0=-0.1999999999999208, w1=0.19999999999999518\n",
      "Gradient Descent(4/99): loss =6.502431929230421e+30, w0=-0.19999999999988355, w1=0.19999999999998588\n",
      "Gradient Descent(5/99): loss =6.472396121753653e+30, w0=-0.19999999999984724, w1=0.19999999999997287\n",
      "Gradient Descent(6/99): loss =6.449729878088031e+30, w0=-0.19999999999981175, w1=0.19999999999995666\n",
      "Gradient Descent(7/99): loss =6.43262245159918e+30, w0=-0.19999999999977697, w1=0.19999999999993767\n",
      "Gradient Descent(8/99): loss =6.419708386563619e+30, w0=-0.1999999999997428, w1=0.19999999999991627\n",
      "Gradient Descent(9/99): loss =6.409957850760992e+30, w0=-0.19999999999970916, w1=0.19999999999989276\n",
      "Gradient Descent(10/99): loss =6.402594047589284e+30, w0=-0.19999999999967596, w1=0.19999999999986745\n",
      "Gradient Descent(11/99): loss =6.397030990861113e+30, w0=-0.19999999999964319, w1=0.19999999999984056\n",
      "Gradient Descent(12/99): loss =6.392826604481849e+30, w0=-0.19999999999961074, w1=0.19999999999981227\n",
      "Gradient Descent(13/99): loss =6.389647362979095e+30, w0=-0.1999999999995786, w1=0.1999999999997828\n",
      "Gradient Descent(14/99): loss =6.387241627422708e+30, w0=-0.1999999999995467, w1=0.1999999999997523\n",
      "Gradient Descent(15/99): loss =6.385419535239236e+30, w0=-0.19999999999951507, w1=0.1999999999997209\n",
      "Gradient Descent(16/99): loss =6.384037831198032e+30, w0=-0.19999999999948362, w1=0.19999999999968873\n",
      "Gradient Descent(17/99): loss =6.382988424468897e+30, w0=-0.19999999999945234, w1=0.19999999999965587\n",
      "Gradient Descent(18/99): loss =6.382189755903723e+30, w0=-0.1999999999994212, w1=0.19999999999962242\n",
      "Gradient Descent(19/99): loss =6.381580285057288e+30, w0=-0.1999999999993902, w1=0.19999999999958845\n",
      "Gradient Descent(20/99): loss =6.38111357626112e+30, w0=-0.1999999999993593, w1=0.19999999999955403\n",
      "Gradient Descent(21/99): loss =6.380754591046185e+30, w0=-0.19999999999932852, w1=0.19999999999951923\n",
      "Gradient Descent(22/99): loss =6.380476890698925e+30, w0=-0.19999999999929782, w1=0.1999999999994841\n",
      "Gradient Descent(23/99): loss =6.380260525496746e+30, w0=-0.19999999999926718, w1=0.19999999999944867\n",
      "Gradient Descent(24/99): loss =6.380090442045992e+30, w0=-0.19999999999923662, w1=0.199999999999413\n",
      "Gradient Descent(25/99): loss =6.379955281538972e+30, w0=-0.19999999999920612, w1=0.19999999999937712\n",
      "Gradient Descent(26/99): loss =6.379846472972484e+30, w0=-0.19999999999917567, w1=0.19999999999934104\n",
      "Gradient Descent(27/99): loss =6.37975754892733e+30, w0=-0.19999999999914525, w1=0.1999999999993048\n",
      "Gradient Descent(28/99): loss =6.379683629281238e+30, w0=-0.19999999999911486, w1=0.19999999999926837\n",
      "Gradient Descent(29/99): loss =6.379621031636841e+30, w0=-0.19999999999908452, w1=0.19999999999923185\n",
      "Gradient Descent(30/99): loss =6.379566977363638e+30, w0=-0.1999999999990542, w1=0.1999999999991952\n",
      "Gradient Descent(31/99): loss =6.379519369786753e+30, w0=-0.1999999999990239, w1=0.19999999999915846\n",
      "Gradient Descent(32/99): loss =6.379476626815001e+30, w0=-0.19999999999899362, w1=0.19999999999912163\n",
      "Gradient Descent(33/99): loss =6.379437554647171e+30, w0=-0.19999999999896337, w1=0.19999999999908474\n",
      "Gradient Descent(34/99): loss =6.379401252474539e+30, w0=-0.19999999999893311, w1=0.1999999999990478\n",
      "Gradient Descent(35/99): loss =6.379367040572129e+30, w0=-0.1999999999989029, w1=0.1999999999990108\n",
      "Gradient Descent(36/99): loss =6.379334406038502e+30, w0=-0.19999999999887266, w1=0.19999999999897375\n",
      "Gradient Descent(37/99): loss =6.379302961852594e+30, w0=-0.19999999999884246, w1=0.19999999999893664\n",
      "Gradient Descent(38/99): loss =6.379272415979091e+30, w0=-0.19999999999881227, w1=0.1999999999988995\n",
      "Gradient Descent(39/99): loss =6.379242548056263e+30, w0=-0.19999999999878207, w1=0.19999999999886234\n",
      "Gradient Descent(40/99): loss =6.379213191805301e+30, w0=-0.1999999999987519, w1=0.19999999999882515\n",
      "Gradient Descent(41/99): loss =6.379184221756819e+30, w0=-0.19999999999872173, w1=0.19999999999878793\n",
      "Gradient Descent(42/99): loss =6.379155543235099e+30, w0=-0.19999999999869156, w1=0.19999999999875068\n",
      "Gradient Descent(43/99): loss =6.379127084800469e+30, w0=-0.1999999999986614, w1=0.19999999999871343\n",
      "Gradient Descent(44/99): loss =6.379098792546531e+30, w0=-0.19999999999863122, w1=0.19999999999867615\n",
      "Gradient Descent(45/99): loss =6.379070625797011e+30, w0=-0.19999999999860105, w1=0.19999999999863885\n",
      "Gradient Descent(46/99): loss =6.379042553858717e+30, w0=-0.1999999999985709, w1=0.19999999999860155\n",
      "Gradient Descent(47/99): loss =6.379014553571399e+30, w0=-0.19999999999854076, w1=0.19999999999856424\n",
      "Gradient Descent(48/99): loss =6.37898660745899e+30, w0=-0.19999999999851062, w1=0.1999999999985269\n",
      "Gradient Descent(49/99): loss =6.378958702334508e+30, w0=-0.19999999999848048, w1=0.19999999999848958\n",
      "Gradient Descent(50/99): loss =6.378930828247448e+30, w0=-0.19999999999845033, w1=0.19999999999845225\n",
      "Gradient Descent(51/99): loss =6.378902977689403e+30, w0=-0.1999999999984202, w1=0.1999999999984149\n",
      "Gradient Descent(52/99): loss =6.378875144994758e+30, w0=-0.19999999999839005, w1=0.19999999999837753\n",
      "Gradient Descent(53/99): loss =6.378847325888368e+30, w0=-0.1999999999983599, w1=0.19999999999834017\n",
      "Gradient Descent(54/99): loss =6.378819517144349e+30, w0=-0.19999999999832976, w1=0.1999999999983028\n",
      "Gradient Descent(55/99): loss =6.378791716328527e+30, w0=-0.19999999999829962, w1=0.19999999999826545\n",
      "Gradient Descent(56/99): loss =6.378763921604134e+30, w0=-0.19999999999826948, w1=0.19999999999822807\n",
      "Gradient Descent(57/99): loss =6.378736131585197e+30, w0=-0.19999999999823934, w1=0.19999999999819068\n",
      "Gradient Descent(58/99): loss =6.378708345225898e+30, w0=-0.1999999999982092, w1=0.1999999999981533\n",
      "Gradient Descent(59/99): loss =6.378680561737085e+30, w0=-0.19999999999817905, w1=0.1999999999981159\n",
      "Gradient Descent(60/99): loss =6.378652780523285e+30, w0=-0.1999999999981489, w1=0.19999999999807852\n",
      "Gradient Descent(61/99): loss =6.37862500113518e+30, w0=-0.19999999999811877, w1=0.19999999999804113\n",
      "Gradient Descent(62/99): loss =6.378597223233706e+30, w0=-0.19999999999808862, w1=0.19999999999800375\n",
      "Gradient Descent(63/99): loss =6.378569446563018e+30, w0=-0.19999999999805848, w1=0.19999999999796636\n",
      "Gradient Descent(64/99): loss =6.378541670930063e+30, w0=-0.19999999999802834, w1=0.19999999999792897\n",
      "Gradient Descent(65/99): loss =6.378513896189172e+30, w0=-0.1999999999979982, w1=0.1999999999978916\n",
      "Gradient Descent(66/99): loss =6.378486122230413e+30, w0=-0.19999999999796805, w1=0.19999999999785417\n",
      "Gradient Descent(67/99): loss =6.378458348970834e+30, w0=-0.1999999999979379, w1=0.19999999999781676\n",
      "Gradient Descent(68/99): loss =6.37843057634785e+30, w0=-0.19999999999790777, w1=0.19999999999777934\n",
      "Gradient Descent(69/99): loss =6.378402804314215e+30, w0=-0.19999999999787763, w1=0.19999999999774193\n",
      "Gradient Descent(70/99): loss =6.378375032834294e+30, w0=-0.19999999999784748, w1=0.19999999999770451\n",
      "Gradient Descent(71/99): loss =6.378347261881176e+30, w0=-0.19999999999781734, w1=0.1999999999976671\n",
      "Gradient Descent(72/99): loss =6.378319491434578e+30, w0=-0.1999999999977872, w1=0.19999999999762968\n",
      "Gradient Descent(73/99): loss =6.378291721479167e+30, w0=-0.19999999999775708, w1=0.19999999999759227\n",
      "Gradient Descent(74/99): loss =6.378263952003387e+30, w0=-0.19999999999772697, w1=0.19999999999755486\n",
      "Gradient Descent(75/99): loss =6.378236182998508e+30, w0=-0.19999999999769685, w1=0.19999999999751744\n",
      "Gradient Descent(76/99): loss =6.378208414457942e+30, w0=-0.19999999999766674, w1=0.19999999999748003\n",
      "Gradient Descent(77/99): loss =6.378180646376716e+30, w0=-0.19999999999763662, w1=0.1999999999974426\n",
      "Gradient Descent(78/99): loss =6.378152878751076e+30, w0=-0.1999999999976065, w1=0.1999999999974052\n",
      "Gradient Descent(79/99): loss =6.378125111578176e+30, w0=-0.1999999999975764, w1=0.19999999999736778\n",
      "Gradient Descent(80/99): loss =6.378097344855879e+30, w0=-0.19999999999754628, w1=0.19999999999733037\n",
      "Gradient Descent(81/99): loss =6.378069578582565e+30, w0=-0.19999999999751616, w1=0.19999999999729295\n",
      "Gradient Descent(82/99): loss =6.378041812757017e+30, w0=-0.19999999999748605, w1=0.19999999999725554\n",
      "Gradient Descent(83/99): loss =6.378014047378291e+30, w0=-0.19999999999745594, w1=0.19999999999721813\n",
      "Gradient Descent(84/99): loss =6.3779862824457e+30, w0=-0.19999999999742582, w1=0.19999999999718068\n",
      "Gradient Descent(85/99): loss =6.377958517958703e+30, w0=-0.1999999999973957, w1=0.19999999999714324\n",
      "Gradient Descent(86/99): loss =6.377930753916896e+30, w0=-0.1999999999973656, w1=0.1999999999971058\n",
      "Gradient Descent(87/99): loss =6.377902990319971e+30, w0=-0.19999999999733548, w1=0.19999999999706836\n",
      "Gradient Descent(88/99): loss =6.377875227167701e+30, w0=-0.19999999999730536, w1=0.19999999999703091\n",
      "Gradient Descent(89/99): loss =6.377847464459895e+30, w0=-0.19999999999727525, w1=0.19999999999699347\n",
      "Gradient Descent(90/99): loss =6.377819702196422e+30, w0=-0.19999999999724513, w1=0.19999999999695603\n",
      "Gradient Descent(91/99): loss =6.377791940377164e+30, w0=-0.19999999999721502, w1=0.1999999999969186\n",
      "Gradient Descent(92/99): loss =6.377764179002054e+30, w0=-0.1999999999971849, w1=0.19999999999688114\n",
      "Gradient Descent(93/99): loss =6.37773641807101e+30, w0=-0.1999999999971548, w1=0.1999999999968437\n",
      "Gradient Descent(94/99): loss =6.377708657583993e+30, w0=-0.19999999999712467, w1=0.19999999999680626\n",
      "Gradient Descent(95/99): loss =6.377680897540951e+30, w0=-0.19999999999709456, w1=0.19999999999676882\n",
      "Gradient Descent(96/99): loss =6.377653137941858e+30, w0=-0.19999999999706444, w1=0.19999999999673138\n",
      "Gradient Descent(97/99): loss =6.377625378786677e+30, w0=-0.19999999999703433, w1=0.19999999999669393\n",
      "Gradient Descent(98/99): loss =6.377597620075386e+30, w0=-0.1999999999970042, w1=0.1999999999966565\n",
      "Gradient Descent(99/99): loss =6.377569861807965e+30, w0=-0.1999999999969741, w1=0.19999999999661905\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =1.5829339768933277e+38, w0=0.0, w1=0.2\n",
      "Gradient Descent(2/99): loss =2.57465459957264e+17, w0=6.128868723692745e-12, w1=0.20000000002140905\n",
      "Gradient Descent(3/99): loss =1.3708426317435894e+16, w0=7.145332223071124e-12, w1=0.20000000003934684\n",
      "Gradient Descent(4/99): loss =1.1710075553665412e+16, w0=8.60639524575449e-12, w1=0.20000000005681995\n",
      "Gradient Descent(5/99): loss =1.167410636418594e+16, w0=9.99342581884342e-12, w1=0.2000000000737928\n",
      "Gradient Descent(6/99): loss =1.1663941428879164e+16, w0=1.1364948868733426e-11, w1=0.20000000009043847\n",
      "Gradient Descent(7/99): loss =1.1658663695074288e+16, w0=1.2722586689434664e-11, w1=0.20000000010685773\n",
      "Gradient Descent(8/99): loss =1.1655634690161584e+16, w0=1.407099384538426e-11, w1=0.20000000012312144\n",
      "Gradient Descent(9/99): loss =1.165366722399996e+16, w0=1.5413046659452908e-11, w1=0.20000000013927818\n",
      "Gradient Descent(10/99): loss =1.165220113035148e+16, w0=1.675075087247199e-11, w1=0.2000000001553613\n",
      "Gradient Descent(11/99): loss =1.1650971877438148e+16, w0=1.8085482118321445e-11, w1=0.20000000017139372\n",
      "Gradient Descent(12/99): loss =1.1649854541479288e+16, w0=1.9418185908259242e-11, w1=0.2000000001873912\n",
      "Gradient Descent(13/99): loss =1.164879012782461e+16, w0=2.074951191896648e-11, w1=0.20000000020336453\n",
      "Gradient Descent(14/99): loss =1.1647750776519598e+16, w0=2.2079906558373458e-11, w1=0.20000000021932116\n",
      "Gradient Descent(15/99): loss =1.1646723330812416e+16, w0=2.3409676550754297e-11, w1=0.20000000023526618\n",
      "Gradient Descent(16/99): loss =1.1645701577465574e+16, w0=2.4739032634222938e-11, w1=0.2000000002512031\n",
      "Gradient Descent(17/99): loss =1.164468258228242e+16, w0=2.6068119588997064e-11, w1=0.20000000026713433\n",
      "Gradient Descent(18/99): loss =1.1643664959578142e+16, w0=2.739703687243569e-11, w1=0.2000000002830615\n",
      "Gradient Descent(19/99): loss =1.1642648054945946e+16, w0=2.872585279907093e-11, w1=0.20000000029898574\n",
      "Gradient Descent(20/99): loss =1.1641631559322346e+16, w0=3.005461428479863e-11, w1=0.20000000031490783\n",
      "Gradient Descent(21/99): loss =1.1640615326731348e+16, w0=3.138335354279262e-11, w1=0.2000000003308283\n",
      "Gradient Descent(22/99): loss =1.1639599288214766e+16, w0=3.271209268467422e-11, w1=0.2000000003467475\n",
      "Gradient Descent(23/99): loss =1.1638583411186232e+16, w0=3.4040846882201025e-11, w1=0.2000000003626657\n",
      "Gradient Descent(24/99): loss =1.163756768023626e+16, w0=3.536962653976956e-11, w1=0.20000000037858304\n",
      "Gradient Descent(25/99): loss =1.1636552088067336e+16, w0=3.669843878717193e-11, w1=0.20000000039449964\n",
      "Gradient Descent(26/99): loss =1.1635536631213076e+16, w0=3.80272885052511e-11, w1=0.20000000041041557\n",
      "Gradient Descent(27/99): loss =1.163452130801655e+16, w0=3.9356179030582274e-11, w1=0.20000000042633093\n",
      "Gradient Descent(28/99): loss =1.1633506117675454e+16, w0=4.068511263959722e-11, w1=0.2000000004422457\n",
      "Gradient Descent(29/99): loss =1.1632491059791312e+16, w0=4.2014090881156217e-11, w1=0.2000000004581599\n",
      "Gradient Descent(30/99): loss =1.1631476134156378e+16, w0=4.3343114804986106e-11, w1=0.2000000004740736\n",
      "Gradient Descent(31/99): loss =1.1630461340653108e+16, w0=4.467218511856907e-11, w1=0.20000000048998678\n",
      "Gradient Descent(32/99): loss =1.1629446679206642e+16, w0=4.600130229487297e-11, w1=0.20000000050589944\n",
      "Gradient Descent(33/99): loss =1.1628432149762314e+16, w0=4.7330466646309226e-11, w1=0.20000000052181158\n",
      "Gradient Descent(34/99): loss =1.162741775227506e+16, w0=4.865967837549033e-11, w1=0.2000000005377232\n",
      "Gradient Descent(35/99): loss =1.1626403486704398e+16, w0=4.998893761005139e-11, w1=0.20000000055363434\n",
      "Gradient Descent(36/99): loss =1.1625389353012012e+16, w0=5.1318244426526886e-11, w1=0.20000000056954495\n",
      "Gradient Descent(37/99): loss =1.162437535116067e+16, w0=5.264759886671205e-11, w1=0.20000000058545506\n",
      "Gradient Descent(38/99): loss =1.162336148111369e+16, w0=5.397700094886464e-11, w1=0.20000000060136466\n",
      "Gradient Descent(39/99): loss =1.162234774283463e+16, w0=5.5306450675365764e-11, w1=0.20000000061727374\n",
      "Gradient Descent(40/99): loss =1.162133413628721e+16, w0=5.663594803795131e-11, w1=0.20000000063318232\n",
      "Gradient Descent(41/99): loss =1.162032066143524e+16, w0=5.7965493021277616e-11, w1=0.20000000064909038\n",
      "Gradient Descent(42/99): loss =1.1619307318242588e+16, w0=5.92950856053456e-11, w1=0.20000000066499793\n",
      "Gradient Descent(43/99): loss =1.1618294106673182e+16, w0=6.062472576714329e-11, w1=0.20000000068090495\n",
      "Gradient Descent(44/99): loss =1.1617281026690924e+16, w0=6.195441348175389e-11, w1=0.20000000069681148\n",
      "Gradient Descent(45/99): loss =1.1616268078259804e+16, w0=6.328414872309869e-11, w1=0.20000000071271748\n",
      "Gradient Descent(46/99): loss =1.1615255261343844e+16, w0=6.46139314644312e-11, w1=0.20000000072862298\n",
      "Gradient Descent(47/99): loss =1.1614242575907028e+16, w0=6.594376167866208e-11, w1=0.20000000074452795\n",
      "Gradient Descent(48/99): loss =1.1613230021913396e+16, w0=6.72736393385694e-11, w1=0.2000000007604324\n",
      "Gradient Descent(49/99): loss =1.161221759932702e+16, w0=6.860356441693152e-11, w1=0.20000000077633634\n",
      "Gradient Descent(50/99): loss =1.1611205308112008e+16, w0=6.993353688660816e-11, w1=0.20000000079223976\n",
      "Gradient Descent(51/99): loss =1.1610193148232424e+16, w0=7.12635567205868e-11, w1=0.20000000080814268\n",
      "Gradient Descent(52/99): loss =1.1609181119652416e+16, w0=7.259362389200655e-11, w1=0.20000000082404507\n",
      "Gradient Descent(53/99): loss =1.1608169222336102e+16, w0=7.392373837416715e-11, w1=0.20000000083994693\n",
      "Gradient Descent(54/99): loss =1.1607157456247686e+16, w0=7.52539001405288e-11, w1=0.2000000008558483\n",
      "Gradient Descent(55/99): loss =1.1606145821351278e+16, w0=7.658410916470616e-11, w1=0.20000000087174913\n",
      "Gradient Descent(56/99): loss =1.1605134317611144e+16, w0=7.791436542045925e-11, w1=0.20000000088764944\n",
      "Gradient Descent(57/99): loss =1.1604122944991472e+16, w0=7.924466888168257e-11, w1=0.20000000090354925\n",
      "Gradient Descent(58/99): loss =1.1603111703456486e+16, w0=8.057501952239355e-11, w1=0.20000000091944853\n",
      "Gradient Descent(59/99): loss =1.1602100592970434e+16, w0=8.19054173167211e-11, w1=0.20000000093534728\n",
      "Gradient Descent(60/99): loss =1.1601089613497606e+16, w0=8.323586223889443e-11, w1=0.20000000095124554\n",
      "Gradient Descent(61/99): loss =1.1600078765002276e+16, w0=8.456635426323267e-11, w1=0.20000000096714327\n",
      "Gradient Descent(62/99): loss =1.1599068047448754e+16, w0=8.589689336413503e-11, w1=0.20000000098304047\n",
      "Gradient Descent(63/99): loss =1.1598057460801364e+16, w0=8.7227479516072e-11, w1=0.20000000099893717\n",
      "Gradient Descent(64/99): loss =1.1597047005024444e+16, w0=8.855811269357714e-11, w1=0.20000000101483334\n",
      "Gradient Descent(65/99): loss =1.1596036680082358e+16, w0=8.988879287123972e-11, w1=0.20000000103072899\n",
      "Gradient Descent(66/99): loss =1.1595026485939462e+16, w0=9.121952002369812e-11, w1=0.20000000104662413\n",
      "Gradient Descent(67/99): loss =1.159401642256015e+16, w0=9.255029412563394e-11, w1=0.20000000106251875\n",
      "Gradient Descent(68/99): loss =1.159300648990886e+16, w0=9.388111515176661e-11, w1=0.20000000107841284\n",
      "Gradient Descent(69/99): loss =1.1591996687949998e+16, w0=9.521198307684863e-11, w1=0.2000000010943064\n",
      "Gradient Descent(70/99): loss =1.1590987016648004e+16, w0=9.65428978756614e-11, w1=0.20000000111019947\n",
      "Gradient Descent(71/99): loss =1.1589977475967382e+16, w0=9.787385952301142e-11, w1=0.200000001126092\n",
      "Gradient Descent(72/99): loss =1.1588968065872542e+16, w0=9.920486799372692e-11, w1=0.20000000114198402\n",
      "Gradient Descent(73/99): loss =1.158795878632801e+16, w0=1.0053592326265495e-10, w1=0.20000000115787553\n",
      "Gradient Descent(74/99): loss =1.1586949637298346e+16, w0=1.0186702530465869e-10, w1=0.2000000011737665\n",
      "Gradient Descent(75/99): loss =1.158594061874803e+16, w0=1.0319817409461512e-10, w1=0.20000000118965697\n",
      "Gradient Descent(76/99): loss =1.1584931730641622e+16, w0=1.0452936960741305e-10, w1=0.20000000120554692\n",
      "Gradient Descent(77/99): loss =1.1583922972943704e+16, w0=1.0586061181795116e-10, w1=0.20000000122143635\n",
      "Gradient Descent(78/99): loss =1.1582914345618844e+16, w0=1.0719190070113646e-10, w1=0.20000000123732525\n",
      "Gradient Descent(79/99): loss =1.1581905848631638e+16, w0=1.0852323623188292e-10, w1=0.20000000125321363\n",
      "Gradient Descent(80/99): loss =1.1580897481946714e+16, w0=1.0985461838511012e-10, w1=0.2000000012691015\n",
      "Gradient Descent(81/99): loss =1.1579889245528714e+16, w0=1.1118604713574224e-10, w1=0.20000000128498885\n",
      "Gradient Descent(82/99): loss =1.1578881139342262e+16, w0=1.1251752245870708e-10, w1=0.20000000130087567\n",
      "Gradient Descent(83/99): loss =1.1577873163352052e+16, w0=1.1384904432893521e-10, w1=0.200000001316762\n",
      "Gradient Descent(84/99): loss =1.1576865317522778e+16, w0=1.1518061272135923e-10, w1=0.20000000133264778\n",
      "Gradient Descent(85/99): loss =1.1575857601819116e+16, w0=1.1651222761091315e-10, w1=0.20000000134853305\n",
      "Gradient Descent(86/99): loss =1.1574850016205822e+16, w0=1.1784388897253183e-10, w1=0.2000000013644178\n",
      "Gradient Descent(87/99): loss =1.1573842560647614e+16, w0=1.1917559678115048e-10, w1=0.20000000138030205\n",
      "Gradient Descent(88/99): loss =1.1572835235109228e+16, w0=1.2050735101170433e-10, w1=0.20000000139618576\n",
      "Gradient Descent(89/99): loss =1.1571828039555458e+16, w0=1.2183915163912815e-10, w1=0.20000000141206895\n",
      "Gradient Descent(90/99): loss =1.1570820973951086e+16, w0=1.2317099863835606e-10, w1=0.20000000142795163\n",
      "Gradient Descent(91/99): loss =1.156981403826094e+16, w0=1.2450289198432123e-10, w1=0.20000000144383379\n",
      "Gradient Descent(92/99): loss =1.1568807232449796e+16, w0=1.2583483165195566e-10, w1=0.20000000145971542\n",
      "Gradient Descent(93/99): loss =1.1567800556482532e+16, w0=1.2716681761619003e-10, w1=0.20000000147559654\n",
      "Gradient Descent(94/99): loss =1.1566794010323998e+16, w0=1.2849884985195358e-10, w1=0.20000000149147715\n",
      "Gradient Descent(95/99): loss =1.1565787593939068e+16, w0=1.2983092833417393e-10, w1=0.20000000150735722\n",
      "Gradient Descent(96/99): loss =1.1564781307292624e+16, w0=1.3116305303777704e-10, w1=0.2000000015232368\n",
      "Gradient Descent(97/99): loss =1.1563775150349576e+16, w0=1.3249522393768713e-10, w1=0.20000000153911585\n",
      "Gradient Descent(98/99): loss =1.1562769123074858e+16, w0=1.3382744100882662e-10, w1=0.20000000155499437\n",
      "Gradient Descent(99/99): loss =1.1561763225433396e+16, w0=1.3515970422611617e-10, w1=0.2000000015708724\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =5.008942965305848e+55, w0=-0.2, w1=0.2\n",
      "Gradient Descent(2/99): loss =7.836890950982909e+30, w0=-0.19999999999993587, w1=0.20000000000010687\n",
      "Gradient Descent(3/99): loss =7.834879492840114e+30, w0=-0.19999999999987225, w1=0.2000000000002137\n",
      "Gradient Descent(4/99): loss =7.833459208427259e+30, w0=-0.1999999999998086, w1=0.2000000000003205\n",
      "Gradient Descent(5/99): loss =7.832049265063783e+30, w0=-0.199999999999745, w1=0.20000000000042725\n",
      "Gradient Descent(6/99): loss =7.830641355395286e+30, w0=-0.19999999999968138, w1=0.20000000000053397\n",
      "Gradient Descent(7/99): loss =7.829235351161819e+30, w0=-0.1999999999996178, w1=0.20000000000064067\n",
      "Gradient Descent(8/99): loss =7.827831245534513e+30, w0=-0.1999999999995542, w1=0.2000000000007473\n",
      "Gradient Descent(9/99): loss =7.826429033634922e+30, w0=-0.19999999999949064, w1=0.2000000000008539\n",
      "Gradient Descent(10/99): loss =7.825028710733894e+30, w0=-0.19999999999942708, w1=0.2000000000009605\n",
      "Gradient Descent(11/99): loss =7.823630272204765e+30, w0=-0.19999999999936355, w1=0.20000000000106705\n",
      "Gradient Descent(12/99): loss =7.822233713506675e+30, w0=-0.19999999999930004, w1=0.20000000000117354\n",
      "Gradient Descent(13/99): loss =7.820839030171892e+30, w0=-0.19999999999923654, w1=0.20000000000128001\n",
      "Gradient Descent(14/99): loss =7.819446217795899e+30, w0=-0.19999999999917306, w1=0.20000000000138646\n",
      "Gradient Descent(15/99): loss =7.818055272029607e+30, w0=-0.19999999999910958, w1=0.20000000000149287\n",
      "Gradient Descent(16/99): loss =7.81666618857326e+30, w0=-0.19999999999904614, w1=0.20000000000159926\n",
      "Gradient Descent(17/99): loss =7.815278963171629e+30, w0=-0.19999999999898269, w1=0.2000000000017056\n",
      "Gradient Descent(18/99): loss =7.813893591610176e+30, w0=-0.19999999999891926, w1=0.2000000000018119\n",
      "Gradient Descent(19/99): loss =7.812510069712078e+30, w0=-0.19999999999885584, w1=0.20000000000191817\n",
      "Gradient Descent(20/99): loss =7.811128393335756e+30, w0=-0.19999999999879245, w1=0.20000000000202442\n",
      "Gradient Descent(21/99): loss =7.809748558372978e+30, w0=-0.19999999999872908, w1=0.20000000000213064\n",
      "Gradient Descent(22/99): loss =7.80837056074723e+30, w0=-0.19999999999866572, w1=0.20000000000223683\n",
      "Gradient Descent(23/99): loss =7.80699439641248e+30, w0=-0.19999999999860238, w1=0.20000000000234297\n",
      "Gradient Descent(24/99): loss =7.805620061352047e+30, w0=-0.19999999999853904, w1=0.20000000000244908\n",
      "Gradient Descent(25/99): loss =7.80424755157776e+30, w0=-0.19999999999847573, w1=0.20000000000255516\n",
      "Gradient Descent(26/99): loss =7.802876863129165e+30, w0=-0.19999999999841242, w1=0.20000000000266122\n",
      "Gradient Descent(27/99): loss =7.801507992072877e+30, w0=-0.19999999999834914, w1=0.20000000000276724\n",
      "Gradient Descent(28/99): loss =7.800140934502031e+30, w0=-0.19999999999828585, w1=0.20000000000287324\n",
      "Gradient Descent(29/99): loss =7.79877568653574e+30, w0=-0.1999999999982226, w1=0.2000000000029792\n",
      "Gradient Descent(30/99): loss =7.797412244318691e+30, w0=-0.19999999999815934, w1=0.20000000000308513\n",
      "Gradient Descent(31/99): loss =7.796050604020677e+30, w0=-0.19999999999809612, w1=0.20000000000319101\n",
      "Gradient Descent(32/99): loss =7.794690761836262e+30, w0=-0.1999999999980329, w1=0.20000000000329687\n",
      "Gradient Descent(33/99): loss =7.793332713984431e+30, w0=-0.1999999999979697, w1=0.2000000000034027\n",
      "Gradient Descent(34/99): loss =7.791976456708195e+30, w0=-0.1999999999979065, w1=0.2000000000035085\n",
      "Gradient Descent(35/99): loss =7.790621986274379e+30, w0=-0.19999999999784332, w1=0.2000000000036143\n",
      "Gradient Descent(36/99): loss =7.789269298973211e+30, w0=-0.19999999999778015, w1=0.20000000000372004\n",
      "Gradient Descent(37/99): loss =7.787918391118108e+30, w0=-0.199999999997717, w1=0.20000000000382576\n",
      "Gradient Descent(38/99): loss =7.786569259045378e+30, w0=-0.1999999999976539, w1=0.20000000000393142\n",
      "Gradient Descent(39/99): loss =7.785221899113931e+30, w0=-0.19999999999759077, w1=0.20000000000403706\n",
      "Gradient Descent(40/99): loss =7.783876307705032e+30, w0=-0.19999999999752768, w1=0.20000000000414267\n",
      "Gradient Descent(41/99): loss =7.782532481222033e+30, w0=-0.1999999999974646, w1=0.20000000000424825\n",
      "Gradient Descent(42/99): loss =7.781190416090124e+30, w0=-0.19999999999740153, w1=0.2000000000043538\n",
      "Gradient Descent(43/99): loss =7.779850108756087e+30, w0=-0.19999999999733847, w1=0.20000000000445933\n",
      "Gradient Descent(44/99): loss =7.778511555688055e+30, w0=-0.19999999999727544, w1=0.20000000000456483\n",
      "Gradient Descent(45/99): loss =7.777174753375246e+30, w0=-0.1999999999972124, w1=0.20000000000467028\n",
      "Gradient Descent(46/99): loss =7.775839698327767e+30, w0=-0.1999999999971494, w1=0.2000000000047757\n",
      "Gradient Descent(47/99): loss =7.774506387076348e+30, w0=-0.1999999999970864, w1=0.20000000000488108\n",
      "Gradient Descent(48/99): loss =7.773174816172152e+30, w0=-0.19999999999702342, w1=0.20000000000498644\n",
      "Gradient Descent(49/99): loss =7.77184498218649e+30, w0=-0.19999999999696044, w1=0.20000000000509177\n",
      "Gradient Descent(50/99): loss =7.770516881710643e+30, w0=-0.1999999999968975, w1=0.20000000000519708\n",
      "Gradient Descent(51/99): loss =7.76919051135564e+30, w0=-0.19999999999683454, w1=0.20000000000530235\n",
      "Gradient Descent(52/99): loss =7.76786586775203e+30, w0=-0.19999999999677162, w1=0.2000000000054076\n",
      "Gradient Descent(53/99): loss =7.766542947549668e+30, w0=-0.1999999999967087, w1=0.2000000000055128\n",
      "Gradient Descent(54/99): loss =7.76522174741751e+30, w0=-0.1999999999966458, w1=0.20000000000561796\n",
      "Gradient Descent(55/99): loss =7.76390226404339e+30, w0=-0.1999999999965829, w1=0.2000000000057231\n",
      "Gradient Descent(56/99): loss =7.762584494133819e+30, w0=-0.19999999999652004, w1=0.2000000000058282\n",
      "Gradient Descent(57/99): loss =7.761268434413815e+30, w0=-0.19999999999645718, w1=0.2000000000059333\n",
      "Gradient Descent(58/99): loss =7.759954081626644e+30, w0=-0.19999999999639434, w1=0.20000000000603835\n",
      "Gradient Descent(59/99): loss =7.758641432533662e+30, w0=-0.1999999999963315, w1=0.20000000000614337\n",
      "Gradient Descent(60/99): loss =7.757330483914112e+30, w0=-0.1999999999962687, w1=0.20000000000624837\n",
      "Gradient Descent(61/99): loss =7.756021232564932e+30, w0=-0.19999999999620588, w1=0.20000000000635335\n",
      "Gradient Descent(62/99): loss =7.754713675300532e+30, w0=-0.19999999999614307, w1=0.20000000000645826\n",
      "Gradient Descent(63/99): loss =7.753407808952664e+30, w0=-0.19999999999608029, w1=0.20000000000656315\n",
      "Gradient Descent(64/99): loss =7.752103630370189e+30, w0=-0.1999999999960175, w1=0.200000000006668\n",
      "Gradient Descent(65/99): loss =7.750801136418905e+30, w0=-0.19999999999595475, w1=0.20000000000677284\n",
      "Gradient Descent(66/99): loss =7.749500323981373e+30, w0=-0.199999999995892, w1=0.20000000000687765\n",
      "Gradient Descent(67/99): loss =7.748201189956734e+30, w0=-0.19999999999582926, w1=0.20000000000698243\n",
      "Gradient Descent(68/99): loss =7.746903731260507e+30, w0=-0.19999999999576654, w1=0.20000000000708718\n",
      "Gradient Descent(69/99): loss =7.745607944824477e+30, w0=-0.19999999999570384, w1=0.2000000000071919\n",
      "Gradient Descent(70/99): loss =7.744313827596448e+30, w0=-0.19999999999564114, w1=0.20000000000729656\n",
      "Gradient Descent(71/99): loss =7.743021376540122e+30, w0=-0.19999999999557846, w1=0.2000000000074012\n",
      "Gradient Descent(72/99): loss =7.741730588634904e+30, w0=-0.1999999999955158, w1=0.2000000000075058\n",
      "Gradient Descent(73/99): loss =7.740441460875751e+30, w0=-0.19999999999545315, w1=0.2000000000076104\n",
      "Gradient Descent(74/99): loss =7.739153990272997e+30, w0=-0.1999999999953905, w1=0.20000000000771495\n",
      "Gradient Descent(75/99): loss =7.737868173852191e+30, w0=-0.1999999999953279, w1=0.20000000000781948\n",
      "Gradient Descent(76/99): loss =7.736584008653946e+30, w0=-0.19999999999526527, w1=0.20000000000792398\n",
      "Gradient Descent(77/99): loss =7.735301491733773e+30, w0=-0.19999999999520268, w1=0.20000000000802845\n",
      "Gradient Descent(78/99): loss =7.734020620161915e+30, w0=-0.1999999999951401, w1=0.2000000000081329\n",
      "Gradient Descent(79/99): loss =7.732741391023217e+30, w0=-0.19999999999507753, w1=0.20000000000823728\n",
      "Gradient Descent(80/99): loss =7.73146380141695e+30, w0=-0.19999999999501497, w1=0.20000000000834164\n",
      "Gradient Descent(81/99): loss =7.730187848456661e+30, w0=-0.1999999999949524, w1=0.20000000000844598\n",
      "Gradient Descent(82/99): loss =7.728913529270043e+30, w0=-0.19999999999488988, w1=0.20000000000855028\n",
      "Gradient Descent(83/99): loss =7.727640840998757e+30, w0=-0.19999999999482734, w1=0.20000000000865456\n",
      "Gradient Descent(84/99): loss =7.726369780798327e+30, w0=-0.19999999999476484, w1=0.2000000000087588\n",
      "Gradient Descent(85/99): loss =7.725100345837954e+30, w0=-0.19999999999470233, w1=0.20000000000886303\n",
      "Gradient Descent(86/99): loss =7.723832533300403e+30, w0=-0.19999999999463985, w1=0.20000000000896723\n",
      "Gradient Descent(87/99): loss =7.722566340381846e+30, w0=-0.19999999999457738, w1=0.2000000000090714\n",
      "Gradient Descent(88/99): loss =7.721301764291728e+30, w0=-0.19999999999451493, w1=0.2000000000091755\n",
      "Gradient Descent(89/99): loss =7.720038802252641e+30, w0=-0.19999999999445248, w1=0.2000000000092796\n",
      "Gradient Descent(90/99): loss =7.718777451500178e+30, w0=-0.19999999999439005, w1=0.20000000000938364\n",
      "Gradient Descent(91/99): loss =7.717517709282797e+30, w0=-0.19999999999432763, w1=0.20000000000948767\n",
      "Gradient Descent(92/99): loss =7.716259572861674e+30, w0=-0.1999999999942652, w1=0.20000000000959167\n",
      "Gradient Descent(93/99): loss =7.71500303951062e+30, w0=-0.19999999999420282, w1=0.20000000000969564\n",
      "Gradient Descent(94/99): loss =7.713748106515911e+30, w0=-0.19999999999414042, w1=0.2000000000097996\n",
      "Gradient Descent(95/99): loss =7.712494771176168e+30, w0=-0.19999999999407805, w1=0.2000000000099035\n",
      "Gradient Descent(96/99): loss =7.711243030802237e+30, w0=-0.1999999999940157, w1=0.2000000000100074\n",
      "Gradient Descent(97/99): loss =7.709992882717065e+30, w0=-0.19999999999395335, w1=0.20000000001011123\n",
      "Gradient Descent(98/99): loss =7.708744324255584e+30, w0=-0.199999999993891, w1=0.20000000001021503\n",
      "Gradient Descent(99/99): loss =7.707497352764556e+30, w0=-0.1999999999938287, w1=0.2000000000103188\n"
     ]
    }
   ],
   "source": [
    "losses_tr, losses_te, accs_tr, accs_te = params_optimization(y_tr, tX_tr, k_fold, model, degrees, params = {'max_iters':max_iters}, seed = seed, feedback = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we optimized over degrees only, so we plot degree vs accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_vs_err(degrees, accs_tr, accs_te, model, 'Accuracy', save_img = True, img_name = 'least_squares_GD_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.40521213e-01 3.97291331e-01 3.84215119e+01 6.89254943e+04\n",
      " 4.67730754e+08 2.97067822e+11 1.33935490e+12 4.24062353e+13\n",
      " 1.35801287e+16 5.49291875e+25 5.30340075e+27 2.44442492e+29\n",
      " 8.53989943e+30 2.54002022e+32 5.04069403e+33] (15,)\n"
     ]
    }
   ],
   "source": [
    "print(losses_te, losses_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually choose best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_degree = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute performance on k splits with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation 1/4 for least_squares_GD, extended feature of degree 2 and arguments : {'max_iters': 100, 'plot': True}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/99): loss =7.839059935623126, w0=-0.0026156943882950368, w1=0.19999316275092116\n",
      "Gradient Descent(2/99): loss =8.75329449924172, w0=-0.049107858033846194, w1=0.16678973987966308\n",
      "Gradient Descent(3/99): loss =1.8621114635202523, w0=-0.022850700072873317, w1=0.1726996630256814\n",
      "Gradient Descent(4/99): loss =0.7383762858533274, w0=-0.02701458582923431, w1=0.15578907023023617\n",
      "Gradient Descent(5/99): loss =0.5662999384979516, w0=-0.021923980358801984, w1=0.1486280847753041\n",
      "Gradient Descent(6/99): loss =0.5156944373733406, w0=-0.021017199928453953, w1=0.14030139119986393\n",
      "Gradient Descent(7/99): loss =0.48810568484290595, w0=-0.019649961929925353, w1=0.1341529768138526\n",
      "Gradient Descent(8/99): loss =0.46901730367531697, w0=-0.01913208615736729, w1=0.12872586535326513\n",
      "Gradient Descent(9/99): loss =0.45474042160114586, w0=-0.018869814395055542, w1=0.12420143984865449\n",
      "Gradient Descent(10/99): loss =0.4436061254789799, w0=-0.01892850243342921, w1=0.12029547107675036\n",
      "Gradient Descent(11/99): loss =0.4346440702968133, w0=-0.019188281057969252, w1=0.11693350232873535\n",
      "Gradient Descent(12/99): loss =0.4272391938952367, w0=-0.019614762596568268, w1=0.11401174841549663\n",
      "Gradient Descent(13/99): loss =0.4209853862663078, w0=-0.020163763643151724, w1=0.11146559970743769\n",
      "Gradient Descent(14/99): loss =0.4156060227675358, w0=-0.02080843635664514, w1=0.10923848514048146\n",
      "Gradient Descent(15/99): loss =0.41090740269902987, w0=-0.02152654929090058, w1=0.10728697178545459\n",
      "Gradient Descent(16/99): loss =0.40675037679517834, w0=-0.02230213125513223, w1=0.10557501244815369\n",
      "Gradient Descent(17/99): loss =0.40303259616363574, w0=-0.023122874138783066, w1=0.10407312949895886\n",
      "Gradient Descent(18/99): loss =0.3996771318101973, w0=-0.023979542503003444, w1=0.10275660798362515\n",
      "Gradient Descent(19/99): loss =0.3966250094052997, w0=-0.024865060837867405, w1=0.10160460676562937\n",
      "Gradient Descent(20/99): loss =0.39383020286708625, w0=-0.02577401241522336, w1=0.10059932131799326\n",
      "Gradient Descent(21/99): loss =0.3912562055125026, w0=-0.026702202581713557, w1=0.0997254163385572\n",
      "Gradient Descent(22/99): loss =0.3888736344128424, w0=-0.027646359937290702, w1=0.09896956455039031\n",
      "Gradient Descent(23/99): loss =0.386658525058665, w0=-0.028603904239552336, w1=0.0983201006651155\n",
      "Gradient Descent(24/99): loss =0.38459109610457165, w0=-0.029572777592395044, w1=0.09776674607327517\n",
      "Gradient Descent(25/99): loss =0.38265484006955297, w0=-0.030551316854735652, w1=0.09730039297893242\n",
      "Gradient Descent(26/99): loss =0.3808358439427492, w0=-0.031538159002259085, w1=0.09691293090297175\n",
      "Gradient Descent(27/99): loss =0.37912227455878444, w0=-0.03253217001478434, w1=0.09659710672065247\n",
      "Gradient Descent(28/99): loss =0.37750398383131095, w0=-0.03353239171910362, w1=0.09634641006475483\n",
      "Gradient Descent(29/99): loss =0.3759722023872525, w0=-0.034538001854355356, w1=0.09615497869395927\n",
      "Gradient Descent(30/99): loss =0.3745192992385789, w0=-0.03554828410100614, w1=0.0960175194593783\n",
      "Gradient Descent(31/99): loss =0.3731385913710672, w0=-0.036562605532312206, w1=0.09592924169663718\n",
      "Gradient Descent(32/99): loss =0.3718241914777759, w0=-0.03758039964063864, w1=0.09588580056375463\n",
      "Gradient Descent(33/99): loss =0.3705708851360639, w0=-0.03860115353521199, w1=0.09588324844453347\n",
      "Gradient Descent(34/99): loss =0.3693740309246791, w0=-0.03962439826978405, w1=0.09591799294834454\n",
      "Gradient Descent(35/99): loss =0.36822947856958255, w0=-0.04064970151502547, w1=0.09598676036300258\n",
      "Gradient Descent(36/99): loss =0.3671335013739608, w0=-0.04167666198824831, w1=0.09608656365673683\n",
      "Gradient Descent(37/99): loss =0.3660827400521508, w0=-0.0427049051985624, w1=0.0962146743108104\n",
      "Gradient Descent(38/99): loss =0.3650741557337672, w0=-0.04373408017596334, w1=0.09636859740520445\n",
      "Gradient Descent(39/99): loss =0.36410499039248717, w0=-0.04476385693528622, w1=0.09654604948922944\n",
      "Gradient Descent(40/99): loss =0.36317273332564776, w0=-0.04579392448821304, w1=0.09674493885380675\n",
      "Gradient Descent(41/99): loss =0.3622750925960749, w0=-0.046823989263290185, w1=0.09696334788885669\n",
      "Gradient Descent(42/99): loss =0.3614099705680758, w0=-0.047853773829184806, w1=0.09719951726189954\n",
      "Gradient Descent(43/99): loss =0.36057544284115683, w0=-0.04888301584295818, w1=0.09745183169598837\n",
      "Gradient Descent(44/99): loss =0.3597697400194523, w0=-0.049911467165133, w1=0.097718807158861\n",
      "Gradient Descent(45/99): loss =0.3589912318607727, w0=-0.05093889309837366, w1=0.0979990793025898\n",
      "Gradient Descent(46/99): loss =0.3582384134331153, w0=-0.05196507171790121, w1=0.09829139301541528\n",
      "Gradient Descent(47/99): loss =0.35750989297334634, w0=-0.052989793270238846, w1=0.09859459296594207\n",
      "Gradient Descent(48/99): loss =0.3568043811963269, w0=-0.054012859623221436, w1=0.0989076150352675\n",
      "Gradient Descent(49/99): loss =0.3561206818458572, w0=-0.05503408375492592, w1=0.09922947854553062\n",
      "Gradient Descent(50/99): loss =0.3554576833136855, w0=-0.05605328927268386, w1=0.09955927920429852\n",
      "Gradient Descent(51/99): loss =0.3548143511811677, w0=-0.05707030995592364, w1=0.09989618269352311\n",
      "Gradient Descent(52/99): loss =0.3541897215613066, w0=-0.05808498931848476, w1=0.1002394188398002\n",
      "Gradient Descent(53/99): loss =0.35358289513788344, w0=-0.059097180187423146, w1=0.1005882763095768\n",
      "Gradient Descent(54/99): loss =0.3529930318140404, w0=-0.06010674429631461, w1=0.1009420977789641\n",
      "Gradient Descent(55/99): loss =0.3524193458956281, w0=-0.06111355189176397, w1=0.10130027553307111\n",
      "Gradient Descent(56/99): loss =0.3518611017453944, w0=-0.062117481352312626, w1=0.10166224745439435\n",
      "Gradient Descent(57/99): loss =0.35131760985309196, w0=-0.06311841881926657, w1=0.10202749336387762\n",
      "Gradient Descent(58/99): loss =0.3507882232741185, w0=-0.06411625783918048, w1=0.10239553168187161\n",
      "Gradient Descent(59/99): loss =0.3502723343956601, w0=-0.06511089901786557, w1=0.10276591637943824\n",
      "Gradient Descent(60/99): loss =0.34976937199467656, w0=-0.06610224968586104, w1=0.10313823419331387\n",
      "Gradient Descent(61/99): loss =0.3492787985566309, w0=-0.06709022357534178, w1=0.1035121020804117\n",
      "Gradient Descent(62/99): loss =0.34880010782775417, w0=-0.068074740508439, w1=0.10388716489004482\n",
      "Gradient Descent(63/99): loss =0.34833282257695686, w0=-0.06905572609693704, w1=0.10426309323411875\n",
      "Gradient Descent(64/99): loss =0.34787649254636416, w0=-0.07003311145328578, w1=0.10463958153740255\n",
      "Gradient Descent(65/99): loss =0.3474306925719056, w0=-0.07100683291283702, w1=0.10501634625166424\n",
      "Gradient Descent(66/99): loss =0.3469950208575221, w0=-0.07197683176718149, w1=0.10539312421896922\n",
      "Gradient Descent(67/99): loss =0.3465690973884021, w0=-0.07294305400843018, w1=0.10576967117080667\n",
      "Gradient Descent(68/99): loss =0.34615256247026266, w0=-0.07390545008425314, w1=0.10614576035094415\n",
      "Gradient Descent(69/99): loss =0.34574507538309557, w0=-0.07486397466346102, w1=0.1065211812510285\n",
      "Gradient Descent(70/99): loss =0.3453463131390307, w0=-0.07581858641188956, w1=0.10689573844896216\n",
      "Gradient Descent(71/99): loss =0.3449559693350421, w0=-0.07676924777832658, w1=0.10726925054100082\n",
      "Gradient Descent(72/99): loss =0.34457375309217686, w0=-0.07771592479020309, w1=0.10764154915934786\n",
      "Gradient Descent(73/99): loss =0.3441993880738217, w0=-0.07865858685875618, w1=0.10801247806777375\n",
      "Gradient Descent(74/99): loss =0.34383261157626716, w0=-0.07959720659336099, w1=0.10838189232847055\n",
      "Gradient Descent(75/99): loss =0.3434731736854816, w0=-0.08053175962472085, w1=0.10874965753396985\n",
      "Gradient Descent(76/99): loss =0.3431208364945981, w0=-0.08146222443660034, w1=0.10911564909851422\n",
      "Gradient Descent(77/99): loss =0.34277537337713176, w0=-0.08238858220578321, w1=0.10947975160378023\n",
      "Gradient Descent(78/99): loss =0.3424365683114135, w0=-0.08331081664993732, w1=0.10984185819431358\n",
      "Gradient Descent(79/99): loss =0.34210421525213874, w0=-0.08422891388306995, w1=0.11020187001845527\n",
      "Gradient Descent(80/99): loss =0.34177811754530385, w0=-0.08514286227826073, w1=0.11055969571091841\n",
      "Gradient Descent(81/99): loss =0.3414580873831339, w0=-0.08605265233736352, w1=0.11091525091351982\n",
      "Gradient Descent(82/99): loss =0.34114394529590886, w0=-0.08695827656737508, w1=0.11126845783088433\n",
      "Gradient Descent(83/99): loss =0.3408355196778616, w0=-0.08785972936317459, w1=0.1116192448182235\n",
      "Gradient Descent(84/99): loss =0.3405326463445665, w0=-0.08875700689634614, w1=0.11196754599854883\n",
      "Gradient Descent(85/99): loss =0.3402351681194582, w0=-0.08965010700980407, w1=0.11231330090691415\n",
      "Gradient Descent(86/99): loss =0.3399429344473154, w0=-0.09053902911794998, w1=0.11265645415949402\n",
      "Gradient Descent(87/99): loss =0.3396558010327295, w0=-0.09142377411209898, w1=0.11299695514549904\n",
      "Gradient Descent(88/99): loss =0.3393736295017357, w0=-0.09230434427092211, w1=0.11333475774010399\n",
      "Gradient Descent(89/99): loss =0.33909628708493583, w0=-0.09318074317566094, w1=0.11366982003672434\n",
      "Gradient Descent(90/99): loss =0.3388236463205772, w0=-0.09405297562987995, w1=0.1140021040971218\n",
      "Gradient Descent(91/99): loss =0.3385555847761706, w0=-0.09492104758353154, w1=0.11433157571795097\n",
      "Gradient Descent(92/99): loss =0.33829198478734623, w0=-0.09578496606111754, w1=0.11465820421247887\n",
      "Gradient Descent(93/99): loss =0.33803273321274685, w0=-0.09664473909374073, w1=0.11498196220631837\n",
      "Gradient Descent(94/99): loss =0.33777772120384764, w0=-0.09750037565484837, w1=0.11530282544611489\n",
      "Gradient Descent(95/99): loss =0.33752684398868377, w0=-0.09835188559947874, w1=0.1156207726202161\n",
      "Gradient Descent(96/99): loss =0.33728000066853553, w0=-0.09919927960683025, w1=0.11593578519043617\n",
      "Gradient Descent(97/99): loss =0.3370370940267008, w0=-0.1000425691259809, w1=0.11624784723410078\n",
      "Gradient Descent(98/99): loss =0.33679803034854416, w0=-0.10088176632459392, w1=0.1165569452956265\n",
      "Gradient Descent(99/99): loss =0.33656271925207265, w0=-0.1017168840404535, w1=0.1168630682469504\n",
      "Starting cross-validation 2/4 for least_squares_GD, extended feature of degree 2 and arguments : {'max_iters': 100, 'plot': True}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =23.218819551185142, w0=-0.0020510006922389717, w1=0.1999930949740934\n",
      "Gradient Descent(2/99): loss =1.7069669301591468, w0=-0.03396607658744928, w1=0.1690559486179855\n",
      "Gradient Descent(3/99): loss =0.5726444386439423, w0=-0.024211784986175496, w1=0.17004295386238566\n",
      "Gradient Descent(4/99): loss =0.46677094460534024, w0=-0.02149860327065887, w1=0.16105178656120314\n",
      "Gradient Descent(5/99): loss =0.4225764690527554, w0=-0.018797038362096615, w1=0.15357027736961631\n",
      "Gradient Descent(6/99): loss =0.3948155370765971, w0=-0.01711676744769795, w1=0.14669099945660716\n",
      "Gradient Descent(7/99): loss =0.3770714648911897, w0=-0.01604159185094181, w1=0.14078531139132835\n",
      "Gradient Descent(8/99): loss =0.3654080625309092, w0=-0.015486911312984684, w1=0.1355541388225885\n",
      "Gradient Descent(9/99): loss =0.3573863205854477, w0=-0.01530260610356254, w1=0.13095865431315462\n",
      "Gradient Descent(10/99): loss =0.35163854679842077, w0=-0.015411619329248444, w1=0.12685950666861862\n",
      "Gradient Descent(11/99): loss =0.3473749456496453, w0=-0.015736799716261716, w1=0.12320553184956098\n",
      "Gradient Descent(12/99): loss =0.3441163310846299, w0=-0.016230261261968675, w1=0.11992655950010864\n",
      "Gradient Descent(13/99): loss =0.3415598967617122, w0=-0.016850916001337816, w1=0.11698229700198702\n",
      "Gradient Descent(14/99): loss =0.3395079072341074, w0=-0.01757069395290656, w1=0.11433108480252545\n",
      "Gradient Descent(15/99): loss =0.3378275281254208, w0=-0.018367004189139533, w1=0.11194259574569443\n",
      "Gradient Descent(16/99): loss =0.3364271959131066, w0=-0.019223520921392588, w1=0.10978857130658752\n",
      "Gradient Descent(17/99): loss =0.33524226230563003, w0=-0.0201274165319058, w1=0.10784590653063736\n",
      "Gradient Descent(18/99): loss =0.33422604626718566, w0=-0.021069085967963773, w1=0.10609374718268237\n",
      "Gradient Descent(19/99): loss =0.3333441297443167, w0=-0.02204101603550966, w1=0.10451408751191944\n",
      "Gradient Descent(20/99): loss =0.3325706446703583, w0=-0.02303743940804837, w1=0.10309076137928837\n",
      "Gradient Descent(21/99): loss =0.3318858072558858, w0=-0.024053819523054847, w1=0.10180944515275785\n",
      "Gradient Descent(22/99): loss =0.33127424831203583, w0=-0.025086603127016188, w1=0.10065724512213074\n",
      "Gradient Descent(23/99): loss =0.33072386080406224, w0=-0.026132962825704698, w1=0.09962258870199718\n",
      "Gradient Descent(24/99): loss =0.3302249894245567, w0=-0.027190641398268895, w1=0.09869501804674356\n",
      "Gradient Descent(25/99): loss =0.3297698502795603, w0=-0.028257813949403906, w1=0.09786508180996797\n",
      "Gradient Descent(26/99): loss =0.32935210807703186, w0=-0.02933299383820917, w1=0.09712421079538955\n",
      "Gradient Descent(27/99): loss =0.32896656296919424, w0=-0.030414955330757935, w1=0.0964646296939365\n",
      "Gradient Descent(28/99): loss =0.3286089150274391, w0=-0.03150267736304122, w1=0.09587927159202025\n",
      "Gradient Descent(29/99): loss =0.3282755845875087, w0=-0.032595298794314775, w1=0.09536170851278146\n",
      "Gradient Descent(30/99): loss =0.3279635734442593, w0=-0.0336920847721315, w1=0.09490608779431149\n",
      "Gradient Descent(31/99): loss =0.3276703563668851, w0=-0.03479240032749299, w1=0.09450707723943848\n",
      "Gradient Descent(32/99): loss =0.3273937954395894, w0=-0.035895690170821636, w1=0.09415981561080168\n",
      "Gradient Descent(33/99): loss =0.32713207181121823, w0=-0.03700146290390246, w1=0.09385986880221653\n",
      "Gradient Descent(34/99): loss =0.32688363088162453, w0=-0.03810927881844595, w1=0.0936031903371955\n",
      "Gradient Descent(35/99): loss =0.32664713797055095, w0=-0.03921874036690496, w1=0.09338608595415374\n",
      "Gradient Descent(36/99): loss =0.3264214422427579, w0=-0.040329484751591926, w1=0.09320518162273647\n",
      "Gradient Descent(37/99): loss =0.32620554719105815, w0=-0.041441178127579345, w1=0.09305739468404009\n",
      "Gradient Descent(38/99): loss =0.3259985863671306, w0=-0.04255351107109815, w1=0.09293990772489347\n",
      "Gradient Descent(39/99): loss =0.32579980333923625, w0=-0.04366619502225998, w1=0.09285014491965338\n",
      "Gradient Descent(40/99): loss =0.3256085350741836, w0=-0.044778959487878786, w1=0.0927857505720355\n",
      "Gradient Descent(41/99): loss =0.32542419810743295, w0=-0.04589154983208143, w1=0.09274456964216297\n",
      "Gradient Descent(42/99): loss =0.3252462769936835, w0=-0.04700372552401755, w1=0.09272463005940172\n",
      "Gradient Descent(43/99): loss =0.32507431463029723, w0=-0.04811525873939268, w1=0.092724126649593\n",
      "Gradient Descent(44/99): loss =0.32490790412445636, w0=-0.049225933236404784, w1=0.09274140652119486\n",
      "Gradient Descent(45/99): loss =0.3247466819370858, w0=-0.050335543443876765, w1=0.09277495577278899\n",
      "Gradient Descent(46/99): loss =0.3245903220860575, w0=-0.0514438937134978, w1=0.0928233873977649\n",
      "Gradient Descent(47/99): loss =0.3244385312308379, w0=-0.0525507976987217, w1=0.09288543027489155\n",
      "Gradient Descent(48/99): loss =0.3242910444926528, w0=-0.053656077831364196, w1=0.09295991914423779\n",
      "Gradient Descent(49/99): loss =0.3241476218900585, w0=-0.0547595648734642, w1=0.09304578547777784\n",
      "Gradient Descent(50/99): loss =0.3240080452907689, w0=-0.05586109752711659, w1=0.09314204916264524\n",
      "Gradient Descent(51/99): loss =0.32387211579766567, w0=-0.05696052208896383, w1=0.09324781092281367\n",
      "Gradient Descent(52/99): loss =0.3237396515008849, w0=-0.05805769213914781, w1=0.09336224541194833\n",
      "Gradient Descent(53/99): loss =0.32361048553932076, w0=-0.05915246825693651, w1=0.09348459491646702\n",
      "Gradient Descent(54/99): loss =0.323484464424298, w0=-0.060244717757116296, w1=0.09361416361351277\n",
      "Gradient Descent(55/99): loss =0.3233614465859261, w0=-0.06133431444269093, w1=0.09375031233366167\n",
      "Gradient Descent(56/99): loss =0.32324130110905364, w0=-0.06242113837054824, w1=0.09389245378281756\n",
      "Gradient Descent(57/99): loss =0.32312390663105095, w0=-0.06350507562761475, w1=0.09404004818193644\n",
      "Gradient Descent(58/99): loss =0.3230091503780532, w0=-0.06458601811567696, w1=0.09419259928701959\n",
      "Gradient Descent(59/99): loss =0.32289692731995845, w0=-0.06566386334354762, w1=0.09434965075525553\n",
      "Gradient Descent(60/99): loss =0.32278713942752807, w0=-0.06673851422563232, w1=0.09451078282631184\n",
      "Gradient Descent(61/99): loss =0.32267969501748617, w0=-0.06780987888623467, w1=0.0946756092906088\n",
      "Gradient Descent(62/99): loss =0.32257450817364824, w0=-0.06887787046914659, w1=0.09484377471897612\n",
      "Gradient Descent(63/99): loss =0.3224714982338954, w0=-0.06994240695222234, w1=0.09501495193042585\n",
      "Gradient Descent(64/99): loss =0.32237058933431706, w0=-0.0710034109667443, w1=0.09518883967689101\n",
      "Gradient Descent(65/99): loss =0.32227171000310423, w0=-0.07206080962146391, w1=0.09536516052570194\n",
      "Gradient Descent(66/99): loss =0.32217479279784905, w0=-0.07311453433125278, w1=0.09554365892231648\n",
      "Gradient Descent(67/99): loss =0.3220797739808005, w0=-0.07416452065033101, w1=0.09572409941740533\n",
      "Gradient Descent(68/99): loss =0.3219865932273948, w0=-0.07521070811005905, w1=0.09590626504383229\n",
      "Gradient Descent(69/99): loss =0.3218951933640251, w0=-0.07625304006128791, w1=0.09608995583037601\n",
      "Gradient Descent(70/99): loss =0.3218055201315635, w0=-0.07729146352126452, w1=0.09627498744022656\n",
      "Gradient Descent(71/99): loss =0.32171752197162196, w0=-0.07832592902508569, w1=0.09646118992336802\n",
      "Gradient Descent(72/99): loss =0.321631149832933, w0=-0.07935639048168806, w1=0.09664840657293744\n",
      "Gradient Descent(73/99): loss =0.32154635699557904, w0=-0.08038280503435295, w1=0.09683649287653945\n",
      "Gradient Descent(74/99): loss =0.3214630989110874, w0=-0.08140513292569557, w1=0.0970253155543044\n",
      "Gradient Descent(75/99): loss =0.321381333056664, w0=-0.08242333736709859, w1=0.0972147516762111\n",
      "Gradient Descent(76/99): loss =0.3213010188020516, w0=-0.08343738441254032, w1=0.0974046878518629\n",
      "Gradient Descent(77/99): loss =0.32122211728768774, w0=-0.08444724283675852, w1=0.09759501948651116\n",
      "Gradient Descent(78/99): loss =0.3211445913129963, w0=-0.08545288401768242, w1=0.09778565009767151\n",
      "Gradient Descent(79/99): loss =0.32106840523378755, w0=-0.08645428182305807, w1=0.0979764906871787\n",
      "Gradient Descent(80/99): loss =0.3209935248678635, w0=-0.08745141250118486, w1=0.09816745916398063\n",
      "Gradient Descent(81/99): loss =0.3209199174080248, w0=-0.08844425457567565, w1=0.0983584798133864\n",
      "Gradient Descent(82/99): loss =0.3208475513417741, w0=-0.0894327887441477, w1=0.09854948280885904\n",
      "Gradient Descent(83/99): loss =0.32077639637708577, w0=-0.09041699778074748, w1=0.09874040376278617\n",
      "Gradient Descent(84/99): loss =0.32070642337368066, w0=-0.0913968664424093, w1=0.09893118331297271\n",
      "Gradient Descent(85/99): loss =0.3206376042793096, w0=-0.09237238137874497, w1=0.09912176674188343\n",
      "Gradient Descent(86/99): loss =0.3205699120705984, w0=-0.09334353104546025, w1=0.09931210362592055\n",
      "Gradient Descent(87/99): loss =0.3205033206980568, w0=-0.09431030562119233, w1=0.09950214751225649\n",
      "Gradient Descent(88/99): loss =0.3204378050348925, w0=-0.09527269692766219, w1=0.09969185562095541\n",
      "Gradient Descent(89/99): loss =0.3203733408293114, w0=-0.09623069835303578, w1=0.09988118857031175\n",
      "Gradient Descent(90/99): loss =0.32030990466001275, w0=-0.0971843047783882, w1=0.10007011012351116\n",
      "Gradient Descent(91/99): loss =0.3202474738946209, w0=-0.09813351250716587, w1=0.10025858695488091\n",
      "Gradient Descent(92/99): loss =0.32018602665081564, w0=-0.09907831919754308, w1=0.10044658843414378\n",
      "Gradient Descent(93/99): loss =0.3201255417599522, w0=-0.10001872379757051, w1=0.10063408642722398\n",
      "Gradient Descent(94/99): loss =0.3200659987329721, w0=-0.1009547264830154, w1=0.10082105511227563\n",
      "Gradient Descent(95/99): loss =0.32000737772843635, w0=-0.10188632859779453, w1=0.10100747080971637\n",
      "Gradient Descent(96/99): loss =0.3199496595225161, w0=-0.10281353259690386, w1=0.10119331182515026\n",
      "Gradient Descent(97/99): loss =0.319892825480798, w0=-0.10373634199175039, w1=0.10137855830415739\n",
      "Gradient Descent(98/99): loss =0.3198368575317726, w0=-0.10465476129779475, w1=0.10156319209801211\n",
      "Gradient Descent(99/99): loss =0.3197817381418818, w0=-0.10556879598441499, w1=0.10174719663946999\n",
      "Starting cross-validation 3/4 for least_squares_GD, extended feature of degree 2 and arguments : {'max_iters': 100, 'plot': True}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/99): loss =5.578057212932534, w0=-0.002047111290296444, w1=0.19999399172225774\n",
      "Gradient Descent(2/99): loss =1.7141143981388647, w0=-0.03389075983595424, w1=0.1696272482543819\n",
      "Gradient Descent(3/99): loss =0.6339522592005832, w0=-0.01886759405924081, w1=0.1694274386845904\n",
      "Gradient Descent(4/99): loss =0.47481818301930356, w0=-0.019030693047147003, w1=0.15700158285496552\n",
      "Gradient Descent(5/99): loss =0.423130454478631, w0=-0.016324825882761535, w1=0.14953121687621543\n",
      "Gradient Descent(6/99): loss =0.39353447333673064, w0=-0.01548409148260912, w1=0.14252124693016052\n",
      "Gradient Descent(7/99): loss =0.37521297483373234, w0=-0.014925985162894216, w1=0.13695441447300577\n",
      "Gradient Descent(8/99): loss =0.363355417013044, w0=-0.014906284270334658, w1=0.13217694908123329\n",
      "Gradient Descent(9/99): loss =0.3552831313742584, w0=-0.01518100497516602, w1=0.12813488390115982\n",
      "Gradient Descent(10/99): loss =0.3495566098309737, w0=-0.01570272265927653, w1=0.12464300303786183\n",
      "Gradient Descent(11/99): loss =0.3453567299343533, w0=-0.016390733272057458, w1=0.12161911423739591\n",
      "Gradient Descent(12/99): loss =0.3421880019492587, w0=-0.01720330932638111, w1=0.11897775327046112\n",
      "Gradient Descent(13/99): loss =0.33973687900227195, w0=-0.018103350456262445, w1=0.1166630022405215\n",
      "Gradient Descent(14/99): loss =0.33779804584800505, w0=-0.019067202789988566, w1=0.11462576371985761\n",
      "Gradient Descent(15/99): loss =0.3362332452003369, w0=-0.02007643842702523, w1=0.11282889326736716\n",
      "Gradient Descent(16/99): loss =0.33494709539375606, w0=-0.021118506822360404, w1=0.11124081859899307\n",
      "Gradient Descent(17/99): loss =0.33387235786344166, w0=-0.02218405773906319, w1=0.1098359249242065\n",
      "Gradient Descent(18/99): loss =0.3329606981340034, w0=-0.0232665457975421, w1=0.10859237580017182\n",
      "Gradient Descent(19/99): loss =0.33217674702989075, w0=-0.024361189273504835, w1=0.10749175603284514\n",
      "Gradient Descent(20/99): loss =0.33149419856264556, w0=-0.02546460044112696, w1=0.10651817917719228\n",
      "Gradient Descent(21/99): loss =0.33089319430617165, w0=-0.026574329935172433, w1=0.1056579322066511\n",
      "Gradient Descent(22/99): loss =0.3303585375814156, w0=-0.027688634092616255, w1=0.10489904142066886\n",
      "Gradient Descent(23/99): loss =0.32987845321914694, w0=-0.02880625885795573, w1=0.10423101700198427\n",
      "Gradient Descent(24/99): loss =0.32944371241265763, w0=-0.02992630788986587, w1=0.10364460938616508\n",
      "Gradient Descent(25/99): loss =0.3290470059237914, w0=-0.03104813486979688, w1=0.1031316353539945\n",
      "Gradient Descent(26/99): loss =0.3286824888465438, w0=-0.032171271762130256, w1=0.10268482590350106\n",
      "Gradient Descent(27/99): loss =0.3283454456069583, w0=-0.03329537383955552, w1=0.10229770657784577\n",
      "Gradient Descent(28/99): loss =0.328032040393795, w0=-0.034420181612990215, w1=0.10196449502012314\n",
      "Gradient Descent(29/99): loss =0.3277391290858649, w0=-0.03554549270624904, w1=0.10168001618950029\n",
      "Gradient Descent(30/99): loss =0.32746411599931036, w0=-0.03667114218982625, w1=0.10143962961953017\n",
      "Gradient Descent(31/99): loss =0.32720484368741615, w0=-0.03779698848528378, w1=0.10123916748239167\n",
      "Gradient Descent(32/99): loss =0.32695950738827984, w0=-0.03892290363933507, w1=0.10107488097447392\n",
      "Gradient Descent(33/99): loss =0.326726588046912, w0=-0.04004876662536919, w1=0.1009433938731033\n",
      "Gradient Descent(34/99): loss =0.32650479947329086, w0=-0.04117445891836284, w1=0.10084166196153006\n",
      "Gradient Descent(35/99): loss =0.3262930463575765, w0=-0.04229986167030141, w1=0.10076693748509778\n",
      "Gradient Descent(36/99): loss =0.32609039069529117, w0=-0.04342485404969297, w1=0.10071673786010486\n",
      "Gradient Descent(37/99): loss =0.32589602477805985, w0=-0.04454931239381334, w1=0.1006888180516619\n",
      "Gradient Descent(38/99): loss =0.32570924934697904, w0=-0.04567310993039224, w1=0.1006811461120712\n",
      "Gradient Descent(39/99): loss =0.325529455832275, w0=-0.04679611688274704, w1=0.10069188146912371\n",
      "Gradient Descent(40/99): loss =0.3253561118468549, w0=-0.0479182008264266, w1=0.10071935561194034\n",
      "Gradient Descent(41/99): loss =0.3251887492852428, w0=-0.04903922719977387, w1=0.10076205487887319\n",
      "Gradient Descent(42/99): loss =0.3250269545192067, w0=-0.050159059899265314, w1=0.10081860509322331\n",
      "Gradient Descent(43/99): loss =0.3248703602885711, w0=-0.051277561910120596, w1=0.10088775782903874\n",
      "Gradient Descent(44/99): loss =0.32471863896848846, w0=-0.052394595937963355, w1=0.1009683781182709\n",
      "Gradient Descent(45/99): loss =0.3245714969588411, w0=-0.05351002501822351, w1=0.10105943343554369\n",
      "Gradient Descent(46/99): loss =0.32442866999185715, w0=-0.05462371308817225, w1=0.10115998381754636\n",
      "Gradient Descent(47/99): loss =0.324289919193725, w0=-0.055735525512368225, w1=0.10126917299188871\n",
      "Gradient Descent(48/99): loss =0.3241550277674342, w0=-0.05684532955659337, w1=0.10138622040542905\n",
      "Gradient Descent(49/99): loss =0.3240237981890838, w0=-0.057952994808398765, w1=0.10151041405518993\n",
      "Gradient Descent(50/99): loss =0.3238960498299103, w0=-0.059058393544521214, w1=0.10164110403628945\n",
      "Gradient Descent(51/99): loss =0.323771616932338, w0=-0.060161401046884194, w1=0.10177769673115539\n",
      "Gradient Descent(52/99): loss =0.3236503468812988, w0=-0.06126189586985051, w1=0.101919649572865\n",
      "Gradient Descent(53/99): loss =0.32353209872253963, w0=-0.06235976006197758, w1=0.10206646632295971\n",
      "Gradient Descent(54/99): loss =0.32341674188812036, w0=-0.06345487934584228, w1=0.10221769281067006\n",
      "Gradient Descent(55/99): loss =0.32330415509622457, w0=-0.064547143259626, w1=0.10237291308628241\n",
      "Gradient Descent(56/99): loss =0.3231942253980439, w0=-0.06563644526413923, w1=0.10253174594649012\n",
      "Gradient Descent(57/99): loss =0.32308684734911586, w0=-0.06672268281885951, w1=0.10269384179408927\n",
      "Gradient Descent(58/99): loss =0.32298192228628303, w0=-0.0678057574303912, w1=0.10285887979837846\n",
      "Gradient Descent(59/99): loss =0.32287935769455633, w0=-0.06888557467654982, w1=0.10302656532616873\n",
      "Gradient Descent(60/99): loss =0.32277906665073614, w0=-0.06996204420904832, w1=0.10319662761646028\n",
      "Gradient Descent(61/99): loss =0.32268096733276525, w0=-0.07103507973752712, w1=0.1033688176746435\n",
      "Gradient Descent(62/99): loss =0.32258498258554547, w0=-0.07210459899743411, w1=0.10354290636457658\n",
      "Gradient Descent(63/99): loss =0.322491039535407, w0=-0.07317052370403082, w1=0.10371868267911558\n",
      "Gradient Descent(64/99): loss =0.32239906924663453, w0=-0.0742327794945805, w1=0.10389595217165647\n",
      "Gradient Descent(65/99): loss =0.32230900641446003, w0=-0.07529129586056593, w1=0.10407453553302114\n",
      "Gradient Descent(66/99): loss =0.3222207890897798, w0=-0.07634600607159, w1=0.10425426729960294\n",
      "Gradient Descent(67/99): loss =0.322134358431554, w0=-0.0773968470924323, w1=0.1044349946801047\n",
      "Gradient Descent(68/99): loss =0.3220496584834461, w0=-0.07844375949456943, w1=0.10461657648947066\n",
      "Gradient Descent(69/99): loss =0.3219666359717459, w0=-0.07948668736331543, w1=0.10479888217975024\n",
      "Gradient Descent(70/99): loss =0.32188524012204944, w0=-0.08052557820160136, w1=0.10498179095865072\n",
      "Gradient Descent(71/99): loss =0.3218054224925128, w0=-0.08156038283128833, w1=0.10516519098744982\n",
      "Gradient Descent(72/99): loss =0.321727136821802, w0=-0.08259105529279649, w1=0.10534897865075923\n",
      "Gradient Descent(73/99): loss =0.3216503388901097, w0=-0.0836175527437311, w1=0.10553305789136731\n",
      "Gradient Descent(74/99): loss =0.32157498639182686, w0=-0.08463983535709677, w1=0.1057173396040507\n",
      "Gradient Descent(75/99): loss =0.3215010388186393, w0=-0.08565786621960997, w1=0.10590174108283984\n",
      "Gradient Descent(76/99): loss =0.3214284573519749, w0=-0.086671611230548, w1=0.10608618551675822\n",
      "Gradient Descent(77/99): loss =0.3213572047638625, w0=-0.08768103900150837, w1=0.10627060152953724\n",
      "Gradient Descent(78/99): loss =0.32128724532537517, w0=-0.08868612075739585, w1=0.10645492275924126\n",
      "Gradient Descent(79/99): loss =0.3212185447219322, w0=-0.08968683023890428, w1=0.10663908747412829\n",
      "Gradient Descent(80/99): loss =0.3211510699748162, w0=-0.09068314360671523, w1=0.10682303822142314\n",
      "Gradient Descent(81/99): loss =0.3210847893683371, w0=-0.091675039347597, w1=0.10700672150599688\n",
      "Gradient Descent(82/99): loss =0.32101967238213736, w0=-0.09266249818255268, w1=0.10719008749623193\n",
      "Gradient Descent(83/99): loss =0.32095568962818766, w0=-0.09364550297713559, w1=0.10737308975461018\n",
      "Gradient Descent(84/99): loss =0.32089281279207205, w0=-0.09462403865402433, w1=0.10755568499079367\n",
      "Gradient Descent(85/99): loss =0.32083101457820024, w0=-0.09559809210792616, w1=0.10773783283517742\n",
      "Gradient Descent(86/99): loss =0.32077026865862696, w0=-0.09656765212285746, w1=0.10791949563108351\n",
      "Gradient Descent(87/99): loss =0.32071054962518425, w0=-0.09753270929183255, w1=0.1081006382439364\n",
      "Gradient Descent(88/99): loss =0.32065183294466565, w0=-0.09849325593897702, w1=0.10828122788591431\n",
      "Gradient Descent(89/99): loss =0.3205940949168254, w0=-0.09944928604406857, w1=0.10846123395471122\n",
      "Gradient Descent(90/99): loss =0.32053731263497587, w0=-0.10040079516949735, w1=0.1086406278851702\n",
      "Gradient Descent(91/99): loss =0.3204814639489892, w0=-0.10134778038962822, w1=0.10881938301266322\n",
      "Gradient Descent(92/99): loss =0.3204265274305257, w0=-0.10229024022253912, w1=0.10899747444719585\n",
      "Gradient Descent(93/99): loss =0.32037248234032445, w0=-0.10322817456410309, w1=0.10917487895730876\n",
      "Gradient Descent(94/99): loss =0.32031930859741164, w0=-0.10416158462437569, w1=0.10935157486293293\n",
      "Gradient Descent(95/99): loss =0.3202669867500888, w0=-0.10509047286624483, w1=0.10952754193643144\n",
      "Gradient Descent(96/99): loss =0.320215497948576, w0=-0.1060148429462964, w1=0.10970276131113113\n",
      "Gradient Descent(97/99): loss =0.32016482391920026, w0=-0.10693469965784563, w1=0.10987721539670943\n",
      "Gradient Descent(98/99): loss =0.32011494694001885, w0=-0.1078500488760821, w1=0.11005088780085928\n",
      "Gradient Descent(99/99): loss =0.3200658498177857, w0=-0.10876089750527401, w1=0.11022376325670662\n",
      "Starting cross-validation 4/4 for least_squares_GD, extended feature of degree 2 and arguments : {'max_iters': 100, 'plot': True}\n",
      "Gradient Descent(0/99): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/99): loss =23.466899011446415, w0=-0.002038632791394962, w1=0.19999210909830573\n",
      "Gradient Descent(2/99): loss =1.70393562722261, w0=-0.03389926990921584, w1=0.16951806985661821\n",
      "Gradient Descent(3/99): loss =0.5741206124037165, w0=-0.024835240817469007, w1=0.16827237368189496\n",
      "Gradient Descent(4/99): loss =0.46823941579859873, w0=-0.022280131940082586, w1=0.15773300821939001\n",
      "Gradient Descent(5/99): loss =0.4223777936231357, w0=-0.01970966013960773, w1=0.1489535348254683\n",
      "Gradient Descent(6/99): loss =0.39428992621597886, w0=-0.01807126605972524, w1=0.14125744384724875\n",
      "Gradient Descent(7/99): loss =0.3766760955867179, w0=-0.017022023891492398, w1=0.13480012863410618\n",
      "Gradient Descent(8/99): loss =0.3650528008181287, w0=-0.01646093243502827, w1=0.12929877333571324\n",
      "Gradient Descent(9/99): loss =0.35702574600486736, w0=-0.016263651612295444, w1=0.12459252684239017\n",
      "Gradient Descent(10/99): loss =0.35127574529484507, w0=-0.01634781823649628, w1=0.12052170944488041\n",
      "Gradient Descent(11/99): loss =0.34702636031042533, w0=-0.016645135642591843, w1=0.11697864067417378\n",
      "Gradient Descent(12/99): loss =0.3437978101064592, w0=-0.017106571825808835, w1=0.11387348989948054\n",
      "Gradient Descent(13/99): loss =0.3412829886104663, w0=-0.01769400204704518, w1=0.11113941298708586\n",
      "Gradient Descent(14/99): loss =0.3392796174054282, w0=-0.018379257071405086, w1=0.1087218054631607\n",
      "Gradient Descent(15/99): loss =0.33765107215631135, w0=-0.019140780713697424, w1=0.10657744659036107\n",
      "Gradient Descent(16/99): loss =0.3363029335827154, w0=-0.019962349493843734, w1=0.1046706467098308\n",
      "Gradient Descent(17/99): loss =0.33516855377693267, w0=-0.020831534789388213, w1=0.10297202780076503\n",
      "Gradient Descent(18/99): loss =0.33419995660946655, w0=-0.021738839519668828, w1=0.10145688384409651\n",
      "Gradient Descent(19/99): loss =0.3333619743138316, w0=-0.022676925075188245, w1=0.10010432816869799\n",
      "Gradient Descent(20/99): loss =0.33262839175690107, w0=-0.02364010055430633, w1=0.09889648036797029\n",
      "Gradient Descent(21/99): loss =0.3319793621536357, w0=-0.024623912145313148, w1=0.09781793203750243\n",
      "Gradient Descent(22/99): loss =0.33139964369512825, w0=-0.025624850914042676, w1=0.09685529522324333\n",
      "Gradient Descent(23/99): loss =0.3308773759185121, w0=-0.026640126797970124, w1=0.0959968696467109\n",
      "Gradient Descent(24/99): loss =0.33040321705901954, w0=-0.027667501852195357, w1=0.09523237054819716\n",
      "Gradient Descent(25/99): loss =0.3299697267323993, w0=-0.02870516268889496, w1=0.0945527155456292\n",
      "Gradient Descent(26/99): loss =0.329570917889501, w0=-0.029751624511399368, w1=0.0939498499966351\n",
      "Gradient Descent(27/99): loss =0.32920192725059955, w0=-0.03080565771456437, w1=0.09341660511053712\n",
      "Gradient Descent(28/99): loss =0.3288587698075067, w0=-0.03186623200326283, w1=0.09294658009040427\n",
      "Gradient Descent(29/99): loss =0.3285381537596803, w0=-0.03293247349115906, w1=0.09253404393340804\n",
      "Gradient Descent(30/99): loss =0.3282373394408891, w0=-0.03400363176668125, w1=0.0921738525610797\n",
      "Gradient Descent(31/99): loss =0.32795403065236073, w0=-0.03507905448696519, w1=0.09186137847498525\n",
      "Gradient Descent(32/99): loss =0.32768629014285655, w0=-0.036158167749013606, w1=0.09159245053947473\n",
      "Gradient Descent(33/99): loss =0.3274324732774588, w0=-0.03724046087313838, w1=0.09136330213839838\n",
      "Gradient Descent(34/99): loss =0.3271911755480973, w0=-0.03832547458237689, w1=0.09117052627086865\n",
      "Gradient Descent(35/99): loss =0.3269611907194979, w0=-0.03941279179418973, w1=0.09101103647088488\n",
      "Gradient Descent(36/99): loss =0.32674147722055646, w0=-0.04050203042986772, w1=0.09088203264152907\n",
      "Gradient Descent(37/99): loss =0.3265311309816143, w0=-0.041592837783529686, w1=0.09078097107140727\n",
      "Gradient Descent(38/99): loss =0.32632936334969626, w0=-0.04268488609942029, w1=0.09070553802997816\n",
      "Gradient Descent(39/99): loss =0.32613548303245654, w0=-0.043777869086152534, w1=0.09065362644406229\n",
      "Gradient Descent(40/99): loss =0.3259488812592159, w0=-0.04487149915843451, w1=0.09062331523972549\n",
      "Gradient Descent(41/99): loss =0.3257690195263805, w0=-0.04596550524399125, w1=0.09061285100041533\n",
      "Gradient Descent(42/99): loss =0.3255954194304143, w0=-0.047059631029884356, w1=0.09062063164577314\n",
      "Gradient Descent(43/99): loss =0.32542765419565417, w0=-0.048153633550531164, w1=0.09064519187953171\n",
      "Gradient Descent(44/99): loss =0.3252653415846444, w0=-0.04924728204151354, w1=0.0906851901910589\n",
      "Gradient Descent(45/99): loss =0.3251081379412136, w0=-0.050340357000150035, w1=0.09073939722519188\n",
      "Gradient Descent(46/99): loss =0.3249557331655059, w0=-0.051432649406934094, w1=0.09080668536015832\n",
      "Gradient Descent(47/99): loss =0.3248078464587943, w0=-0.05252396007215399, w1=0.09088601935457905\n",
      "Gradient Descent(48/99): loss =0.3246642227065204, w0=-0.053614099079969765, w1=0.09097644794250545\n",
      "Gradient Descent(49/99): loss =0.3245246293924026, w0=-0.05470288530842683, w1=0.0910770962707496\n",
      "Gradient Descent(50/99): loss =0.32438885395599776, w0=-0.055790146008726574, w1=0.09118715908586694\n",
      "Gradient Descent(51/99): loss =0.32425670152182173, w0=-0.05687571643084927, w1=0.09130589458942032\n",
      "Gradient Descent(52/99): loss =0.3241279929408312, w0=-0.05795943948556889, w1=0.09143261888988222\n",
      "Gradient Descent(53/99): loss =0.32400256309536735, w0=-0.05904116543519269, w1=0.09156670098796182\n",
      "Gradient Descent(54/99): loss =0.32388025942703863, w0=-0.06012075160714357, w1=0.09170755823947109\n",
      "Gradient Descent(55/99): loss =0.32376094065385935, w0=-0.06119806212588973, w1=0.09185465224623153\n",
      "Gradient Descent(56/99): loss =0.32364447564856474, w0=-0.062272967659800886, w1=0.09200748513110756\n",
      "Gradient Descent(57/99): loss =0.3235307424546255, w0=-0.06334534518034131, w1=0.09216559615814468\n",
      "Gradient Descent(58/99): loss =0.32341962742027114, w0=-0.06441507773164931, w1=0.09232855866308746\n",
      "Gradient Descent(59/99): loss =0.32331102443397075, w0=-0.06548205420904443, w1=0.09249597726333295\n",
      "Gradient Descent(60/99): loss =0.32320483424740526, w0=-0.06654616914537698, w1=0.09266748531970877\n",
      "Gradient Descent(61/99): loss =0.3231009638741282, w0=-0.06760732250441936, w1=0.09284274262540915\n",
      "Gradient Descent(62/99): loss =0.32299932605389836, w0=-0.06866541948071166, w1=0.09302143330002624\n",
      "Gradient Descent(63/99): loss =0.3228998387741708, w0=-0.069720370305433, w1=0.09320326386892212\n",
      "Gradient Descent(64/99): loss =0.3228024248414825, w0=-0.0707720900579878, w1=0.09338796151023371\n",
      "Gradient Descent(65/99): loss =0.3227070114965217, w0=-0.07182049848308036, w1=0.09357527245362257\n",
      "Gradient Descent(66/99): loss =0.3226135300675539, w0=-0.07286551981311186, w1=0.09376496051649955\n",
      "Gradient Descent(67/99): loss =0.3225219156576246, w0=-0.07390708259577572, w1=0.0939568057648958\n",
      "Gradient Descent(68/99): loss =0.32243210686158713, w0=-0.07494511952675464, w1=0.09415060328743749\n",
      "Gradient Descent(69/99): loss =0.32234404550953855, w0=-0.0759795672874404, w1=0.09434616207202877\n",
      "Gradient Descent(70/99): loss =0.3222576764337029, w0=-0.07701036638760717, w1=0.09454330397587364\n",
      "Gradient Descent(71/99): loss =0.3221729472561842, w0=-0.07803746101297344, w1=0.09474186278038454\n",
      "Gradient Descent(72/99): loss =0.3220898081953431, w0=-0.07906079887758848, w1=0.09494168332334735\n",
      "Gradient Descent(73/99): loss =0.3220082118888356, w0=-0.08008033108097691, w1=0.09514262070144881\n",
      "Gradient Descent(74/99): loss =0.32192811323158915, w0=-0.08109601196997195, w1=0.09534453953693314\n",
      "Gradient Descent(75/99): loss =0.3218494692272078, w0=-0.08210779900516277, w1=0.09554731330274766\n",
      "Gradient Descent(76/99): loss =0.32177223885147027, w0=-0.08311565263187705, w1=0.09575082370107038\n",
      "Gradient Descent(77/99): loss =0.32169638292674585, w0=-0.08411953615561445, w1=0.09595496009059219\n",
      "Gradient Descent(78/99): loss =0.32162186400628534, w0=-0.08511941562184232, w1=0.09615961895835717\n",
      "Gradient Descent(79/99): loss =0.32154864626746044, w0=-0.08611525970006038, w1=0.0963647034323539\n",
      "Gradient Descent(80/99): loss =0.321476695413131, w0=-0.08710703957203732, w1=0.09657012283140053\n",
      "Gradient Descent(81/99): loss =0.3214059785804036, w0=-0.08809472882411895, w1=0.09677579224918274\n",
      "Gradient Descent(82/99): loss =0.3213364642561262, w0=-0.08907830334350496, w1=0.09698163216958874\n",
      "Gradient Descent(83/99): loss =0.32126812219853235, w0=-0.09005774121838889, w1=0.09718756811074329\n",
      "Gradient Descent(84/99): loss =0.3212009233645047, w0=-0.0910330226418548, w1=0.09739353029537583\n",
      "Gradient Descent(85/99): loss =0.3211348398419863, w0=-0.09200412981942252, w1=0.09759945334536808\n",
      "Gradient Descent(86/99): loss =0.3210698447871097, w0=-0.09297104688013351, w1=0.09780527599851775\n",
      "Gradient Descent(87/99): loss =0.3210059123656591, w0=-0.09393375979106874, w1=0.09801094084572709\n",
      "Gradient Descent(88/99): loss =0.32094301769851535, w0=-0.09489225627519067, w1=0.09821639408698205\n",
      "Gradient Descent(89/99): loss =0.32088113681076813, w0=-0.095846525732402, w1=0.09842158530462951\n",
      "Gradient Descent(90/99): loss =0.3208202465842072, w0=-0.09679655916371514, w1=0.09862646725258871\n",
      "Gradient Descent(91/99): loss =0.32076032471293137, w0=-0.09774234909842736, w1=0.09883099566025016\n",
      "Gradient Descent(92/99): loss =0.32070134966183766, w0=-0.0986838895241985, w1=0.09903512904992144\n",
      "Gradient Descent(93/99): loss =0.3206433006277741, w0=-0.09962117581992998, w1=0.0992388285667755\n",
      "Gradient Descent(94/99): loss =0.3205861575031556, w0=-0.10055420469134546, w1=0.0994420578203456\n",
      "Gradient Descent(95/99): loss =0.3205299008418653, w0=-0.1014829741091761, w1=0.09964478273668999\n",
      "Gradient Descent(96/99): loss =0.32047451182727144, w0=-0.10240748324985555, w1=0.09984697142042301\n",
      "Gradient Descent(97/99): loss =0.32041997224221047, w0=-0.10332773243863184, w1=0.1000485940258749\n",
      "Gradient Descent(98/99): loss =0.3203662644407943, w0=-0.10424372309500643, w1=0.1002496226367035\n",
      "Gradient Descent(99/99): loss =0.3203133713219148, w0=-0.10515545768041247, w1=0.10045003115333574\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = 'least_squares_GD'\n",
    "seed = 1\n",
    "k_fold = 4\n",
    "k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "max_iters=100\n",
    "params = {'max_iters':max_iters, 'plot':True}\n",
    "\n",
    "\n",
    "accs_te = []\n",
    "losses_te = []\n",
    "for k in range(k_fold):\n",
    "    _, loss_te, _, acc_te = cross_validation(y_tr, tX_tr, k_indices, k, model, best_degree, params, feedback = True)\n",
    "    accs_te.append(acc_te)\n",
    "    losses_te.append(loss_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, 100 iterations is too much so we will re-run it with less iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation 1/4 for least_squares_GD, extended feature of degree 2 and arguments : {'max_iters': 20, 'plot': True}\n",
      "Gradient Descent(0/19): loss =0.5, w0=-0.10000000000000002, w1=0.10000000000000002\n",
      "Gradient Descent(1/19): loss =7.839059935623126, w0=-0.0026156943882950368, w1=0.19999316275092116\n",
      "Gradient Descent(2/19): loss =8.75329449924172, w0=-0.049107858033846194, w1=0.16678973987966308\n",
      "Gradient Descent(3/19): loss =1.8621114635202523, w0=-0.022850700072873317, w1=0.1726996630256814\n",
      "Gradient Descent(4/19): loss =0.7383762858533274, w0=-0.02701458582923431, w1=0.15578907023023617\n",
      "Gradient Descent(5/19): loss =0.5662999384979516, w0=-0.021923980358801984, w1=0.1486280847753041\n",
      "Gradient Descent(6/19): loss =0.5156944373733406, w0=-0.021017199928453953, w1=0.14030139119986393\n",
      "Gradient Descent(7/19): loss =0.48810568484290595, w0=-0.019649961929925353, w1=0.1341529768138526\n",
      "Gradient Descent(8/19): loss =0.46901730367531697, w0=-0.01913208615736729, w1=0.12872586535326513\n",
      "Gradient Descent(9/19): loss =0.45474042160114586, w0=-0.018869814395055542, w1=0.12420143984865449\n",
      "Gradient Descent(10/19): loss =0.4436061254789799, w0=-0.01892850243342921, w1=0.12029547107675036\n",
      "Gradient Descent(11/19): loss =0.4346440702968133, w0=-0.019188281057969252, w1=0.11693350232873535\n",
      "Gradient Descent(12/19): loss =0.4272391938952367, w0=-0.019614762596568268, w1=0.11401174841549663\n",
      "Gradient Descent(13/19): loss =0.4209853862663078, w0=-0.020163763643151724, w1=0.11146559970743769\n",
      "Gradient Descent(14/19): loss =0.4156060227675358, w0=-0.02080843635664514, w1=0.10923848514048146\n",
      "Gradient Descent(15/19): loss =0.41090740269902987, w0=-0.02152654929090058, w1=0.10728697178545459\n",
      "Gradient Descent(16/19): loss =0.40675037679517834, w0=-0.02230213125513223, w1=0.10557501244815369\n",
      "Gradient Descent(17/19): loss =0.40303259616363574, w0=-0.023122874138783066, w1=0.10407312949895886\n",
      "Gradient Descent(18/19): loss =0.3996771318101973, w0=-0.023979542503003444, w1=0.10275660798362515\n",
      "Gradient Descent(19/19): loss =0.3966250094052997, w0=-0.024865060837867405, w1=0.10160460676562937\n",
      "Starting cross-validation 2/4 for least_squares_GD, extended feature of degree 2 and arguments : {'max_iters': 20, 'plot': True}\n",
      "Gradient Descent(0/19): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/19): loss =23.218819551185142, w0=-0.0020510006922389717, w1=0.1999930949740934\n",
      "Gradient Descent(2/19): loss =1.7069669301591468, w0=-0.03396607658744928, w1=0.1690559486179855\n",
      "Gradient Descent(3/19): loss =0.5726444386439423, w0=-0.024211784986175496, w1=0.17004295386238566\n",
      "Gradient Descent(4/19): loss =0.46677094460534024, w0=-0.02149860327065887, w1=0.16105178656120314\n",
      "Gradient Descent(5/19): loss =0.4225764690527554, w0=-0.018797038362096615, w1=0.15357027736961631\n",
      "Gradient Descent(6/19): loss =0.3948155370765971, w0=-0.01711676744769795, w1=0.14669099945660716\n",
      "Gradient Descent(7/19): loss =0.3770714648911897, w0=-0.01604159185094181, w1=0.14078531139132835\n",
      "Gradient Descent(8/19): loss =0.3654080625309092, w0=-0.015486911312984684, w1=0.1355541388225885\n",
      "Gradient Descent(9/19): loss =0.3573863205854477, w0=-0.01530260610356254, w1=0.13095865431315462\n",
      "Gradient Descent(10/19): loss =0.35163854679842077, w0=-0.015411619329248444, w1=0.12685950666861862\n",
      "Gradient Descent(11/19): loss =0.3473749456496453, w0=-0.015736799716261716, w1=0.12320553184956098\n",
      "Gradient Descent(12/19): loss =0.3441163310846299, w0=-0.016230261261968675, w1=0.11992655950010864\n",
      "Gradient Descent(13/19): loss =0.3415598967617122, w0=-0.016850916001337816, w1=0.11698229700198702\n",
      "Gradient Descent(14/19): loss =0.3395079072341074, w0=-0.01757069395290656, w1=0.11433108480252545\n",
      "Gradient Descent(15/19): loss =0.3378275281254208, w0=-0.018367004189139533, w1=0.11194259574569443\n",
      "Gradient Descent(16/19): loss =0.3364271959131066, w0=-0.019223520921392588, w1=0.10978857130658752\n",
      "Gradient Descent(17/19): loss =0.33524226230563003, w0=-0.0201274165319058, w1=0.10784590653063736\n",
      "Gradient Descent(18/19): loss =0.33422604626718566, w0=-0.021069085967963773, w1=0.10609374718268237\n",
      "Gradient Descent(19/19): loss =0.3333441297443167, w0=-0.02204101603550966, w1=0.10451408751191944\n",
      "Starting cross-validation 3/4 for least_squares_GD, extended feature of degree 2 and arguments : {'max_iters': 20, 'plot': True}\n",
      "Gradient Descent(0/19): loss =0.5, w0=-0.1, w1=0.1\n",
      "Gradient Descent(1/19): loss =5.578057212932534, w0=-0.002047111290296444, w1=0.19999399172225774\n",
      "Gradient Descent(2/19): loss =1.7141143981388647, w0=-0.03389075983595424, w1=0.1696272482543819\n",
      "Gradient Descent(3/19): loss =0.6339522592005832, w0=-0.01886759405924081, w1=0.1694274386845904\n",
      "Gradient Descent(4/19): loss =0.47481818301930356, w0=-0.019030693047147003, w1=0.15700158285496552\n",
      "Gradient Descent(5/19): loss =0.423130454478631, w0=-0.016324825882761535, w1=0.14953121687621543\n",
      "Gradient Descent(6/19): loss =0.39353447333673064, w0=-0.01548409148260912, w1=0.14252124693016052\n",
      "Gradient Descent(7/19): loss =0.37521297483373234, w0=-0.014925985162894216, w1=0.13695441447300577\n",
      "Gradient Descent(8/19): loss =0.363355417013044, w0=-0.014906284270334658, w1=0.13217694908123329\n",
      "Gradient Descent(9/19): loss =0.3552831313742584, w0=-0.01518100497516602, w1=0.12813488390115982\n",
      "Gradient Descent(10/19): loss =0.3495566098309737, w0=-0.01570272265927653, w1=0.12464300303786183\n",
      "Gradient Descent(11/19): loss =0.3453567299343533, w0=-0.016390733272057458, w1=0.12161911423739591\n",
      "Gradient Descent(12/19): loss =0.3421880019492587, w0=-0.01720330932638111, w1=0.11897775327046112\n",
      "Gradient Descent(13/19): loss =0.33973687900227195, w0=-0.018103350456262445, w1=0.1166630022405215\n",
      "Gradient Descent(14/19): loss =0.33779804584800505, w0=-0.019067202789988566, w1=0.11462576371985761\n",
      "Gradient Descent(15/19): loss =0.3362332452003369, w0=-0.02007643842702523, w1=0.11282889326736716\n",
      "Gradient Descent(16/19): loss =0.33494709539375606, w0=-0.021118506822360404, w1=0.11124081859899307\n",
      "Gradient Descent(17/19): loss =0.33387235786344166, w0=-0.02218405773906319, w1=0.1098359249242065\n",
      "Gradient Descent(18/19): loss =0.3329606981340034, w0=-0.0232665457975421, w1=0.10859237580017182\n",
      "Gradient Descent(19/19): loss =0.33217674702989075, w0=-0.024361189273504835, w1=0.10749175603284514\n",
      "Starting cross-validation 4/4 for least_squares_GD, extended feature of degree 2 and arguments : {'max_iters': 20, 'plot': True}\n",
      "Gradient Descent(0/19): loss =0.5, w0=-0.09999999999999999, w1=0.1\n",
      "Gradient Descent(1/19): loss =23.466899011446415, w0=-0.002038632791394962, w1=0.19999210909830573\n",
      "Gradient Descent(2/19): loss =1.70393562722261, w0=-0.03389926990921584, w1=0.16951806985661821\n",
      "Gradient Descent(3/19): loss =0.5741206124037165, w0=-0.024835240817469007, w1=0.16827237368189496\n",
      "Gradient Descent(4/19): loss =0.46823941579859873, w0=-0.022280131940082586, w1=0.15773300821939001\n",
      "Gradient Descent(5/19): loss =0.4223777936231357, w0=-0.01970966013960773, w1=0.1489535348254683\n",
      "Gradient Descent(6/19): loss =0.39428992621597886, w0=-0.01807126605972524, w1=0.14125744384724875\n",
      "Gradient Descent(7/19): loss =0.3766760955867179, w0=-0.017022023891492398, w1=0.13480012863410618\n",
      "Gradient Descent(8/19): loss =0.3650528008181287, w0=-0.01646093243502827, w1=0.12929877333571324\n",
      "Gradient Descent(9/19): loss =0.35702574600486736, w0=-0.016263651612295444, w1=0.12459252684239017\n",
      "Gradient Descent(10/19): loss =0.35127574529484507, w0=-0.01634781823649628, w1=0.12052170944488041\n",
      "Gradient Descent(11/19): loss =0.34702636031042533, w0=-0.016645135642591843, w1=0.11697864067417378\n",
      "Gradient Descent(12/19): loss =0.3437978101064592, w0=-0.017106571825808835, w1=0.11387348989948054\n",
      "Gradient Descent(13/19): loss =0.3412829886104663, w0=-0.01769400204704518, w1=0.11113941298708586\n",
      "Gradient Descent(14/19): loss =0.3392796174054282, w0=-0.018379257071405086, w1=0.1087218054631607\n",
      "Gradient Descent(15/19): loss =0.33765107215631135, w0=-0.019140780713697424, w1=0.10657744659036107\n",
      "Gradient Descent(16/19): loss =0.3363029335827154, w0=-0.019962349493843734, w1=0.1046706467098308\n",
      "Gradient Descent(17/19): loss =0.33516855377693267, w0=-0.020831534789388213, w1=0.10297202780076503\n",
      "Gradient Descent(18/19): loss =0.33419995660946655, w0=-0.021738839519668828, w1=0.10145688384409651\n",
      "Gradient Descent(19/19): loss =0.3333619743138316, w0=-0.022676925075188245, w1=0.10010432816869799\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_degree=2\n",
    "model = 'least_squares_GD'\n",
    "seed = 1\n",
    "k_fold = 4\n",
    "k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "max_iters=20\n",
    "params = {'max_iters':max_iters, 'plot':True}\n",
    "\n",
    "\n",
    "accs_te = []\n",
    "losses_te = []\n",
    "for k in range(k_fold):\n",
    "    _, loss_te, _, acc_te = cross_validation(y_tr, tX_tr, k_indices, k, model, best_degree, params, feedback = True)\n",
    "    accs_te.append(acc_te)\n",
    "    losses_te.append(loss_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of best least squares gradient descent model:  0.759\n",
      "Mean loss of best least squares gradient descent model:  0.396\n"
     ]
    }
   ],
   "source": [
    "print('Mean accuracy of best least squares gradient descent model: ', round(np.mean(accs_te),3))\n",
    "print('Mean loss of best least squares gradient descent model: ', round(np.mean(losses_te),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store values for comparison with other models later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('best_models_perf/best_accs_' + model, accs_te)\n",
    "np.save('best_models_perf/best_losses_' + model, losses_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and parameters definition for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr, tX_tr, ids_tr, tX_te, ids_te = preprocess_data(y_train, tX_train, ids_train, tX_test, ids_test, param={'Build_poly': False})\n",
    "\n",
    "seed = 1\n",
    "k_fold = 4 \n",
    "model = 'ridge_regression'\n",
    "degrees = np.arange(4,15,1)\n",
    "lambdas = np.logspace(-6,0,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing degree 4/14, model: ridge_regression, arguments: {'lambda': 1e-06}\n",
      "Optimizing degree 4/14, model: ridge_regression, arguments: {'lambda': 1e-05}\n",
      "Optimizing degree 4/14, model: ridge_regression, arguments: {'lambda': 0.0001}\n",
      "Optimizing degree 4/14, model: ridge_regression, arguments: {'lambda': 0.001}\n",
      "Optimizing degree 4/14, model: ridge_regression, arguments: {'lambda': 0.01}\n",
      "Optimizing degree 4/14, model: ridge_regression, arguments: {'lambda': 0.1}\n",
      "Optimizing degree 4/14, model: ridge_regression, arguments: {'lambda': 1.0}\n",
      "Optimizing degree 5/14, model: ridge_regression, arguments: {'lambda': 1e-06}\n",
      "Optimizing degree 5/14, model: ridge_regression, arguments: {'lambda': 1e-05}\n",
      "Optimizing degree 5/14, model: ridge_regression, arguments: {'lambda': 0.0001}\n",
      "Optimizing degree 5/14, model: ridge_regression, arguments: {'lambda': 0.001}\n",
      "Optimizing degree 5/14, model: ridge_regression, arguments: {'lambda': 0.01}\n",
      "Optimizing degree 5/14, model: ridge_regression, arguments: {'lambda': 0.1}\n",
      "Optimizing degree 5/14, model: ridge_regression, arguments: {'lambda': 1.0}\n",
      "Optimizing degree 6/14, model: ridge_regression, arguments: {'lambda': 1e-06}\n",
      "Optimizing degree 6/14, model: ridge_regression, arguments: {'lambda': 1e-05}\n",
      "Optimizing degree 6/14, model: ridge_regression, arguments: {'lambda': 0.0001}\n",
      "Optimizing degree 6/14, model: ridge_regression, arguments: {'lambda': 0.001}\n",
      "Optimizing degree 6/14, model: ridge_regression, arguments: {'lambda': 0.01}\n",
      "Optimizing degree 6/14, model: ridge_regression, arguments: {'lambda': 0.1}\n",
      "Optimizing degree 6/14, model: ridge_regression, arguments: {'lambda': 1.0}\n",
      "Optimizing degree 7/14, model: ridge_regression, arguments: {'lambda': 1e-06}\n",
      "Optimizing degree 7/14, model: ridge_regression, arguments: {'lambda': 1e-05}\n",
      "Optimizing degree 7/14, model: ridge_regression, arguments: {'lambda': 0.0001}\n",
      "Optimizing degree 7/14, model: ridge_regression, arguments: {'lambda': 0.001}\n",
      "Optimizing degree 7/14, model: ridge_regression, arguments: {'lambda': 0.01}\n",
      "Optimizing degree 7/14, model: ridge_regression, arguments: {'lambda': 0.1}\n",
      "Optimizing degree 7/14, model: ridge_regression, arguments: {'lambda': 1.0}\n",
      "Optimizing degree 8/14, model: ridge_regression, arguments: {'lambda': 1e-06}\n",
      "Optimizing degree 8/14, model: ridge_regression, arguments: {'lambda': 1e-05}\n",
      "Optimizing degree 8/14, model: ridge_regression, arguments: {'lambda': 0.0001}\n",
      "Optimizing degree 8/14, model: ridge_regression, arguments: {'lambda': 0.001}\n",
      "Optimizing degree 8/14, model: ridge_regression, arguments: {'lambda': 0.01}\n",
      "Optimizing degree 8/14, model: ridge_regression, arguments: {'lambda': 0.1}\n",
      "Optimizing degree 8/14, model: ridge_regression, arguments: {'lambda': 1.0}\n",
      "Optimizing degree 9/14, model: ridge_regression, arguments: {'lambda': 1e-06}\n",
      "Optimizing degree 9/14, model: ridge_regression, arguments: {'lambda': 1e-05}\n",
      "Optimizing degree 9/14, model: ridge_regression, arguments: {'lambda': 0.0001}\n",
      "Optimizing degree 9/14, model: ridge_regression, arguments: {'lambda': 0.001}\n",
      "Optimizing degree 9/14, model: ridge_regression, arguments: {'lambda': 0.01}\n",
      "Optimizing degree 9/14, model: ridge_regression, arguments: {'lambda': 0.1}\n",
      "Optimizing degree 9/14, model: ridge_regression, arguments: {'lambda': 1.0}\n",
      "Optimizing degree 10/14, model: ridge_regression, arguments: {'lambda': 1e-06}\n",
      "Optimizing degree 10/14, model: ridge_regression, arguments: {'lambda': 1e-05}\n",
      "Optimizing degree 10/14, model: ridge_regression, arguments: {'lambda': 0.0001}\n",
      "Optimizing degree 10/14, model: ridge_regression, arguments: {'lambda': 0.001}\n",
      "Optimizing degree 10/14, model: ridge_regression, arguments: {'lambda': 0.01}\n",
      "Optimizing degree 10/14, model: ridge_regression, arguments: {'lambda': 0.1}\n",
      "Optimizing degree 10/14, model: ridge_regression, arguments: {'lambda': 1.0}\n",
      "Optimizing degree 11/14, model: ridge_regression, arguments: {'lambda': 1e-06}\n",
      "Optimizing degree 11/14, model: ridge_regression, arguments: {'lambda': 1e-05}\n",
      "Optimizing degree 11/14, model: ridge_regression, arguments: {'lambda': 0.0001}\n",
      "Optimizing degree 11/14, model: ridge_regression, arguments: {'lambda': 0.001}\n",
      "Optimizing degree 11/14, model: ridge_regression, arguments: {'lambda': 0.01}\n",
      "Optimizing degree 11/14, model: ridge_regression, arguments: {'lambda': 0.1}\n",
      "Optimizing degree 11/14, model: ridge_regression, arguments: {'lambda': 1.0}\n",
      "Optimizing degree 12/14, model: ridge_regression, arguments: {'lambda': 1e-06}\n",
      "Optimizing degree 12/14, model: ridge_regression, arguments: {'lambda': 1e-05}\n",
      "Optimizing degree 12/14, model: ridge_regression, arguments: {'lambda': 0.0001}\n",
      "Optimizing degree 12/14, model: ridge_regression, arguments: {'lambda': 0.001}\n",
      "Optimizing degree 12/14, model: ridge_regression, arguments: {'lambda': 0.01}\n",
      "Optimizing degree 12/14, model: ridge_regression, arguments: {'lambda': 0.1}\n",
      "Optimizing degree 12/14, model: ridge_regression, arguments: {'lambda': 1.0}\n",
      "Optimizing degree 13/14, model: ridge_regression, arguments: {'lambda': 1e-06}\n",
      "Optimizing degree 13/14, model: ridge_regression, arguments: {'lambda': 1e-05}\n",
      "Optimizing degree 13/14, model: ridge_regression, arguments: {'lambda': 0.0001}\n",
      "Optimizing degree 13/14, model: ridge_regression, arguments: {'lambda': 0.001}\n",
      "Optimizing degree 13/14, model: ridge_regression, arguments: {'lambda': 0.01}\n",
      "Optimizing degree 13/14, model: ridge_regression, arguments: {'lambda': 0.1}\n",
      "Optimizing degree 13/14, model: ridge_regression, arguments: {'lambda': 1.0}\n",
      "Optimizing degree 14/14, model: ridge_regression, arguments: {'lambda': 1e-06}\n",
      "Optimizing degree 14/14, model: ridge_regression, arguments: {'lambda': 1e-05}\n",
      "Optimizing degree 14/14, model: ridge_regression, arguments: {'lambda': 0.0001}\n",
      "Optimizing degree 14/14, model: ridge_regression, arguments: {'lambda': 0.001}\n",
      "Optimizing degree 14/14, model: ridge_regression, arguments: {'lambda': 0.01}\n",
      "Optimizing degree 14/14, model: ridge_regression, arguments: {'lambda': 0.1}\n",
      "Optimizing degree 14/14, model: ridge_regression, arguments: {'lambda': 1.0}\n"
     ]
    }
   ],
   "source": [
    "losses_tr, losses_te, accs_tr, accs_te = params_optimization(y_tr, tX_tr, k_fold, model, degrees, lambdas, params = None, seed = seed, feedback = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we optimized over 2 parameters, so we use heatmap to visualize best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_heatmap(accs_tr, accs_te, degrees, lambdas, model, measure_type = 'Accuracy', save_img = True, img_name = 'Heatmap_accuracy_ridge_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually choose best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambda = 1e-6\n",
    "best_degree = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute performance on k splits with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation 1/6 for ridge_regression, extended feature of degree 11 and arguments : {'lambda': 1e-06}\n",
      "Starting cross-validation 2/6 for ridge_regression, extended feature of degree 11 and arguments : {'lambda': 1e-06}\n",
      "Starting cross-validation 3/6 for ridge_regression, extended feature of degree 11 and arguments : {'lambda': 1e-06}\n",
      "Starting cross-validation 4/6 for ridge_regression, extended feature of degree 11 and arguments : {'lambda': 1e-06}\n",
      "Starting cross-validation 5/6 for ridge_regression, extended feature of degree 11 and arguments : {'lambda': 1e-06}\n",
      "Starting cross-validation 6/6 for ridge_regression, extended feature of degree 11 and arguments : {'lambda': 1e-06}\n"
     ]
    }
   ],
   "source": [
    "model = 'ridge_regression'\n",
    "seed = 1\n",
    "k_fold = 6\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "params = {'lambda' : best_lambda}\n",
    "\n",
    "accs_te = []\n",
    "losses_te = []\n",
    "for k in range(k_fold):\n",
    "    _, loss_te, _, acc_te = cross_validation(y_tr, tX_tr, k_indices, k, model, best_degree, params, feedback = True)\n",
    "    accs_te.append(acc_te)\n",
    "    losses_te.append(loss_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of best ridge regression model:  0.816\n"
     ]
    }
   ],
   "source": [
    "print('Mean accuracy of best ridge regression model: ', round(np.mean(accs_te),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store values for comparison with other models later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('best_models_perf/best_accs_' + model, accs_te)\n",
    "#np.save('best_models_perf/best_losses_' + model, losses_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boxplots and variance of best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False positive, False negative, ids wrongly classified ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea for ids wrongly classified:\n",
    "get ids wrongly classified in train set for all models, and then color their position in the parameters distribution plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean of models predictions ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Test data loading done before for preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#DATA_TEST_PATH = '../Projet_1_data/test.csv' # TODO: download train data and supply path here \n",
    "#_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge trial submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model creation and test_data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr, tX_tr, ids_tr, tX_te, ids_te = preprocess_data(y_train, tX_train, ids_train, tX_test, ids_test, param={'Build_poly': False})\n",
    "best_lambda = 1e-6\n",
    "weights, loss = ridge_regression(y_tr, tX_tr, best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'submissions/ridge_1' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_te)\n",
    "create_csv_submission(ids_te, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
